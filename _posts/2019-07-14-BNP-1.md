---
layout: post
title: "1. Exchangeability and De Finetti's Theorem"
tags: [Bayesian Nonparametrics]
comments: true
---
 

# 1. Exchangeability and De Finetti's Theorem
## 1.1 Exchangeability
확률변수의 infinite sequence $X_1, \cdots , X_n ,\cdots$가 다음을 만족할 때, 그 sequence $X_1, \cdots , X_n ,\cdots$를 *exchangeable*, 혹은 *infinitely exchangeable*하다고 한다.

$$
\forall n \in \mathbb N, \text{ } (X_1, \cdots , X_n)  \stackrel{d}{=} (X_{\pi(1)}, \cdots , X_{\pi(n)}) \text{ for all }\pi \in S(n),
$$

$$
\text{where } S(n) \text{ is the collection of all permutations of } \{ 1, \cdots , n \}
$$

즉, Exchangeability는 확률변수의 infinite sequence의 marginal 분포가 permutation에 invariant하다는 것을, 더 쉽게 이야기한다면 우리의 자료가 재정렬되더라도 분포가 같다는 것을 의미한다. 통계학의 많은 분석 기법은 IID(independent and identically distributed) 상황을 가정한다. 실제로 IID를 만족하는 $(X_1, \cdots , X_N)$은 항상 exchangeable하다. 그러나 exchangeable한 확률변수들은 항상 IID인 것은 아니다. 이는 다음과 같은 반례를 들 수 있다.

> Let $(X_1, \cdots , X_N)$ be iid random variables, and $X_0$ be a random variable independent to all $(X_1, \cdots , X_N)$. Then $(X_0 + X_1, \cdots , X_0 + X_N)$ is exchangeable but not independent anymore.

## 1.2 De Finetti's Theorem

De Finetti 정리는 베이지안 통계학에서 parameter를 확률변수로 보고, 또 prior distribution을 부여하는 것에 정당성을 부여해준다는 점에서 큰 의미를 갖는 정리이다. De Finetti 정리의 간소화된 statement는 아래와 같다.
> If $(X_1, \cdots , X_N)$ are infinitely exchangeable, then the joint probability $p(X_1, \cdots , X_N)$ has a representation as a mixture:
> 
> $$
> p(X_1, \cdots , X_N)= \int \left( \prod_{i=1}^{N} p(X_i \mid \theta ) \right) d \pi(\theta)
> $$
> 
> for some random variable $\theta$.

다시 말해서, 어떤 확률변수들 $(X_1, \cdots , X_N)$에 대해 exchangeability를 가정한다면, 다음과 같은 사실이 만족한다.

 * $(X_1, \cdots , X_N)$의 joint distribution에는 underlying parameter가 있다.
 * 그리고 그 parameter $\theta$는 어떤 분포를 갖는 확률변수이다. 이를 prior distribution으로 부르자.
 * 우리의 자료는 그 parameter에 대해 conditionally independent하다.
	 * parameter가 한 값으로 고정되면 자료들 $(X_1, \cdots , X_N)$가 서로 independent하다는 의미이다.

여기서 parameter $\theta$는 Euclidean vector space 위의 "$\mathbb R^k$-valued random variable"로 한정되지 않고, probability measure의 space, $\mathfrak M = M(\mathcal X)$ 위의 "probability measure-valued random variable"로 이해할 수 있다. 이를 도식으로 나타내면 아래와 같다.

Parametric setting 하에서 parameter 확률변수 $\theta$에 대한 prior distribution $\pi$는 아래와 같은 induced probability measure이다.

![image](https://user-images.githubusercontent.com/45325895/60751877-50ee2180-9ff8-11e9-851a-ccba1460d6f7.png){: .center-image}

Parametric 가정이 없는 일반적인 상황에 대해 이야기하기 위해서는, 먼저 random measure의 정의에 대해 알아야 한다. 어떤 measurable space $(\mathfrak X, \mathcal X)$과 probability space $(\Omega,\mathcal F, \phi)$에 대해, **random measure**는 다음을 만족하는 mapping $P:\Omega \times \mathcal X \rightarrow \mathbb R$을 의미한다.
 * 각 $\omega \in \Omega$에 대해, $P(\omega, \cdot)$는 $(\mathfrak X, \mathcal X)$에서 정의된 probability measure이다.
 * 각 $A \in \mathcal X$에 대해, $P(\cdot, A)$는 real-valued random variable, 즉 measurable function이다.

다음 두 가지 사실을 기억하자.

 * $\omega \mapsto P(\omega, \cdot)$는 $\Omega$에서 $\mathfrak M$으로 가는 map.
 * 임의의 $A \in \mathcal X$에 대해, $P(\cdot, A)$는 real-valued random variable.


모형에 대한 parametric 가정이 없는 일반적인 상황에서는 모수공간 $\Theta$가 아닌 probability measure들의 collection, $\mathfrak M$ 위에 prior distribution, $\Pi$를 정의하고자 한다. 따라서, $\Omega$에서 $\mathfrak M$으로 가는 map으로서의 random measure $\omega \mapsto P(\omega, \cdot)$를 고려하는 것이다.
![image](https://user-images.githubusercontent.com/45325895/60751891-90b50900-9ff8-11e9-8504-f75d07387594.png){: .center-image}


위와 같이 모형에 대한 parametric 가정이 없는 일반적인 상황에서의 De Finetti 정리의 statement는 아래와 같다.

> Let $(\mathfrak X, \mathcal X)$ be a Polish space and $\mu$ be a probability measure on $\mathfrak X^\infty$. Then $X_1, X_2, \cdots$ is exchangeable if and only if there is a unique probability measure $\Pi$ on $M(\mathfrak X)$ such that for all $n \in \mathbb N$ and for any Borel sets $B_1, B_2, \cdots , B_n$,
> 
> $$
> \mu( \{ X_1 \in B_1, \cdots X_n \in B_n \}) = \int_{M(\mathbb R)} \prod_{i=1}^{n} P(B_i) d \Pi(P)
> $$

여기서 $(\mathfrak X ,\mathcal X)$를 Polish space, 즉 complete, separable, metrizable topological space로 가정하는 이유는 RCPD의 existence떄문. **(내용 보충 필요)**



# Reference
> Ghosal, Subhashis, and Aad Van der Vaart. Fundamentals of nonparametric Bayesian inference. Vol. 44. Cambridge University Press, 2017.  
> K. Ghosh, J & Ramamoorthi, R. Bayesian Nonparametrics. Springer Series in Statistics. 16. 2011.  
> Talk slide of YeeWhey Teh at The Machine Learning Summer School 2013 at the Max Planck Institute for Intelligent Systems, Tübingen, Germany (http://mlss.tuebingen.mpg.de/2013/2013/slides_teh.pdf)  
> Tutorial lecture of Michael I. Jordan at NIPS'05 (http://faculty.dbmi.pitt.edu/day/Bioinf2132-advanced-Bayes-and-R/Bioinf2132-documents-2017/2017-11-30/nips-tutorial05.pdf)



