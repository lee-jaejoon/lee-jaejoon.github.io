---
layout: post
title: "Introduction to Bayesian Nonparametrics 1"
tags: [Bayesian Statistics]
comments: true
---

> **Contents**  
> **[1. Exchangeability and De Finetti's Theorem](#1-exchangeability-and-de-finettis-theorem)**  
>  
> **[2. Bayesian Finite Mixture Model](#2-bayesian-finite-mixture-model)**  
> [2.1 Finite Mixture Model](#21-finite-mixture-model)  
> [2.2 Bayesian Finite Mixture Model](#22-bayesian-finite-mixture-model)  
> [2.3 Inference of Bayesian Finite Mixture Model](#23-inference-of-bayesian-finite-mixture-model)  
> [2.4 Model Selection for Finite Mixture Models](#24-model-selection-for-finite-mixture-models)  

# 1. Exchangeability and De Finetti's Theorem
## Exchangeability
확률변수의 infinite sequence $X_1, \cdots , X_n ,\cdots$가 다음을 만족할 때, 그 sequence $X_1, \cdots , X_n ,\cdots$를 *exchangeable*, 혹은 *infinitely exchangeable*하다고 한다.

$$
\forall n \in \mathbb N, \text{ } (X_1, \cdots , X_n)  \stackrel{d}{=} (X_{\pi(1)}, \cdots , X_{\pi(n)}) \text{ for all }\pi \in S(n),
$$

$$
\text{where } S(n) \text{ is the collection of all permutations of } \{ 1, \cdots , n \}
$$

즉, Exchangeability는 확률변수의 infinite sequence의 marginal 분포가 permutation에 invariant하다는 것을, 더 쉽게 이야기한다면 우리의 자료가 재정렬되더라도 분포가 같다는 것을 의미한다. 통계학의 많은 분석 기법은 IID(independent and identically distributed) 상황을 가정한다. 실제로 IID를 만족하는 $(X_1, \cdots , X_N)$은 항상 exchangeable하다. 그러나 exchangeable한 확률변수들은 항상 IID인 것은 아니다. 이는 다음과 같은 반례를 들 수 있다.

> Let $(X_1, \cdots , X_N)$ be iid random variables, and $X_0$ be a random variable independent to all $(X_1, \cdots , X_N)$. Then $(X_0 + X_1, \cdots , X_0 + X_N)$ is exchangeable but not independent anymore.

## De Finetti's Theorem

De Finetti 정리는 베이지안 통계학에서 parameter를 확률변수로 보고, 또 prior distribution을 부여하는 것에 정당성을 부여해준다는 점에서 큰 의미를 갖는 정리이다. De Finetti 정리의 간소화된 statement는 아래와 같다.
> If $(X_1, \cdots , X_N)$ are infinitely exchangeable, then the joint probability $p(X_1, \cdots , X_N)$ has a representation as a mixture:
> 
> $$
> p(X_1, \cdots , X_N)= \int \left( \prod_{i=1}^{N} p(X_i \mid \theta ) \right) d \pi(\theta)
> $$
> 
> for some random variable $\theta$.

다시 말해서, 어떤 확률변수들 $(X_1, \cdots , X_N)$에 대해 exchangeability를 가정한다면, 다음과 같은 사실이 만족한다.

 * $(X_1, \cdots , X_N)$의 joint distribution에는 underlying parameter가 있다.
 * 그리고 그 parameter $\theta$는 어떤 분포를 갖는 확률변수이다. 이를 prior distribution으로 부르자.
 * 우리의 자료는 그 parameter에 대해 conditionally independent하다.
	 * parameter가 한 값으로 고정되면 자료들 $(X_1, \cdots , X_N)$가 서로 independent하다는 의미이다.

여기서 parameter $\theta$는 Euclidean vector space 위의 "$\mathbb R^k$-valued random variable"로 한정되지 않고, probability measure의 space, $\mathfrak M = M(\mathcal X)$ 위의 "probability measure-valued random variable"로 이해할 수 있다. 이를 도식으로 나타내면 아래와 같다.

Parametric setting 하에서 parameter 확률변수 $\theta$에 대한 prior distribution $\pi$는 아래와 같은 induced probability measure이다.

![image](https://user-images.githubusercontent.com/45325895/60751877-50ee2180-9ff8-11e9-851a-ccba1460d6f7.png){: .center-image}

Parametric 가정이 없는 일반적인 상황에 대해 이야기하기 위해서는, 먼저 random measure의 정의에 대해 알아야 한다. 어떤 measurable space $(\mathfrak X, \mathcal X)$과 probability space $(\Omega,\mathcal F, \phi)$에 대해, **random measure**는 다음을 만족하는 mapping $P:\Omega \times \mathcal X \rightarrow \mathbb R$을 의미한다.
 * 각 $\omega \in \Omega$에 대해, $P(\omega, \cdot)$는 $(\mathfrak X, \mathcal X)$에서 정의된 probability measure이다.
 * 각 $A \in \mathcal X$에 대해, $P(\cdot, A)$는 real-valued random variable, 즉 measurable function이다.

다음 두 가지 사실을 기억하자.

 * $\omega \mapsto P(\omega, \cdot)$는 $\Omega$에서 $\mathfrak M$으로 가는 map.
 * 임의의 $A \in \mathcal X$에 대해, $P(\cdot, A)$는 real-valued random variable.


모형에 대한 parametric 가정이 없는 일반적인 상황에서는 모수공간 $\Theta$가 아닌 probability measure들의 collection, $\mathfrak M$ 위에 prior distribution, $\Pi$를 정의하고자 한다. 따라서, $\Omega$에서 $\mathfrak M$으로 가는 map으로서의 random measure $\omega \mapsto P(\omega, \cdot)$를 고려하는 것이다.
![image](https://user-images.githubusercontent.com/45325895/60751891-90b50900-9ff8-11e9-8504-f75d07387594.png){: .center-image}


위와 같이 모형에 대한 parametric 가정이 없는 일반적인 상황에서의 De Finetti 정리의 statement는 아래와 같다.

> Let $(\mathfrak X, \mathcal X)$ be a Polish space and $\mu$ be a probability measure on $\mathfrak X^\infty$. Then $X_1, X_2, \cdots$ is exchangeable if and only if there is a unique probability measure $\Pi$ on $M(\mathfrak X)$ such that for all $n \in \mathbb N$ and for any Borel sets $B_1, B_2, \cdots , B_n$,
> 
> $$
> \mu( \{ X_1 \in B_1, \cdots X_n \in B_n \}) = \int_{M(\mathbb R)} \prod_{i=1}^{n} P(B_i) d \Pi(P)
> $$

여기서 $(\mathfrak X ,\mathcal X)$를 Polish space, 즉 complete, separable, metrizable topological space로 가정하는 이유는 RCPD의 existence떄문. **(내용 보충 필요)**

# 2. Bayesian Finite Mixture Model

## 2.1 Finite Mixture Model

Bayesian nonparametric을 이용한 mixture model을 설명하기에 앞서, 기본적인 Bayesian finite mixture model에 대해서 살펴보고자 한다. Bayesian이 아닌 일반적인 mixture model을 이용한 clustering은 아래와 같은 generative 과정으로 나타낼 수 있다.

 * $\pi = (\pi_1 , \cdots , \pi_K)$의 분포로부터, $K$개의 cluster 중 하나를 고른다. 
 * 선택된 cluster의 확률분포로부터 data를 generate한다.

만약 $K$개의 cluster들이 같은 parametrized family의 분포를 따른다면, 위의 generative process로부터 다음과 같은 finite mixture model을 얻을 수 있다.

$$
p(x \mid \phi, \pi)= \sum_{k=1}^{K} \pi_k \text{ } p(x \mid \phi_k)
$$

예를 들면, finite Gaussian mixture의 경우, $\phi_k = (\mu_k, \Sigma_k)$이고 $ p(x \mid \phi_k)$는 $(\mu_k, \Sigma_k)$를 mean과 covariance로 갖는 gaussian distribution일 것이다.

$$
p(x \mid (\mu, \Sigma), \pi)= \sum_{k=1}^{K} \pi_k \text{ } \mathcal N(x \mid \mu_k, \Sigma_k)
$$

Bayesian mixture model로의 확장을 위해서 이를 아래와 같이 다른 방법으로 나타내보자. 다음과 같은 probability measure $G$를 정의한다.

$$
G = \sum_{k=1}^{K} \pi_k \text{ } \delta_{\phi_k} , \text{ where } \delta_{\phi_k} \text{ is a dirac measure given } \phi_k
$$

즉 $G$는 $\phi_k$에 $\pi_k$의 확률값을 부여하는 probability measure이다. $i=1,\cdots , N$에 대해, 다음과 같은 sample generating process를 생각해보자. 

![image](https://user-images.githubusercontent.com/45325895/61167998-d0906900-a581-11e9-86bf-bc29416d2c3f.png){: .center-image}

$$
\begin{align*}
\theta_i &\sim G \\
x_i &\sim p(\cdot \mid \theta_i)
\end{align*}
$$


각 $\theta_i$는 $\{ \phi_1 ,\cdots , \phi_K \}$ 중 하나의 값을 가지며, 같은 $\phi_k$의 값을 갖는 $\theta_i$들의 집합을 $k$th cluster로 이해할 수 있다.  

## 2.2 Bayesian Finite Mixture Model

이제 finite mixture model의 parameter $\phi_1, \cdots , \phi_K , \pi_1, \cdots , \pi_K$에 prior distribution을 부여하고자 한다. 여기서도 각 cluster들은 같은 parametric family의 분포를 갖는다고 가정하자.  
  
먼저 $\phi_k$에 대한 prior distribution을 생각해보자. 각 cluster distribution의 parameter인 $\phi_k$에 대한 prior distribution은 그 cluster가 어떤 parametric family의 분포를 갖는지에 따라 결정된다. 예를 들면 Gaussian mixture model의 경우는, conjugacy를 위해 mean $\mu_k$와 covariance $\Sigma_k$에 대해 normal/inverse-gamma prior를 부여할 것이다. 이를 $\phi_k \sim G_0$로 나타내자.  
  
Cluster probability인 $\pi_k$에 대한 prior distribution은 symmetric Dirichlet prior를 부여할 수 있다.

$$
(\pi_1, \cdots , \pi_K) \sim \text{Dirichlet}(\alpha_0 / K , \cdots , \alpha_0 / K )
$$

그 이유는 다음과 같다.

 * $(\pi_1, \cdots , \pi_K)$는 데이터가 어느 cluster에 assign되는지에 대한 확률이므로, 합해서 $1$이 되어야 한다.
 * Symmetry는 각 mixture component(cluster)의 labeling을 permutation하더라도 model이 달라지지 않도록 하는 역할을 한다.
 * Dirichlet distribution의 parameter들은 concentration parameter의 역할을 한다.
	 * Concentration parameter는 각 확률변수의 값이 얼마나 고르게 분포해있는지를 결정하는 parameter이다. Concentration parameter의 값이 클 수록 서로 유사한 값이 나올 확률이 크며, 그 값이 작을 수록 확률변수가 더 sparse한 분포를 갖는다.
 * Dirichlet distribution의 parameter를 mixture component의 수, $K$로 나누어 준 것은 Dirichlet parameter의 총 합($\alpha_0$)을 concentration parameter로 정의하는 convention을 따른 것이다. 이 경우, concentration parameter가 $\alpha_0=K$일 때 Dirichlet distribution이 uniform distribution over $K-1$ simplex가 된다.

Bayesian finite mixture model의 sample generating process는 다음과 같이 나타낼 수 있다.

![image](https://user-images.githubusercontent.com/45325895/61168633-089ca980-a58c-11e9-9c00-ac3de1a430ed.png){: .center-image}

$$
\begin{align*}
\phi_k &\sim G_0 \\
\pi &\sim \text{Dirichlet}(\alpha_0 / K , \cdots , \alpha_0 / K) \\
G &= \sum_{k=1}^{K} \pi_k \text{ } \delta_{\phi_k} \\
\theta_i &\sim G \\
x_i &\sim p(\cdot \mid \theta_i)
\end{align*}
$$

여기서 짚고 넘어갈 부분은 $i$번째 observation의 cluster를 결정하는 probability measure $G = \sum_{k=1}^{K} \pi_k \text{ } \delta_{\phi_k}$이 위의 finite mixture model에서처럼 고정된(deterministic) measure가 아니라, $\phi_k , \pi_k$들이 어떻게 뽑히느냐에 따라 달라지는 random measure라는 점이다.

## 2.3 Inference of Bayesian Finite Mixture Model

우리는 Bayesian finite mixture model의 parameter인 $\pi_k, \phi_k$에 대해 Bayesian inference를 수행하고자 한다. 즉, $\pi_k, \phi_k$의 posterior 분포를 구하고자 한다. 그런데 지금까지 construct한 mixture model의 likelihood는 아래와 같다.

$$
L( \phi, \pi \mid \mathbf x )= \prod_{i=1}^{N} \sum_{k=1}^{K} \pi_k \text{ } p(x_i \mid \phi_k)
$$

이는 $K^N$개의 항을 가지므로 computational cost가 굉장히 높다. 이 문제를 해결하기 위해서 latent variable $z_i$를 도입하여 likelihood를 다르게 표현할 것이다. $z_i$는 $i$번째 observation이 어떤 mixture component에 포함되는 지를 나타내는 latent variable이다. 

$$
z_i \in \{ (1,0,\cdots ,0)^T, (0,1,\cdots ,0)^T, \cdots, (0,0,\cdots ,1)^T \} \subset \mathbb R^k
$$

이 latent variable $z_i$를 도입한 likelihood는 다음과 같다.

$$
\begin{align*}
L( \phi, \pi \mid \mathbf x, \mathbf z ) &= \prod_{i=1}^{N} p(x_i, z_i \mid \phi, \pi)\\
&=\prod_{i=1}^{N} \prod_{k=1}^{K} p(x_i, z_{ik}=1 \mid \phi, \pi)^{z_{ik}} \\  
&= \prod_{i=1}^{N} \prod_{k=1}^{K} p(z_{ik} = 1 \mid \phi, \pi)^{z_{ik}} p(x_i \mid z_{ik}=1 , \phi, \pi)^{z_{ik}} \\
&= \prod_{i=1}^{N} \prod_{k=1}^{K} \pi_k^{z_{ik}} \text{ } p(x_i \mid \phi_k)^{z_{ik}}
\end{align*}
$$

Gibbs sampling를 이용하여 posterior approximation을 수행할 수 있다. posterior을 추정해야하는 parameter들은 아래와 같다.

$$
\phi_1 , \cdots , \phi_K , \pi , z_1, \cdots , z_N 
$$

$\phi_k$의 prior, $G_0$은 conjugate prior로 설정되어 있을 것이다. $\phi_k$의 conditional은 다음과 같이 구할 수 있다. 

$$
\begin{align*}
p( \phi_k \mid \phi_{\backslash k}, \pi, \mathbf z , \mathbf x) &= p( \phi_k \mid \pi, \mathbf z , \mathbf x) \\
&\propto p( \phi_k \mid \pi, \mathbf z ) p(  \mathbf x \mid \pi, \mathbf z , \phi_k) \\
&= p( \phi_k ) p(  \mathbf x \mid \mathbf z , \phi_k) \\
&= p( \phi_k ) \prod_{i=1}^{N} p(  x_i \mid z_i , \phi_k) \\
&\propto p( \phi_k ) \prod_{i:z_{ik}=1} p(  x_i \mid z_i , \phi_k) \\
\end{align*}
$$

첫 번째 항 $p( \phi_k )$는 $\phi_k$의 prior, $G_0$를 의미한다. 두 번째 항 $ \prod_{i:z_{ik}=1} p(  x_i \mid z_i , \phi_k)$은 cluster assign을 결정하는 latent variable $z_i$가 주어졌을 때 $k$번째 cluster에 포함되는 observation만이, 그 식에 $\phi_k$를 포함하고 있다는 것을 의미한다. 예를 들어 만약 이 mixture model이 Gaussian mixture model이었다면, prior $G_0$는 Gaussian-inverse-Wishart prior였을 것이고, conjugacy에 따라 $\phi_k$의 posterior도 update된 parameter의 Gaussian-inverse-Wishart 분포를 따랐을 것이다.

$\pi$의 conditional은 다음과 같이 구할 수 있다. $n_k = \sum_{i=1}^{N}I(z_{ik}=1)$이라고 하자.

$$
\begin{align*}
p(\pi \mid \mathbf x, \mathbf z , \phi) &\propto p(\mathbf x , \mathbf z \mid \phi , \pi) p(\pi \mid \phi) \\
&= L( \phi, \pi \mid \mathbf x, \mathbf z ) p(\pi ) \\
&\propto \left[ \prod_{i=1}^{N} \prod_{k=1}^{K} \pi_k^{z_{ik}} \right]  \prod_{k=1}^{K} \pi_k^{(\alpha_0 /K)-1} \\
&= \prod_{k=1}^{K} \pi_k^{(\alpha_0 /K)+n_k-1} \\
&\sim \text{Dirichlet}(\alpha_0 /K+n_1, \cdots , \alpha_0 /K+n_K)
\end{align*}
$$

$z_i$의 conditional은 다음과 같이 구할 수 있다.

$$
\begin{align*}
p(z_{ik} = 1 \mid \phi, z_{\backslash i}, \pi, \mathbf x) &= p(z_{ik} = 1 \mid \phi, \pi, x_i)\\
&\propto p(z_{ik} = 1 \mid \phi, \pi) p(x_i \mid z_{ik} = 1, \phi, \pi) \\
&= \pi_k \text{ } p(x_i \mid \phi_k, \pi_k) \\
&= \pi_k \text{ } p(x_i \mid \phi_k) \\
\end{align*}
$$

$$
\therefore \enspace p(z_{ik} = 1 \mid \phi, z_{\backslash i}, \pi, \mathbf x) \stackrel{let}{=} p_{ik} = \frac{\pi_k \text{ } p(x_i \mid \phi_k)}{\sum_{l=1}^{K} \pi_l \text{ } p(x_i \mid \phi_l)} ,\enspace \text{ for }k=1, \cdots , K, i= 1, \cdots , N
$$

도출한 conditional을 이용하여 Gibbs sampling을 이용해 Bayesian finite mixture model의 posterior inference를 수행하는 과정은 아래와 같다.

 * Initial value $\phi_1^{(0)} , \cdots , \phi_K^{(0)} , \pi^{(0)} , z_1^{(0)}, \cdots , z_N^{(0)} $를 설정한다.
 * For $k=1, \cdots , K$, $\phi_k$를 update한다. 

$$
\phi_k^{(j+1)} \sim \phi \mid \pi^{(j)}, \mathbf z^{(j)}, \mathbf x
$$

 * $\pi_k$를 update한다. 

$$
\pi^{(j+1)} \sim \pi \mid \phi^{(j+1)}, \mathbf z^{(j)}, \mathbf x \\
\Rightarrow  \text{Dirichlet}(\alpha_0 /K+n_1, \cdots , \alpha_0 /K+n_K)
$$ 

 * For $i=1, \cdots , N$, $z_i$를 update한다. 

$$
z_i^{(j+1)} \sim z_i \mid \phi^{(j+1)}, \pi^{(j+1)}, \mathbf x \\
\Rightarrow  \text{Categorical}(p_{i1}^{(j+1)}, \cdots , p_{iK}^{(j+1)})
$$

 * $j =j+1.$ Repeat until convergence

## 2.4 Model Selection for Finite Mixture Models

Finite mixture model의 개념과 그에 대한 inference에 대해 소개하였다. 지금까지는 mixture component의 개수인 $K$가 고정된 상수라고 가정하고 논의를 진행하였는데, 이 $K$의 최적 값은 어떻게 고를 수 있을까? Cross-validation, bootstrap, AIC, BIC, DIC, MDL, covariance penalties, bridge sampling 등 다양한 방법이 알려져 있다. 그 중에서도 Dirichlet process는 이에 대한 nonparametric Bayesian 관점에서의 대안을 제시하는데, 이와 관련된 내용은 다음 포스트에서 소개하겠다.




# Reference
> Ghosal, Subhashis, and Aad Van der Vaart. Fundamentals of nonparametric Bayesian inference. Vol. 44. Cambridge University Press, 2017.  
> K. Ghosh, J & Ramamoorthi, R. Bayesian Nonparametrics. Springer Series in Statistics. 16. 2011.  
> Talk slide of YeeWhey Teh at The Machine Learning Summer School 2013 at the Max Planck Institute for Intelligent Systems, Tübingen, Germany (http://mlss.tuebingen.mpg.de/2013/2013/slides_teh.pdf)  
> Tutorial lecture of Michael I. Jordan at NIPS'05 (http://faculty.dbmi.pitt.edu/day/Bioinf2132-advanced-Bayes-and-R/Bioinf2132-documents-2017/2017-11-30/nips-tutorial05.pdf)



