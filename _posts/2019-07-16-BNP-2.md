---
layout: post
title: "2. Bayesian Finite Mixture Model"
tags: [Bayesian Nonparametrics]
comments: true
---

# 2.1 Finite Mixture Model

Bayesian nonparametric을 이용한 mixture model을 설명하기에 앞서, 기본적인 Bayesian finite mixture model에 대해서 살펴보고자 한다. Bayesian이 아닌 일반적인 mixture model을 이용한 clustering은 아래와 같은 generative 과정으로 나타낼 수 있다.

 * $\pi = (\pi_1 , \cdots , \pi_K)$의 분포로부터, $K$개의 cluster 중 하나를 고른다. 
 * 선택된 cluster의 확률분포로부터 data를 generate한다.

만약 $K$개의 cluster들이 같은 parametrized family의 분포를 따른다면, 위의 generative process로부터 다음과 같은 finite mixture model을 얻을 수 있다.

$$
p(x \mid \phi, \pi)= \sum_{k=1}^{K} \pi_k \text{ } p(x \mid \phi_k)
$$

예를 들면, finite Gaussian mixture의 경우, $\phi_k = (\mu_k, \Sigma_k)$이고 $ p(x \mid \phi_k)$는 $(\mu_k, \Sigma_k)$를 mean과 covariance로 갖는 gaussian distribution일 것이다.

$$
p(x \mid (\mu, \Sigma), \pi)= \sum_{k=1}^{K} \pi_k \text{ } \mathcal N(x \mid \mu_k, \Sigma_k)
$$

Bayesian mixture model로의 확장을 위해서 이를 아래와 같이 다른 방법으로 나타내보자. 다음과 같은 probability measure $G$를 정의한다.

$$
G = \sum_{k=1}^{K} \pi_k \text{ } \delta_{\phi_k} , \text{ where } \delta_{\phi_k} \text{ is a dirac measure given } \phi_k
$$

즉 $G$는 $\phi_k$에 $\pi_k$의 확률값을 부여하는 probability measure이다. $i=1,\cdots , N$에 대해, 다음과 같은 sample generating process를 생각해보자. 

![image](https://user-images.githubusercontent.com/45325895/61167998-d0906900-a581-11e9-86bf-bc29416d2c3f.png){: .center-image}

$$
\begin{align*}
\theta_i &\sim G \\
x_i &\sim p(\cdot \mid \theta_i)
\end{align*}
$$


각 $\theta_i$는 $\{ \phi_1 ,\cdots , \phi_K \}$ 중 하나의 값을 가지며, 같은 $\phi_k$의 값을 갖는 $\theta_i$들의 집합을 $k$th cluster로 이해할 수 있다.  

# 2.2 Bayesian Finite Mixture Model

이제 finite mixture model의 parameter $\phi_1, \cdots , \phi_K , \pi_1, \cdots , \pi_K$에 prior distribution을 부여하고자 한다. 여기서도 각 cluster들은 같은 parametric family의 분포를 갖는다고 가정하자.  
  
먼저 $\phi_k$에 대한 prior distribution을 생각해보자. 각 cluster distribution의 parameter인 $\phi_k$에 대한 prior distribution은 그 cluster가 어떤 parametric family의 분포를 갖는지에 따라 결정된다. 예를 들면 Gaussian mixture model의 경우는, conjugacy를 위해 mean $\mu_k$와 covariance $\Sigma_k$에 대해 normal/inverse-gamma prior를 부여할 것이다. 이를 $\phi_k \sim G_0$로 나타내자.  
  
Cluster probability인 $\pi_k$에 대한 prior distribution은 symmetric Dirichlet prior를 부여할 수 있다.

$$
(\pi_1, \cdots , \pi_K) \sim \text{Dirichlet}(\alpha_0 / K , \cdots , \alpha_0 / K )
$$

그 이유는 다음과 같다.

 * $(\pi_1, \cdots , \pi_K)$는 데이터가 어느 cluster에 assign되는지에 대한 확률이므로, 합해서 $1$이 되어야 한다.
 * Symmetry는 각 mixture component(cluster)의 labeling을 permutation하더라도 model이 달라지지 않도록 하는 역할을 한다.
 * Dirichlet distribution의 parameter들은 concentration parameter의 역할을 한다.
	 * Concentration parameter는 각 확률변수의 값이 얼마나 고르게 분포해있는지를 결정하는 parameter이다. Concentration parameter의 값이 클 수록 서로 유사한 값이 나올 확률이 크며, 그 값이 작을 수록 확률변수가 더 sparse한 분포를 갖는다.
 * Dirichlet distribution의 parameter를 mixture component의 수, $K$로 나누어 준 것은 Dirichlet parameter의 총 합($\alpha_0$)을 concentration parameter로 정의하는 convention을 따른 것이다. 이 경우, concentration parameter가 $\alpha_0=K$일 때 Dirichlet distribution이 uniform distribution over $K-1$ simplex가 된다.

Bayesian finite mixture model의 sample generating process는 다음과 같이 나타낼 수 있다.

![image](https://user-images.githubusercontent.com/45325895/61168633-089ca980-a58c-11e9-9c00-ac3de1a430ed.png){: .center-image}

$$
\begin{align*}
\phi_k &\sim G_0 \\
\pi &\sim \text{Dirichlet}(\alpha_0 / K , \cdots , \alpha_0 / K) \\
G &= \sum_{k=1}^{K} \pi_k \text{ } \delta_{\phi_k} \\
\theta_i &\sim G \\
x_i &\sim p(\cdot \mid \theta_i)
\end{align*}
$$

여기서 짚고 넘어갈 부분은 $i$번째 observation의 cluster를 결정하는 probability measure $G = \sum_{k=1}^{K} \pi_k \text{ } \delta_{\phi_k}$이 위의 finite mixture model에서처럼 고정된(deterministic) measure가 아니라, $\phi_k , \pi_k$들이 어떻게 뽑히느냐에 따라 달라지는 random measure라는 점이다.

# 2.3 Inference of Bayesian Finite Mixture Model

우리는 Bayesian finite mixture model의 parameter인 $\pi_k, \phi_k$에 대해 Bayesian inference를 수행하고자 한다. 즉, $\pi_k, \phi_k$의 posterior 분포를 구하고자 한다. 그런데 지금까지 construct한 mixture model의 likelihood는 아래와 같다.

$$
L( \phi, \pi \mid \mathbf x )= \prod_{i=1}^{N} \sum_{k=1}^{K} \pi_k \text{ } p(x_i \mid \phi_k)
$$

이는 $K^N$개의 항을 가지므로 computational cost가 굉장히 높다. 이 문제를 해결하기 위해서 latent variable $z_i$를 도입하여 likelihood를 다르게 표현할 것이다. $z_i$는 $i$번째 observation이 어떤 mixture component에 포함되는 지를 나타내는 latent variable이다. 

$$
z_i \in \{ (1,0,\cdots ,0)^T, (0,1,\cdots ,0)^T, \cdots, (0,0,\cdots ,1)^T \} \subset \mathbb R^k
$$

이 latent variable $z_i$를 도입한 likelihood는 다음과 같다.

$$
\begin{align*}
L( \phi, \pi \mid \mathbf x, \mathbf z ) &= \prod_{i=1}^{N} p(x_i, z_i \mid \phi, \pi)\\
&=\prod_{i=1}^{N} \prod_{k=1}^{K} p(x_i, z_{ik}=1 \mid \phi, \pi)^{z_{ik}} \\  
&= \prod_{i=1}^{N} \prod_{k=1}^{K} p(z_{ik} = 1 \mid \phi, \pi)^{z_{ik}} p(x_i \mid z_{ik}=1 , \phi, \pi)^{z_{ik}} \\
&= \prod_{i=1}^{N} \prod_{k=1}^{K} \pi_k^{z_{ik}} \text{ } p(x_i \mid \phi_k)^{z_{ik}}
\end{align*}
$$

Gibbs sampling를 이용하여 posterior approximation을 수행할 수 있다. posterior을 추정해야하는 parameter들은 아래와 같다.

$$
\phi_1 , \cdots , \phi_K , \pi , z_1, \cdots , z_N 
$$

$\phi_k$의 prior, $G_0$은 conjugate prior로 설정되어 있을 것이다. $\phi_k$의 conditional은 다음과 같이 구할 수 있다. 

$$
\begin{align*}
p( \phi_k \mid \phi_{\backslash k}, \pi, \mathbf z , \mathbf x) &= p( \phi_k \mid \pi, \mathbf z , \mathbf x) \\
&\propto p( \phi_k \mid \pi, \mathbf z ) p(  \mathbf x \mid \pi, \mathbf z , \phi_k) \\
&= p( \phi_k ) p(  \mathbf x \mid \mathbf z , \phi_k) \\
&= p( \phi_k ) \prod_{i=1}^{N} p(  x_i \mid z_i , \phi_k) \\
&\propto p( \phi_k ) \prod_{i:z_{ik}=1} p(  x_i \mid z_i , \phi_k) \\
\end{align*}
$$

첫 번째 항 $p( \phi_k )$는 $\phi_k$의 prior, $G_0$를 의미한다. 두 번째 항 $ \prod_{i:z_{ik}=1} p(  x_i \mid z_i , \phi_k)$은 cluster assign을 결정하는 latent variable $z_i$가 주어졌을 때 $k$번째 cluster에 포함되는 observation만이, 그 식에 $\phi_k$를 포함하고 있다는 것을 의미한다. 예를 들어 만약 이 mixture model이 Gaussian mixture model이었다면, prior $G_0$는 Gaussian-inverse-Wishart prior였을 것이고, conjugacy에 따라 $\phi_k$의 posterior도 update된 parameter의 Gaussian-inverse-Wishart 분포를 따랐을 것이다.

$\pi$의 conditional은 다음과 같이 구할 수 있다. $n_k = \sum_{i=1}^{N}I(z_{ik}=1)$이라고 하자.

$$
\begin{align*}
p(\pi \mid \mathbf x, \mathbf z , \phi) &\propto p(\mathbf x , \mathbf z \mid \phi , \pi) p(\pi \mid \phi) \\
&= L( \phi, \pi \mid \mathbf x, \mathbf z ) p(\pi ) \\
&\propto \left[ \prod_{i=1}^{N} \prod_{k=1}^{K} \pi_k^{z_{ik}} \right]  \prod_{k=1}^{K} \pi_k^{(\alpha_0 /K)-1} \\
&= \prod_{k=1}^{K} \pi_k^{(\alpha_0 /K)+n_k-1} \\
&\sim \text{Dirichlet}(\alpha_0 /K+n_1, \cdots , \alpha_0 /K+n_K)
\end{align*}
$$

$z_i$의 conditional은 다음과 같이 구할 수 있다.

$$
\begin{align*}
p(z_{ik} = 1 \mid \phi, z_{\backslash i}, \pi, \mathbf x) &= p(z_{ik} = 1 \mid \phi, \pi, x_i)\\
&\propto p(z_{ik} = 1 \mid \phi, \pi) p(x_i \mid z_{ik} = 1, \phi, \pi) \\
&= \pi_k \text{ } p(x_i \mid \phi_k, \pi_k) \\
&= \pi_k \text{ } p(x_i \mid \phi_k) \\
\end{align*}
$$

$$
\therefore \enspace p(z_{ik} = 1 \mid \phi, z_{\backslash i}, \pi, \mathbf x) \stackrel{let}{=} p_{ik} = \frac{\pi_k \text{ } p(x_i \mid \phi_k)}{\sum_{l=1}^{K} \pi_l \text{ } p(x_i \mid \phi_l)} ,\enspace \text{ for }k=1, \cdots , K, i= 1, \cdots , N
$$

도출한 conditional을 이용하여 Gibbs sampling을 이용해 Bayesian finite mixture model의 posterior inference를 수행하는 과정은 아래와 같다.

 * Initial value $\phi_1^{(0)} , \cdots , \phi_K^{(0)} , \pi^{(0)} , z_1^{(0)}, \cdots , z_N^{(0)} $를 설정한다.
 * For $k=1, \cdots , K$, $\phi_k$를 update한다. 

$$
\phi_k^{(j+1)} \sim \phi \mid \pi^{(j)}, \mathbf z^{(j)}, \mathbf x
$$

 * $\pi_k$를 update한다. 

$$
\pi^{(j+1)} \sim \pi \mid \phi^{(j+1)}, \mathbf z^{(j)}, \mathbf x \\
\Rightarrow  \text{Dirichlet}(\alpha_0 /K+n_1, \cdots , \alpha_0 /K+n_K)
$$ 

 * For $i=1, \cdots , N$, $z_i$를 update한다. 

$$
z_i^{(j+1)} \sim z_i \mid \phi^{(j+1)}, \pi^{(j+1)}, \mathbf x \\
\Rightarrow  \text{Categorical}(p_{i1}^{(j+1)}, \cdots , p_{iK}^{(j+1)})
$$

 * $j =j+1.$ Repeat until convergence

# 2.4 Model Selection for Finite Mixture Models

Finite mixture model의 개념과 그에 대한 inference에 대해 소개하였다. 지금까지는 mixture component의 개수인 $K$가 고정된 상수라고 가정하고 논의를 진행하였는데, 이 $K$의 최적 값은 어떻게 고를 수 있을까? Cross-validation, bootstrap, AIC, BIC, DIC, MDL, covariance penalties, bridge sampling 등 다양한 방법이 알려져 있다. 그 중에서도 Dirichlet process는 이에 대한 nonparametric Bayesian 관점에서의 대안을 제시하는데, 이와 관련된 내용은 다음 포스트에서 소개하겠다.

