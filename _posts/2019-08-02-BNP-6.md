---
layout: post
title: "4. Dirichlet Process Mixture Model"
tags: [Bayesian Nonparametrics]
comments: true
---

## 4.1 Going Nonparametric

먼저 다음 두 개념의 차이에 대해 소개하고자 한다.

 * (Latent) Component : 잠재적(latent) 그룹.
 * Cluster : 우리가 보유한 자료에 실현된(realized) component.

총 1000개의 종이 있는 나비와 같이 실제 *latent component*의 갯수가 1000개인 data generating process를 예시로 생각해보자. 실제로는 1000개의 종이 있지만, 이는 latent structure이고 우리는 관측된 자료에 실현된 *component*, 즉 cluster만을 관측할 뿐이다. 만약 100마리의 나비를 채집했을 때 총 25종의 나비가 관측되었다면, 우리의 데이터에는 25개의 *cluster*가 존재하는 것이다. 이 때 mixture model을 이용해 자료를 분석한다면, *cluster*의 갯수 $K$는 $25$가 되어야할 것이다. 이후 100마리의 나비를 추가로 채집한다면, 처음 관측했던 25 종류의 나비에 더해 추가로 관측되는 종들이 있을 것이다. 또한 채집한 나비의 수가 점점 증가할 수록, 관측된 자료로 실현된 *latent component*, 즉 *cluster*의 수는 점점 실제 *latent component*의 수에 수렴할 것이다.  
  
Finite mixture model에서는 mixture component가 고정된 상수라고 가정한다. 이 가정은 위 예시의 상황과 같이 데이터가 늘어남에 따라 mixture component 역시 늘어나는 상황에서는 적절하지 못한 가정이 될 것이다. 또다른 예로는 Topic modeling의 경우를 들 수 있다. 우리의 자료가 더 많은 단어 및 문서를 포함할 수록, 실현된 latent component, 즉 cluster의 수는 늘어날 것이다.  
  
그렇다면 충분히 큰 상수 $K$를 mixture component의 개수로 사전에 설정하는 것은 어떨까? 우리는 자료가 생성되는 실제 latent structure에 대한 정보를 알 수 없으므로, 얼마나 큰 $K$가 충분히 큰 것인지 알 수 있는 방법이 없다. $K$가 충분히 크도록 하기 위해서 $K$를 $\infty$로 보내는 것은 어떨까? Finite mixture modeling에서처럼 $G = \sum_{k=1}^{\infty} \pi_k \text{ } \delta_{\phi_k} $에서 $\theta_i$를 생성하는 것이다. 그를 위해서는 $\sum^{\infty}_1 \pi_k = 1$인 $\pi_k$를 construct할 방법이 필요하다.

## 4.2 Stick-breaking Process

위의 [3.3.5 Stick-Breaking Representation(Sethuraman Representation)]()에서도 소개했던 Dirichlet process의 Sethuraman representation이다. 
다음과 같이 Beta 분포를 따르는 확률변수들의 infinite sequence, $\{ \beta_k \}_1^\infty$에 대해 생각해보자.

$$
\beta_k \stackrel{\text{iid}}{\sim} \text{Beta}(1, \alpha_0) \text{ , } \enspace k=1,2, \cdots 
$$

그리고 이로부터 다음과 같이 확률변수 $\pi_k$를 정의하자.

$$
\begin{align*}
\pi_1 &= \beta_1 \\
\pi_k &= \beta_k \prod_{\ell=1}^{k-1} (1-\beta_\ell) \text{ , } \enspace k=2,3, \cdots 
\end{align*}
$$

즉 확률변수 $\pi_k$는 길이가 1인 *stick*을 다음과 같이 쪼개는 것으로 이해할 수 있다.

$$
\beta_1 + \beta_2(1-\beta_1) + \beta_3[1-\{\beta_1 + \beta_2(1-\beta_1) \}] + \cdots = 1
$$

$\sum \pi_k = 1$(with probability $1$)이라는 것은 다음과 같이 쉽게 확인할 수 있다.

$$
\begin{align}
1-\sum_{k=1}^{\infty} \pi_k  &= 1-\beta_1 - \beta_2(1-\beta_1) - \beta_3(1-\beta_1)(1-\beta_2) + \cdots  \\
&= (1-\beta_1)[ 1 - \beta_2 - \beta_3(1-\beta_2) + \cdots ] \\
&= \prod_{k=1}^{K} (1-\beta_k) \\
&\longrightarrow 0 \enspace \enspace \text{ almost surely, as }K \rightarrow 0
\end{align}
$$

즉 이는 $[0,1]$을 나누는 infinite partition, $\pi_k$를 construct한 것이다. 이 weight의 sequence의 분포를 다음과 같이 나타낸다.

$$
\pi = (\pi_1, \pi_2 , \cdots) \sim \text{GEM}(\alpha_0)
$$

이제 $\text{GEM}(\alpha_0)$를 따르는 $\pi$로부터 random measure $G = \sum_{k=1}^{\infty} \pi_k \text{ } \delta_{\phi_k} $를 정의할 수 있다. 확률변수 $\pi_1, \pi_2 , \cdots$는 $\alpha_0$의 값에 따라 다르게 생성되는데, 이에 대해서 확인하고 넘어가자.

![image](https://user-images.githubusercontent.com/45325895/61171296-a7d39800-a5b0-11e9-9d79-ff4e2f16b7a6.png){: .center-image}
![image](https://user-images.githubusercontent.com/45325895/61171307-c5086680-a5b0-11e9-937f-4307efd67ef1.png){: .center-image}

위 두 그림은 각각 $\text{Beta}(1,0.5)$와 $\text{Beta}(1,10)$의 그래프이다. $\alpha_0$ 값이 작을 수록 $\beta_k \sim \text{Beta}(1, \alpha_0)$는 1에 가까운 값을 가질 확률이 더 높다는 것을 알 수 있다. 또한 그에 따른 $\pi_k$는 $\alpha_0$ 값이 큰 경우와 비교했을 때 상대적으로, 처음 몇 항의 합이 $[0,1]$의 대부분을 차지할 것이다.


Stick-breaking process를 이용하여 $[0,1]$의 infinite partition을 얻는 과정을 알아보았다. 또한, 이를 통해 얻은 $\pi \sim \text{GEM}(\alpha_0)$를 infinite component에 대한 mixture probability로 사용하여 다음과 같이 random measure $G$를 정의할 수 있다.

$$
G = \sum_{k=1}^{\infty} \pi_k \text{ } \delta_{\phi_k} \sim \text{DP}( \alpha_0 G_0)
$$

이 때의 mixture model을 나타내면 아래와 같다. 

![image](https://user-images.githubusercontent.com/45325895/61171579-a1dfb600-a5b4-11e9-9611-938170ff6584.png){: .center-image}

$$
\begin{align*}
\phi_k &\sim G_0 \text{ , } \enspace k = 1,2,\cdots\\
\pi &\sim \text{GEM}(\alpha_0)\\
G \mid \pi, \phi  &= \sum_{k=1}^{\infty} \pi_k \text{ } \delta_{\phi_k} 
\\
\theta_i \mid G &\sim G \\
x_i  \mid \theta_i &\sim p(\cdot \mid \theta_i)
\end{align*}
$$

[2.2 Bayesian Finite Mixture Model](https://lee-jaejoon.github.io/BNP-1/#22-bayesian-finite-mixture-model)에서는 mixture probability, $\pi =(\pi_1, \cdots , \pi_K)$를 $K$차원 Dirichlet 분포로 prior를 주었던 것에 반해, 여기서는 무한 개의 mixture component에 대해 stick-breaking을 이용해 construct한 Dirichlet process를 이용하여 mixture probability에 대한 prior를 준 것이다. 따라서 위 그림은 다음 그림과 같은 모형을 나타낸다.


![image](https://user-images.githubusercontent.com/45325895/62107978-d1482f80-b2e3-11e9-9a14-7eaf79cab74d.png){: .center-image}

$$
\begin{align*}

G &\sim \text{DP}(\alpha_0 G_0)
\\
\theta_i \mid G &\sim G \\
x_i \mid \theta_i &\sim p(\cdot \mid \theta_i)
\end{align*}
$$



