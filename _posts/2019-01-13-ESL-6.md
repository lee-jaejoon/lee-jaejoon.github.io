---
layout: post
title: "ESL: Ch 6. Kernel Smoothing Methods"
tags: [Elements of Statistical Learning]
comments: true
---
> Contents  
> [6.1 One-Dimensional Kernel Smoothers](#61-one-dimensional-kernel-smoothers)  
> [6.2 Selecting the Width of the Kernel](#62-selecting-the-width-of-the-kernel)  
> [6.3 Local Regression in $\mathbb{R}^p$](#63-local-regression-in-mathbbrp)  
> [6.4 Structured Local Regression Models in $\mathbb{R}^p$](#64-structured-local-regression-models-in-mathbbrp)  
> [6.5 Local Likelihood and Other Models](#65-local-likelihood-and-other-models)  
> [6.6 Kernel Density Estimation and Classification](#66-kernel-density-estimation-and-classification)  
> [6.7 Radial Basis Functions and Kernels](#67-radial-basis-functions-and-kernels)  
> [6.8 Mixture Models for Density Estimation and Classification](#68-mixture-models-for-density-estimation-and-classification)  
> [6.9 Computational Considerations](#69-computational-considerations)  

<br>

이 챕터에서는 $p$개의 input feature들($X \in \mathbb{R}^p$)에 대해 정의된 true regression function $f(X)$를 좀 더 flexible하게 추정하는 방법에 대해 소개한다. 간단히 소개하자면, 가까운 점들만을 이용하거나 가까운 점일 수록 더 큰 영향을 주도록 하는 **Localization**을 이용할 것이며, 추정된 $\hat{f}(X)$가 $\mathbb{R}^p$ 상의 부드러운(smooth) 곡선이 되도록 할 것이다. **Localization**은 **Kernel**이라는 일종의 가중치를 부여하는 함수를 이용하여 달성할 것이다. 이와 같은 방식으로 regression function $f(X)$를 추정하는 방법을 **Kernel smoothing method**라고 한다.   
<br>

# 6.1 One-Dimensional Kernel Smoothers
이 장에서는 input feature가 한 개의 변수로 이루어진 경우의 Kernel smoothing method에 대해 소개한다. 주변 점들에 **weight를 부여**햐는 **kernel** 함수에 대한 감을 얻기 위해, 먼저 $k$-Nearest-Neighbor average를 이용해 regression function $f(X)$를 추정하는 방법을 살펴보자. 

## $k$-Nearest Neighbor Average
$x=x_0$에서의 $k$-Nearest-Neighbor average는 다음과 같이 정의된다.

$$
\hat{f}(x_0)=Ave(y_i \mid x_i \in N_k(x_0))
$$

$N_k(x)$는 training set 내의 점들 중, $x$로부터 가장 가까운 $k$개의 점의 집합을 의미한다. 즉, $x=x_0$에서의 $k$-Nearest-Neighbor average는 $x_0$로부터 가장 가까운 $k$개의 점들의 $y$값을 평균낸 값으로 구한다. 이를 정의역 내 모든 $x$ 값에 대해 수행하여 regression function $f(x)$를 추정하는 것이다. 아래 그림은 $30$-Nearest Neighbor average를 수행하여 true function $f(x)$를 추정한 그림이다. 즉, $x_0$로부터 가장 가까운 $30$개의 점들의 $y$값을 평균낸 값으로 $x_0$에서의 fitted value를 구한 것이다.

![image](https://user-images.githubusercontent.com/45325895/51550119-101da880-1eaf-11e9-849f-4112101c7927.png){: .center-image}

초록색 곡선이 $30$-Nearest Neighbor average로 도출한 regression function의 추정치, $\hat{f}(x)$이다. 빨간색으로 표시된 점은 $x_0$에서 가장 가까운 점 30개에 포함된 점, 즉 $\hat{f}(x_0)$ 계산에 영향을 준 점들인데, 높이가 평평한 노랑색 직사각형으로 표시된 것은 $N_{30}(x_0)$ 안에서는 동등한 weight로 $\hat{f}(x_0)$ 계산에 영향을 주었다는 것을 의미한다. 이 부분은 이후의 다른 모형과 비교를 할 때 더 설명하겠다. 그런데 $k$-Nearest Neighbor average는 위에서와 같이 불연속점이 많고 울퉁불퉁하다는 특징이 있다. 이는 $k$-Nearest Neighborhood에 새로운 점이 들어오고 한 점이 나가는 순간마다, $\hat{f}(x)$에의 영향에 대한 weight가 불연속적으로 변하기 때문이다.  

## Nadaraya-Watson Kernel-weighted Average

가장 간단한 Kernel smoother라고 할 수 있는 Nadaraya-Watson kernel-weighted average에 대해 살펴보자. $x=x_0$에서의 N-W kernel-weighted average는 다음과 같이 정의된다. $ \frac{K_\lambda (x_0,x_i)}{\sum_{i=1}^{N} K_\lambda (x_0,x_i)}$는 training set 내의 $N$개의 점들이 $x=x_0$일 때의 fitted value에 영향을 주는 정도를 나타내는 weight가 되고, 그 weight에 따라 가중평균하는 방식으로 fitted value를 결정하게 된다.

$$
\hat{f}(x_0)= \sum_{i=1}^{N} \frac{K_\lambda (x_0,x_i)}{\sum_{i=1}^{N} K_\lambda (x_0,x_i)} y_i
$$

$$
K_\lambda (x_0,x)=D \Big( \frac{\mid x-x_0\mid}{\lambda} \Big)
$$

$$
D(t)=\begin{cases}
       \frac{3}{4} (1-t^2) &\quad\text{if  } |t| \le 1\\
       0 &\quad\text{otherwise} \\
     \end{cases}
$$

여기서 **Kernel**, $K_\lambda (x_0,x_i)$는 $x_i$가 $x_0$와 가까울 수록 큰 값을 갖고, $x_0$로부터 멀 수록 작은 값을 갖도록 만들어진 다음과 같은 함수이다. Kernel을 어떤 함수로 설정하느냐에 따라 N-W kernel-weighted average의 모양이 달라지는데, 아래의 그림에서는 위의 식과 같은 Epanechnikov kernel을 kernel로 설정하였다. 식을 보면 $x$가 $x_0$로부터 멀 수록 $K_\lambda(x_0,x)$의 값이 작아지는 것을 확인할 수 있다. 또한 kernel 함수 $K_\lambda (x_0,x_i)$는 연속함수이고, 그에 따라 weighted average의 weight도 연속함수이기 때문에, $k$-NN으로 도출한 $\hat{f}(x)$와는 달리 **smooth**한 특징을 보다.

![image](https://user-images.githubusercontent.com/45325895/51550146-2166b500-1eaf-11e9-9c64-18ae9e8fe84f.png){: .center-image}

빨간색으로 표시된 점은 위의 $30$-NN의 그림과 같이, $x=x_0$에서의 fitted value $\hat{f}(x_0)$의 계산에 영향을 준 점들인데, $30$-NN보다 더 많은 점들이 고려된 것을 알 수 있다. 다만 노랑색으로 색칠된 부분이 나타내듯, 가까운 점일 수록 더 높은 가중치로 반영이 된 것을 확인할 수 있다. (그림 상의 노랑색 면적의 모양은 $D(t)$의 모양과 동일하다.) 그런데, 위의 식을 잘 살펴보면 Kernel 함수 안에 있는 $\lambda$가 Kernel이 양의 값을 갖는 영역에 영향을 주는 것을 확인할 수 있다. 위의 그림은 $\lambda=0.2$를 사용해서 그린 그림이다. 여기서 $\lambda$의 값을 증가시키면 더 많은 점들이 $ \mid \frac{\mid x-x_0\mid}{\lambda} \mid \le 1$을 만족하여, 더 많은 점들에서 $K_\lambda (x_0,x)$의 값이 양수가 될 것이고, 한 점 $x=x_0$에서의 fitted value를 계산할 때 더 많은 점들의 함수값을 고려하게 될 것이다. 다시 말해서, $\lambda$는 주어진 Kernel에서 **neighborhood의 너비**를 결정하는 parameter이다.  

위의 예시에서는 fitted value를 구할 때 모두 동일한 neighborhood 너비를 적용했지만, 이를 $x_0$에 대한 함수로 나타내어, 각 $x_0$값들에 대해 서로 다른 neighborhood 너비를 부여할 수도 있다. 그 때 Kernel 함수의 식은 아래와 같다.

$$
K_\lambda (x_0,x)=D \Big( \frac{\mid x-x_0\mid}{h_\lambda (x_0)} \Big)
$$

neighborhood 너비를 결정하는 parameter $\lambda$가 변함에 따라, kernel smoother $\hat{f}(x)$는 어떤 영향을 받을까? 모형의 bias와 variance의 측면에서 생각해보자. **$\lambda$의 값이 크다면,** fitted value $\hat{f}(x_0)$의 값을 결정할 때 $x_0$로부터 더 먼 점들까지 고려하게 된다. **더 많은 점들에 대해 평균**을 내어 fitted value를 결정하기 때문에, $\hat{f}(x)$의 **variance는 낮아질 것이다.** 또한, 우리는 $x=x_0$에 대해 neighborhood를 설정하고, 그 neighborhood 내에서는 true function이 constant라고 가정한 채 $y_i=f(x_i)$들의 average를 구한 뒤, 이를 $x=x_0$일 때의 $\hat{f}$값으로 설정한다. **$x=x_0$로부터 먼 $x_i$일 수록 $y_i=f(x_i)$가 $f(x_0)$와 가깝다는 보장이 없기 때문**에, $\hat{f}(x)$는 neighborhood가 넓을 수록 **높은 bias를 가질 것**이다. 반대로 $\lambda$의 값이 작다면, $\hat{f}(x)$의 variance는 더 높을 것이고, bias는 낮아질 것이다. 따라서 우리는 Kernel의 parameter $\lambda$의 최적 값을 결정해주어야 한다.  

모든 $x$값에 대해 동일한 neighborhood 너비를 적용하는 경우( $h_\lambda (x_0)=\lambda$ ), Bias는 fitted value를 구하고자 하는 $x_0$ 주변의 training data의 density에는 그다지 영향을 받지 않는다. 다만 같은 크기의 neighborhood 너비를 사용한다면, training data가 더 많이 모여있는 곳(high density)에서는 training data가 드문드문한 곳(low density)보다 더 많은 점들의 함수값을 평균내서 fitted value를 구할 것이고, 그때의 estimate $\hat{f}(x_0)$는 variance가 상대적으로 낮을 것이다. 따라서, 어떤 한 점에서의 estimate $\hat{f}(x_0)$의 variance는 그 점 $x=x_0$에서의 density와 역비례관계(inversely proportional)에 있다.  
<br>


## 6.1.1 Local Linear Regression

우리는 간단한 moving average인 $k$-nearest neighbor 방법을 먼저 소개했고, 이 $k$-NN 방법은 weight가 불연속적으로 변해 그 fitted value의 결과 또한 울퉁불퉁한 단점이 있었다. 여기서, 한 점 $x_0$를 기준으로 그로부터 가까운 점일 수록 더 큰 값을 갖는 **Kernel 함수**와 그를 이용한 weight를 도입함으로써, 우리는 weight가 연속적으로 움직여 smooth한 $\hat{f}$를 도출하는 **locally weighted average**를 통한 **Kernel smoother**를 학습하였다. 그런데 위의 N-W average와 같은 kernel smoother는 여전히 문제가 있다.

![image](https://user-images.githubusercontent.com/45325895/51553539-71954580-1eb6-11e9-81c6-e1d4c277e5ec.png){: .center-image}

Training set의 boundary의 근처의 점 $x_0$에서의 fitted value를 구할 때, 위의 그림과 같은 경우처럼, Kernel이 양의 값을 갖는 구간의 한 쪽이 boundary 밖으로 나가게 되어 **kernel함수가 비대칭성**을 갖게 되고, 그 결과로 boundary에 가까운 구간에서는 **비대칭적으로 weight가 부여되는 현상**이 발생한다. 위 경우는 파란색으로 그려진 true function $f$가 $x=x_0$ 주변에서 증가함수이고, $x_0$를 기준으로 오른쪽에 있는 점들이 비대칭적으로 더 높은 가중치를 부여받았기 때문에, $x=x_0$에서의 fitted value $\hat{f}(x_0)$가 true function $f$보다 위쪽으로 biased 되어있는 것을 확인할 수 있다.  

**Local linear regression**은 이와 같은 **kernel asymmetry에 따른 boundary 부근의 bias 발생 문제를 해결**하기 위해 고안되었다. N-W average와 같은 Local average 방법은, fitted value를 구하고자 하는 $x=x_0$에 대해 neighborhood를 설정하고, 그 neighborhood 내에서는 true function $f$가 constant라고 가정한 채 average를 구한 뒤, 이를 $x=x_0$일 때의 $\hat{f}$값으로 설정하는 작업을 반복한다. 하지만 true function $f$는 해당 neighborhood 내에서 constant가 아니기에, boundary 근처에서 kernel이 비대칭이 되면 bias가 생기는 것이다. **Local linear regression**은 이 **bias를 first-order까지 완벽하게 보정**한다. 이 부분에 대한 설명은 아래에서 더 하기로 하고, 먼저 local linear regression을 이용하여 kernel smoother를 구하는 방법에 대해 알아보자.
  
 * fitted value를 구하고자 하는 점, $x_0$를 기준으로 training data $(x_1,y_1)$, $...$ ,$(x_N,y_N)$에 대한 Kernel 함수값 $K_\lambda(x_0,x_1)$ , $...$ ,$K_\lambda(x_0,x_N)$를 구한다.
 * 각 점에서의 Kernel 함수값 $K_\lambda(x_0,x_i)$를 Kernel 값들의 총합으로 나눈 값이 $x=x_0$의 fitted value를 구하기 위한 각 training data 점들의 weight이다.
 * N-W average와 같은 Local kernel-weighted average 방법에서는 이 weight값들을 이용해 $y_1,...,y_N$를 가중평균한 값이 $x=x_0$의 fitted value, $\hat{f}(x_0)$였다. Local linear regression에서는 이 weight값들을 이용해 $(x_1,y_1)$, $...$ ,$(x_N,y_N)$에 대한 **[weighted least squares](https://en.wikipedia.org/wiki/Weighted_least_squares)**를 수행한다.
 * weighted least squares로 구한 회귀선이 $x=x_0$에서 갖는 값을 $x=x_0$의 fitted value, $\hat{f}(x_0)$로 설정한다.
 * 이와 같은 방법을 정의역 내 모든 $x$에 대해 수행하여 얻은 함수 $\hat{f}(x)$가 Local linear regression으로 얻은 Kernel smoother이다.  
  
따라서 이를 식으로 나타내면, $x=x_0$의 fitted value, $\hat{f}(x_0)$를 구하는 것은 아래와 같은 최소화 문제를 푸는 것과 같다.  

$$
\Big( \hat{\alpha}(x_0),\hat{\beta}(x_0) \Big)=\min_{\big( \alpha(x_0),\beta(x_0) \big)} \sum_{i=1}^{N} \frac{K_\lambda (x_0,x_i)}{\sum_{i=1}^{N} K_\lambda (x_0,x_i)} [y_i-\alpha(x_0)-\beta(x_0)x_i]^2
$$

$$
=\min_{\big( \alpha(x_0),\beta(x_0) \big)} \sum_{i=1}^{N} K_\lambda (x_0,x_i) [y_i-\alpha(x_0)-\beta(x_0)x_i]^2
$$

$$
\Rightarrow \enspace \enspace \hat{f}(x_0)=\hat{\alpha}(x_0)+\hat{\beta}(x_0)x_0
$$

Weighted least square의 수행 결과를 행렬과 벡터 표기를 이용하여 나타내면 $ \hat{f}(x_0)$는 아래와 같다. $\mathbf{B}$는 design matrix 역할을 하는 $N \times 2$ 행렬이고, $\mathbf{W}(x_0)$는 N개의 weight들(Kernel 값)을 대각항에 둔 $N \times N$ 대각행렬이다.

$$
b(x)^T=[1 \enspace x] \enspace , \enspace \enspace \mathbf{B}=

\begin{bmatrix} 
\enspace & b(x_1)^T & \enspace \\
\enspace & \vdots & \enspace \\
\enspace & b(x_N)^T & \enspace \\ 
\end{bmatrix}

\enspace , \enspace \enspace
\mathbf{W}(x_0)=
 \begin{bmatrix}
  K_\lambda (x_0,x_1) & \enspace & \mathbf{0} \\
  \enspace & \ddots  & \enspace \\
  \mathbf{0} & \enspace  & K_\lambda (x_0,x_N)  \\
 \end{bmatrix}
$$

$$
\Rightarrow \enspace \enspace \hat{f}(x_0)=b(x_0)^T \big( \mathbf{B}^T \mathbf{W}(x_0) \mathbf{B} \big)^{-1} \mathbf{B}^T \mathbf{W}(x_0)y 
$$

$$
\Rightarrow \enspace \enspace \hat{f}(x_0)=\sum_{i=1}^{N} l_i(x_0)y_i
$$

위 식들의 마지막 형태, $\hat{f}(x_0)=\sum_{i=1}^{N} l_i(x_0)y_i$는 local linear regression으로 얻은 ($x=x_0$에서의) true function $f$의 estimate **$\hat{f}(x_0)$가 $y_1$, $...$ , $y_N$에 대해 linear하다**는 것을 의미한다. 다시 말해서, $x=x_0$에 대해 각 training data point $(x_1,y_1)$, $...$ ,$(x_N,y_N)$의 weight를 $l_1(x_0)$, $...$ , $l_N(x_0)$로 새롭게 정의하고, 그 weight들을 이용해 $y_1,...,y_N$를 가중평균해서 $\hat{f}(x_0)$를 얻은 것이다. 다만 이 새로운 weight들 $l_1(x_0)$, $...$ , $l_N(x_0)$은 기존의 weighting kernel $K_\lambda(x_0,x_i)$과 weighted least squares 작업을 반영한 weight이다. 따라서, $l_i(x_0)$를 **equivalent kernel**이라고도 부른다.

![image](https://user-images.githubusercontent.com/45325895/51577659-ecce1a00-1efd-11e9-8ae0-766b69a0ac5e.png){: .center-image}

위 그림은 local linear regression을 이용하여 kernel smoother(녹색 곡선)를 그린 그림이다. 빨강색 선분으로 나타난 것이 weighted least squares로 얻은 직선이다. $x=x_0$에서의 regression line의 값을 $\hat{f}(x_0)$로 취한 모습을 볼 수 있다. 또한 일반적인 local average 방법과 비교했을 때, boundary 부근에서 bias가 생기는 문제가 대폭 해결된 것을 볼 수 있다. 위에서도 잠깐 언급했지만, **Local linear regression**은 kernel asymmetry로 인한 **bias를 first-order까지 완전히 보정**한다. $x=x_0$에서의 kernel smoother $\hat{f}(x_0)$의 bias를 구하기 위해, true function $f$의 $x=x_0$에서의 taylor 전개를 이용하여 $\hat{f}(x_0)$의 기대값을 아래와 같이 나타낼 수 있다.  

$$
\text{By Taylor expansion of} \enspace f, \enspace \enspace f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+R
$$

$$
E[\hat{f}(x_0)]=\sum_{i=1}^{N} l_i(x_0)E[y_i]=\sum_{i=1}^{N} l_i(x_0)f(x_i)
$$

$$
=f(x_0)\Big( \sum_{i=1}^{N} l_i(x_0) \Big)  +f'(x_0) \Big( \sum_{i=1}^{N} (x_i-x_0)l_i(x_0) \Big) +\frac{f''(x_0)}{2!} \Big( \sum_{i=1}^{N}(x_i-x_0)^2l_i(x_0) \Big) +R \Big( \sum_{i=1}^{N} l_i(x_0) \Big)
$$

$l_i(x_0)$는 $\sum_{i=1}^{N} l_i(x_0)=1$과 $\sum_{i=1}^{N} (x_i-x_0)l_i(x_0)=0$를 만족한다는 것을 어렵지 않게 보일 수 있다. 이를 대입하면 위 식은 아래와 같이 간단하게 나타낼 수 있다. 

$$
E[\hat{f}(x_0)]=f(x_0) +\frac{f''(x_0)}{2!} \Big( \sum_{i=1}^{N}(x_i-x_0)^2l_i(x_0) \Big) +R 
$$

$$
Bias[\hat{f}(x_0)]=E[\hat{f}(x_0)]-f(x_0) = \frac{f''(x_0)}{2!} \Big( \sum_{i=1}^{N}(x_i-x_0)^2l_i(x_0) \Big) +R 
$$

$\hat{f}(x_0)$의 bias에는 $x_i$들에 대해 $2$차 이상의 항들만 남아있는 것을 확인할 수 있다. 즉, **Local linear regression으로 도출한 kernel smoother $\hat{f}(x)$는 bias를 first-order까지 완전히 보정한다**는 장점이 있다. 그를 통해, kernel asymmetry로 인한 bias 발생 문제를 상당 부분 개선한다.  
<br>


## 6.1.2 Local Polynomial Regression

위의 Local **linear** regression 방법을 통해, 우리는 bias를 first-order까지 완전히 보정한 kernel smoother를 얻었다. local하게 $2$차, $3$차, 혹은 $d$차 polynomial regression을 수행하면, bias를 더 높은 차수까지 완전히 없애, bias를 더 줄일 수 있지 않을까? 그렇다.  

$$
\Big( \hat{\alpha}(x_0),\hat{\beta}_1(x_0), ... , ,\hat{\beta}_d(x_0) \Big)=\min_{\big( \alpha(x_0),\beta_1(x_0) , ... , ,\beta_d(x_0) \big)} \sum_{i=1}^{N} K_\lambda (x_0,x_i) \Bigg[ y_i-\alpha(x_0)-\sum_{j=1}^{d}\beta_j(x_0) x_i^j \Bigg]^2
$$

$$
\hat{f}(x_0)=\hat{\alpha}(x_0)+\hat{\beta}_1(x_0)x_0 + \hat{\beta}_2(x_0)x_0^2 +... + \hat{\beta}_d(x_0)x_0^d
$$

Local linear regression에서 local $d$차 polynomial regression으로 일반화하는 것은 그리 어렵지 않게 받아들일 수 있다. 또한 local linear regression에서 first-order까지 bias를 없앤 것을 보인 증명과 유사한 방법으로, **local $d$차 polynomial regression이 $d$차항까지 $\hat{f}(x_0)$의 bias를 완전히 보정한다**는 것도 확인할 수 있다. 아래의 사진은 local linear regression과 local quadratic regression의 수행결과를 비교한 것이다.  

![image](https://user-images.githubusercontent.com/45325895/51579370-15591280-1f04-11e9-9304-0c70c1ddca5e.png){: .center-image}

위 그림에서 $x=0.4$ 지점과 같이 curvature가 존재하는 구간에서는 $2$차 이상의 bias가 유의미한 값을 갖는다. 따라서, local **linear** regression이 $1$차까지 bias를 보정하더라도 여전히 bias가 존재하는 것을 확인할 수 있다. local **quadratic** regression은 $1$차 뿐 아니라 $2$차까지 bias를 보정해주기 때문에, kernel smoother $\hat{f}(x)$의 bias를 상대적으로 크게 개선한 것을 볼 수 있다.  

하지만 **local regression의 차수를 높여 얻은 bias의 개선은 variance의 상승을 대가로 얻어진다**는 점을 짚고 넘어가야 한다. local $d$차 polynomial regression을 이용한 kernel smoother $\hat{f}(x)$의 variance는 아래와 같다. Training data가 $\sigma^2$를 분산으로 갖는 분포에서 iid하게 뽑힌 random sample임을 이용해 식을 전개하였다.

$$
Var[\hat{f}(x)]=Var \Bigg[ \sum_{i=1}^{N} l_i(x_0)y_i \Bigg] \stackrel{\text{ind}}{=} \sum_{i=1}^{N} l_i(x_0)^2 Var[y_i] = \sigma^2 \Bigg( \sum_{i=1}^{N} l_i(x_0)^2 \Bigg)
$$

위의 local linear regression에서 얻은 결과를 이용하면 $\sum_{i=1}^{N} l_i(x_0)^2$는 아래와 같이 나타낼 수 있다. $\mathbf{l}(x_0)$는 $N$개의 weight $l_i(x_0)$를 쌓은 벡터이다.

$$
\sum_{i=1}^{N} l_i(x_0)^2= \| \mathbf{l}(x_0)\|^2 = \| b(x_0)^T \big( \mathbf{B}^T \mathbf{W}(x_0) \mathbf{B} \big)^{-1} \mathbf{B}^T \mathbf{W}(x_0) \|^2
$$

local $d$차 polynomial regression에서 $b(x)^T=[1 \enspace x \enspace \cdots \enspace x^d]$이고, $\mathbf{B}$는 design matrix 역할을 하는 $N \times (d+1)$ 행렬이다. local regression의 차수($d$)가 올라갈 수록, 더 큰 차원의 벡터들을 연산하게 되고, 그 결과가 되는 $\mathbf{l}(x_0)$ 벡터는 전체적으로 더 큰 값을 갖게 될 것임을 추측해볼 수 있다. 이를 엄밀하게 도출하는 것은 The Elements of Statistical Learning의 Exercise 6.3에서 다룬다. 아래의 사진은 local regression의 차수를 늘림에 따라 각 $x$에서의 estimate $\hat{f}(x)$의 variance가 어떻게 달라지는지를 나타낸 그림이다. 차수가 constant에서 linear, quadratic으로 올라갈 수록 variance가 커지는 것을 확인할 수 있다.  

![image](https://user-images.githubusercontent.com/45325895/51580733-eb561f00-1f08-11e9-8a71-a62e1f0ed1f2.png){: .center-image}  


<br>
<br>

# 6.2 Selecting the Width of the Kernel

6.1에서 사용한 Epanechnikov(에파네크니코브?) kernel은 아래와 같은 식을 가진다. 

$$
K_\lambda (x_0,x)=D \Big( \frac{\mid x-x_0\mid}{\lambda} \Big)
$$

$$
D(t)=\begin{cases}
       \frac{3}{4} (1-t^2) &\quad\text{if  }  \mid t \mid \le 1\\
       0 &\quad\text{otherwise} \\
     \end{cases}
$$

이 **Epanechnikov kernel**의 경우, "kernel의 너비"란 kernel이 양의 값을 갖는 구간의 지름을 의미했다. 다른 kernel들은 "kernel의 너비"를 결정하는 parameter가 어떤 방식으로 존재할까?  
 * **Gaussian kernel**의 경우, mean은 기준이 되는 점 $x_0$를 의미할 것이고, standard deviation을 나타내는 parameter $\sigma$가 kernel의 너비를 결정하는 parameter가 된다.
 * **$k$-Nearest Neighbor** 방법의 경우, $x_0$으로부터 가장 가까운 $k$개의 점들에 대해서만 $1$의 값을 부여하고 그 외의 점들에게는 $0$의 값을 부여하는 **indicator function**, $I(x \in N_k(x_0))$이 kernel의 역할을 할 것이다. 이 때, kernel의 너비는 $k$가 결정한다. $k$가 클 수록 더 넓은 범위의 점들에 대해 $1$의 값을 부여할 것이기 때문이다.  
  
위에서 **Epanechnikov kernel**의 예시에 대해 논의했던 것과 같이, kernel의 너비는 kernel smoother의 bias와 variance에 영향을 주며, kernel의 너비 parameter에 대해 bias-variance tradeoff관계가 존재한다. **kernel의 너비가 좁으면**, $\hat{f}(x_0)$는 더 적은 수의 $y_i$에 대한 가중평균(혹은 fitted value of weighted regression)일 것이다. 따라서 **variance가 상대적으로 더 클 것이다.** 또한, kernel의 너비가 넓을 때와 비교했을 때, 각 $(x_i,y_i)$들은 $x=x_0$로부터 가까운 point들만 선택이 되었을 것이므로,(혹은 가까운 point들에 더 큰 weight가 주어졌을 것이므로,) $E(y_i)=f(x_i)$가 $f(x_0)$와 큰 차이가 없을 것이다. 즉, **bias가 더 작을 것이다.**  

그렇다면 이 kernel width parameter, $\lambda$의 값을 어떻게 결정해야 할까? [5.4](https://lee-jaejoon.github.io/ESL-5/#54-smoothing-splines)와 [5.5](https://lee-jaejoon.github.io/ESL-5/#55-automatic-selection-of-the-smoothing-parameters)에서 smoothing spline의 regularization parameter $\lambda$를 고를 때 진행했던 논의를 여기에 그대로 적용할 수 있다. 위에서 확인했듯이, Local regression 역시 $\lambda$가 주어졌을 때, $y$에 대해 linear한 estimator이다.

$$
\hat{f}(x_0)=b(x_0)^T \big( \mathbf{B}^T \mathbf{W}(x_0) \mathbf{B} \big)^{-1} \mathbf{B}^T \mathbf{W}(x_0)\mathbf{y} =\sum_{i=1}^{N} l_i(x_0)y_i= \mathbf{l}(x_0) \cdot \mathbf{y}
$$


# 6.3 Local Regression in $\mathbb{R}^p$


# 6.4 Structured Local Regression Models in $\mathbb{R}^p$


# 6.5 Local Likelihood and Other Models


# 6.6 Kernel Density Estimation and Classification


# 6.7 Radial Basis Functions and Kernels


# 6.8 Mixture Models for Density Estimation and Classification


# 6.9 Computational Considerations  
  
# Reference
> Hastie, T., Tibshirani, R.,, Friedman, J. (2001). The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc.. 




