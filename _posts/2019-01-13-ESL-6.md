---
layout: post
title: "ESL: Ch 6. Kernel Smoothing Methods"
tags: [Elements of Statistical Learning]
comments: true
---
> Contents  
> [6.1 One-Dimensional Kernel Smoothers](#61-one-dimensional-kernel-smoothers)  
> [6.2 Selecting the Width of the Kernel](#62-selecting-the-width-of-the-kernel)  
> [6.3 Local Regression in $\mathbb{R}^p$](#63-local-regression-in-mathbbrp)  
> [6.4 Structured Local Regression Models in $\mathbb{R}^p$](#64-structured-local-regression-models-in-mathbbrp)  
> [6.5 Local Likelihood and Other Models](#65-local-likelihood-and-other-models)  
> [6.6 Kernel Density Estimation and Classification](#66-kernel-density-estimation-and-classification)  
> [6.7 Radial Basis Functions and Kernels](#67-radial-basis-functions-and-kernels)  
> [6.8 Mixture Models for Density Estimation and Classification](#68-mixture-models-for-density-estimation-and-classification)  
> [6.9 Computational Considerations](#69-computational-considerations)  
<br>

이 챕터에서는 $p$개의 input feature들($X \in \mathbb{R}^p$)에 대해 정의된 true regression function $f(X)$를 좀 더 flexible하게 추정하는 방법에 대해 소개한다. 간단히 소개하자면, 가까운 점들만을 이용하거나 가까운 점일 수록 더 큰 영향을 주도록 하는 **Localization**을 이용할 것이며, 추정된 $\hat{f}(X)$가 $\mathbb{R}^p$ 상의 부드러운(smooth) 곡선이 되도록 할 것이다. **Localization**은 **Kernel**이라는 일종의 가중치를 부여하는 함수를 이용하여 달성할 것이다. 이와 같은 방식으로 regression function $f(X)$를 추정하는 방법을 **Kernel smoothing method**라고 한다.   
<br>

# 6.1 One-Dimensional Kernel Smoothers
이 장에서는 input feature가 한 개의 변수로 이루어진 경우의 Kernel smoothing method에 대해 소개한다. 주변 점들에 **weight를 부여**햐는 **kernel** 함수에 대한 감을 얻기 위해, 먼저 $k$-Nearest-Neighbor average를 이용해 regression function $f(X)$를 추정하는 방법을 살펴보자. 

## $k$-Nearest Neighbor Average
$x=x_0$에서의 $k$-Nearest-Neighbor average는 다음과 같이 정의된다.

$$
\hat{f}(x_0)=Ave(y_i \mid x_i \in N_k(x_0))
$$

$N_k(x)$는 training set 내의 점들 중, $x$로부터 가장 가까운 $k$개의 점의 집합을 의미한다. 즉, $x=x_0$에서의 $k$-Nearest-Neighbor average는 $x_0$로부터 가장 가까운 $k$개의 점들의 $y$값을 평균낸 값으로 구한다. 이를 정의역 내 모든 $x$ 값에 대해 수행하여 regression function $f(x)$를 추정하는 것이다. 아래 그림은 $30$-Nearest Neighbor average를 수행하여 true function $f(x)$를 추정한 그림이다. 즉, $x_0$로부터 가장 가까운 $30$개의 점들의 $y$값을 평균낸 값으로 $x_0$에서의 fitted value를 구한 것이다.

![image](https://user-images.githubusercontent.com/45325895/51550119-101da880-1eaf-11e9-849f-4112101c7927.png){: .center-image}

초록색 곡선이 $30$-Nearest Neighbor average로 도출한 regression function의 추정치, $\hat{f}(x)$이다. 빨간색으로 표시된 점은 $x_0$에서 가장 가까운 점 30개에 포함된 점, 즉 $\hat{f}(x_0)$ 계산에 영향을 준 점들인데, 높이가 평평한 노랑색 직사각형으로 표시된 것은 $N_{30}(x_0)$ 안에서는 동등한 weight로 $\hat{f}(x_0)$ 계산에 영향을 주었다는 것을 의미한다. 이 부분은 이후의 다른 모형과 비교를 할 때 더 설명하겠다. 그런데 $k$-Nearest Neighbor average는 위에서와 같이 불연속점이 많고 울퉁불퉁하다는 특징이 있다. 이는 $k$-Nearest Neighborhood에 새로운 점이 들어오고 한 점이 나가는 순간마다, $\hat{f}(x)$에의 영향에 대한 weight가 불연속적으로 변하기 때문이다.  

## Nadaraya-Watson Kernel-weighted Average

가장 간단한 Kernel smoother라고 할 수 있는 Nadaraya-Watson kernel-weighted average에 대해 살펴보자. $x=x_0$에서의 N-W kernel-weighted average는 다음과 같이 정의된다. $ \frac{K_\lambda (x_0,x_i)}{\sum_{i=1}^{N} K_\lambda (x_0,x_i)}$는 training set 내의 $N$개의 점들이 $x=x_0$일 때의 fitted value에 영향을 주는 정도를 나타내는 weight가 되고, 그 weight에 따라 가중평균하는 방식으로 fitted value를 결정하게 된다.

$$
\hat{f}(x_0)= \sum_{i=1}^{N} \frac{K_\lambda (x_0,x_i)}{\sum_{i=1}^{N} K_\lambda (x_0,x_i)} y_i
$$

여기서 **Kernel**, $K_\lambda (x_0,x_i)$는 $x_i$가 $x_0$와 가까울 수록 큰 값을 갖고, $x_0$로부터 멀 수록 작은 값을 갖도록 만들어진 다음과 같은 함수이다. Kernel을 어떤 함수로 설정하느냐에 따라 N-W kernel-weighted average의 모양이 달라지는데, 아래의 그림에서는 아래의 식과 같은 Epanechnikov kernel을 kernel로 설정하였다. 식을 보면 $x$가 $x_0$로부터 멀 수록 $K_\lambda(x_0,x)$의 값이 작아지는 것을 확인할 수 있다. 또한 kernel 함수 $K_\lambda (x_0,x_i)$는 연속함수이고, 그에 따라 weighted average의 weight도 연속함수이기 때문에, $k$-NN으로 도출한 $\hat{f}(x)$와는 달리 **smooth**한 특징을 보일 것이다.

$$
K_\lambda (x_0,x)=D \Big( \frac{\mid x-x_0\mid}{\lambda} \Big)
$$

$$
D(t)=\begin{cases}
       \frac{3}{4} (1-t^2) &\quad\text{if  } \mid t \mid \le 1\\
       0 &\quad\text{otherwise} \\
     \end{cases}
$$

![image](https://user-images.githubusercontent.com/45325895/51550146-2166b500-1eaf-11e9-9c64-18ae9e8fe84f.png){: .center-image}

빨간색으로 표시된 점은 위의 $30$-NN의 그림과 같이, $x=x_0$에서의 fitted value $\hat{f}(x_0)$의 계산에 영향을 준 점들인데, $30$-NN보다 더 많은 점들이 고려된 것을 알 수 있다. 다만 노랑색으로 색칠된 부분이 나타내듯, 가까운 점일 수록 더 높은 가중치로 반영이 된 것을 확인할 수 있다. (그림 상의 노랑색 면적의 모양은 $D(t)$의 모양과 동일하다.) 그런데, 위의 식을 잘 살펴보면 Kernel 함수 안에 있는 $\lambda$가 Kernel이 양의 값을 갖는 영역에 영향을 주는 것을 확인할 수 있다. 위의 그림은 $\lambda=0.2$를 사용해서 그린 그림이다. 여기서 $\lambda$의 값을 증가시키면 더 많은 점들이 $ \mid \frac{\mid x-x_0\mid}{\lambda} \mid \le 1$을 만족하여, 더 많은 점들에서 $K_\lambda (x_0,x)$의 값이 양수가 될 것이고, 한 점 $x=x_0$에서의 fitted value를 계산할 때 더 많은 점들의 함수값을 고려하게 될 것이다. 다시 말해서, $\lambda$는 주어진 Kernel에서 **neighborhood의 너비**를 결정하는 parameter이다.  

위의 예시에서는 fitted value를 구할 때 모두 동일한 neighborhood 너비를 적용했지만, 이를 $x_0$에 대한 함수로 나타내어, 각 $x_0$값들에 대해 서로 다른 neighborhood 너비를 부여할 수도 있다. 그 때 Kernel 함수의 식은 아래와 같다.

$$
K_\lambda (x_0,x)=D \Big( \frac{\mid x-x_0\mid}{h_\lambda (x_0)} \Big)
$$

neighborhood 너비를 결정하는 parameter $\lambda$가 변함에 따라, kernel smoother $\hat{f}(x)$는 어떤 영향을 받을까? 모형의 bias와 variance의 측면에서 생각해보자. **$\lambda$의 값이 크다면,** fitted value $\hat{f}(x_0)$의 값을 결정할 때 $x_0$로부터 더 먼 점들까지 고려하게 된다. **더 많은 점들에 대해 평균**을 내어 fitted value를 결정하기 때문에, $\hat{f}(x)$의 **variance는 낮아질 것이다.** 또한, 우리는 $x=x_0$에 대해 neighborhood를 설정하고, 그 neighborhood 내에서는 true function이 constant라고 가정한 채 $y_i=f(x_i)$들의 average를 구한 뒤, 이를 $x=x_0$일 때의 $\hat{f}$값으로 설정한다. **$x=x_0$로부터 먼 $x_i$일 수록 $y_i=f(x_i)$가 $f(x_0)$와 가깝다는 보장이 없기 때문**에, $\hat{f}(x)$는 neighborhood가 넓을 수록 **높은 bias를 가질 것**이다. 반대로 $\lambda$의 값이 작다면, $\hat{f}(x)$의 variance는 더 높을 것이고, bias는 낮아질 것이다. 따라서 우리는 Kernel의 parameter $\lambda$의 최적 값을 결정해주어야 한다.  

모든 $x$값에 대해 동일한 neighborhood 너비를 적용하는 경우( $h_\lambda (x_0)=\lambda$ ), Bias는 fitted value를 구하고자 하는 $x_0$ 주변의 training data의 density에는 그다지 영향을 받지 않는다. 다만 같은 크기의 neighborhood 너비를 사용한다면, training data가 더 많이 모여있는 곳(high density)에서는 training data가 드문드문한 곳(low density)보다 더 많은 점들의 함수값을 평균내서 fitted value를 구할 것이고, 그때의 estimate $\hat{f}(x_0)$는 variance가 상대적으로 낮을 것이다. 따라서, 어떤 한 점에서의 estimate $\hat{f}(x_0)$의 variance는 그 점 $x=x_0$에서의 density와 역비례관계(inversely proportional)에 있다.  
<br>


## Local Linear Regression

우리는 간단한 moving average인 $k$-nearest neighbor 방법을 먼저 소개했고, 이 $k$-NN 방법은 weight가 불연속적으로 변해 그 fitted value의 결과 또한 울퉁불퉁한 단점이 있었다. 여기서, 한 점 $x_0$를 기준으로 그로부터 가까운 점일 수록 더 큰 값을 갖는 **Kernel 함수**와 그를 이용한 weight를 도입함으로써, 우리는 weight가 연속적으로 움직여 smooth한 $\hat{f}$를 도출하는 **locally weighted average**를 통한 **Kernel smoother**를 학습하였다. 그런데 위의 N-W average와 같은 kernel smoother는 여전히 문제가 있다.

![image](https://user-images.githubusercontent.com/45325895/51553539-71954580-1eb6-11e9-81c6-e1d4c277e5ec.png){: .center-image}

Training set의 boundary의 근처의 점 $x_0$에서의 fitted value를 구할 때, 위의 그림과 같은 경우처럼, Kernel이 양의 값을 갖는 구간의 한 쪽이 boundary 밖으로 나가게 되어 **kernel함수가 비대칭성**을 갖게 되고, 그 결과로 boundary에 가까운 구간에서는 **비대칭적으로 weight가 부여되는 현상**이 발생한다. 위 경우는 파란색으로 그려진 true function $f$가 $x=x_0$ 주변에서 증가함수이고, $x_0$를 기준으로 오른쪽에 있는 점들이 비대칭적으로 더 높은 가중치를 부여받았기 때문에, $x=x_0$에서의 fitted value $\hat{f}(x_0)$가 true function $f$보다 위쪽으로 biased 되어있는 것을 확인할 수 있다.  

**Local linear regression**은 이와 같은 **kernel asymmetry에 따른 boundary 부근의 bias 발생 문제를 해결**하기 위해 고안되었다. N-W average와 같은 Local average 방법은, fitted value를 구하고자 하는 $x=x_0$에 대해 neighborhood를 설정하고, 그 neighborhood 내에서는 true function $f$가 constant라고 가정한 채 average를 구한 뒤, 이를 $x=x_0$일 때의 $\hat{f}$값으로 설정하는 작업을 반복한다. 하지만 true function $f$는 해당 neighborhood 내에서 constant가 아니기에, boundary 근처에서 kernel이 비대칭이 되면 bias가 생기는 것이다. **Local linear regression**은 이 **bias를 first-order까지 완벽하게 보정**한다.
























# 6.2 Selecting the Width of the Kernel


# 6.3 Local Regression in $\mathbb{R}^p$


# 6.4 Structured Local Regression Models in $\mathbb{R}^p$


# 6.5 Local Likelihood and Other Models


# 6.6 Kernel Density Estimation and Classification


# 6.7 Radial Basis Functions and Kernels


# 6.8 Mixture Models for Density Estimation and Classification


# 6.9 Computational Considerations  
  
# Reference
> Hastie, T., Tibshirani, R.,, Friedman, J. (2001). The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc.. 




