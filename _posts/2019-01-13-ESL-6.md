---
layout: post
title: "ESL: Ch 6. Kernel Smoothing Methods"
tags: [Elements of Statistical Learning]
comments: true
---
> Contents  
> [6.1 One-Dimensional Kernel Smoothers](#61-one-dimensional-kernel-smoothers)  
> [6.2 Selecting the Width of the Kernel](#62-selecting-the-width-of-the-kernel)  
> [6.3 Local Regression in $\mathbb{R}^p$](#63-local-regression-in-mathbbrp)  
> [6.4 Structured Local Regression Models in $\mathbb{R}^p$](#64-structured-local-regression-models-in-mathbbrp)  
> [6.5 Local Likelihood and Other Models](#65-local-likelihood-and-other-models)  
> [6.6 Kernel Density Estimation and Classification](#66-kernel-density-estimation-and-classification)  
> [6.7 Radial Basis Functions and Kernels](#67-radial-basis-functions-and-kernels)  
> [6.8 Mixture Models for Density Estimation and Classification](#68-mixture-models-for-density-estimation-and-classification)  
> [6.9 Computational Considerations](#69-computational-considerations)  
<br>

이 챕터에서는 $p$개의 input feature들($X \in \mathbb{R}^p$)에 대해 정의된 true regression function $f(X)$를 좀 더 flexible하게 추정하는 방법에 대해 소개한다. 간단히 소개하자면, 가까운 점들만을 이용하거나 가까운 점일 수록 더 큰 영향을 주도록 하는 **Localization**을 이용할 것이며, 추정된 $\hat{f}(X)$가 $\mathbb{R}^p$ 상의 부드러운(smooth) 곡선이 되도록 할 것이다. **Localization**은 **Kernel**이라는 일종의 가중치를 부여하는 함수를 이용하여 달성할 것이다. 이와 같은 방식으로 regression function $f(X)$를 추정하는 방법을 **Kernel smoothing method**라고 한다.   
<br>

# 6.1 One-Dimensional Kernel Smoothers
이 장에서는 input feature가 한 개의 변수로 이루어진 경우의 Kernel smoothing method에 대해 소개한다. 주변 점들에 **weight를 부여**햐는 **kernel** 함수에 대한 감을 얻기 위해, 먼저 $k$-Nearest-Neighbor average를 이용해 regression function $f(X)$를 추정하는 방법을 살펴보자. 

## $k$-Nearest Neighbor Average
$x=x_0$에서의 $k$-Nearest-Neighbor average는 다음과 같이 정의된다.

$$
\hat{f}(x_0)=Ave(y_i \mid x_i \in N_k(x_0))
$$

$N_k(x)$는 training set 내의 점들 중, $x$로부터 가장 가까운 $k$개의 점의 집합을 의미한다. 즉, $x=x_0$에서의 $k$-Nearest-Neighbor average는 $x_0$로부터 가장 가까운 $k$개의 점들의 $y$값을 평균낸 값으로 구한다. 이를 정의역 내 모든 $x$ 값에 대해 수행하여 regression function $f(x)$를 추정하는 것이다. 아래 그림은 $30$-Nearest Neighbor average를 수행하여 true function $f(x)$를 추정한 그림이다. 즉, $x_0$로부터 가장 가까운 $30$개의 점들의 $y$값을 평균낸 값으로 $x_0$에서의 fitted value를 구한 것이다.

![image](https://user-images.githubusercontent.com/45325895/51260443-a191a400-19f1-11e9-86bb-29496e099dd1.png){: .center-image}

초록색 곡선이 $30$-Nearest Neighbor average로 도출한 regression function의 추정치, $\hat{f}(x)$이다. 빨간색으로 표시된 점은 $x_0$에서 가장 가까운 점 30개에 포함된 점, 즉 $\hat{f}(x_0)$ 계산에 영향을 준 점들인데, 높이가 평평한 노란색 직사각형으로 표시된 것은 $N_{30}(x_0)$ 안에서는 동등한 weight로 $\hat{f}(x_0)$ 계산에 영향을 주었다는 것을 의미한다. 이 부분은 이후의 다른 모형과 비교를 할 때 더 설명하겠다. 그런데 $k$-Nearest Neighbor average는 위에서와 같이 불연속점이 많고 울퉁불퉁하다는 특징이 있다. 이는 $k$-Nearest Neighborhood에 새로운 점이 들어오고 한 점이 나가는 순간마다, $\hat{f}(x)$에의 영향에 대한 weight가 불연속적으로 변하기 때문이다.  

## Nadaraya-Watson Kernel-weighted Average



# 6.2 Selecting the Width of the Kernel


# 6.3 Local Regression in $\mathbb{R}^p$


# 6.4 Structured Local Regression Models in $\mathbb{R}^p$


# 6.5 Local Likelihood and Other Models


# 6.6 Kernel Density Estimation and Classification


# 6.7 Radial Basis Functions and Kernels


# 6.8 Mixture Models for Density Estimation and Classification


# 6.9 Computational Considerations  
  
# Reference
> Hastie, T., Tibshirani, R.,, Friedman, J. (2001). The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc.. 




