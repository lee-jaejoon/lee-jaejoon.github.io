---
layout: post
title: "Introduction to Bayesian Nonparametrics - 2"
tags: [Bayesian Statistics]
comments: true
---

> **Contents**  
> **[3. Dirichlet Process](#3-dirichlet-process)**  
> **[3.1 Definition](#31-definition)**  
> **[3.2 Properties](#32-properties)**  
 > [3.2.1 Expectations, Variances, and Co-variances]()  
 > [3.2.2 Tail-freeness]()  
 > [3.2.3 Self-similarity]()  
 > [3.2.4 Conjugacy]()  
 > [3.2.5 Marginal and Conditional Distribution]()  
 > [3.2.6 Discreteness]()  
> **[3.3 Constructions]()**  
 > [3.3.1 Construction via a Stochastic Process]()  
 > [3.3.2 Construction through a Distribution Function]()  
 > [3.3.3 Construction through a Gamma Process]()  
 > [3.3.4 Construction through Polya Urn Scheme]()  
 > [3.3.5 Stick-Breaking Representation(Sethuraman Representation)]()  
  
<br>
  
# 3 Dirichlet Process

## 3.1 Definition

위에서와 마찬가지로 $\mathfrak M$은 Polish space $(\mathfrak X, \mathcal X)$ 위에서 정의된 probability measure들의 set이다. Dirichlet process의 정의는 아래와 같다.

 > A random measure $P$ on $(\mathfrak X, \mathcal X)$ is said to possess a Dirichlet process distribution $\text{DP}(\alpha)$ with a base measure $\alpha$, if for every finite measurable partition $A_1, \cdots , A_k$ of $\mathfrak X$,
 > 
 > $$
 > (P(A_1), \cdots , P(A_k)) \sim \text{Dirichlet}(\alpha(A_1), \cdots , \alpha(A_k)).
 > $$
 > 

위 정의에서는 $\alpha$가 임의의 finite positive Borel measure on $(\mathfrak X, \mathcal X)$이다. 이를 양수 $\alpha_0$와 probability measure $G_0$를 이용하여 아래와 같이 정의하기도 한다.

 > $$
 > (P(A_1), \cdots , P(A_k)) \sim \text{Dirichlet}( \alpha_0 G_0(A_1), \cdots , \alpha_0 G_0(A_k)).
 > $$

이 때 양수 $\alpha_0$와 probability measure $G_0$는 첫 번째 정의의 base measure $\alpha$와 아래와 같은 관계를 가진다.

$$
\alpha_0 = \alpha(\mathfrak X) , \enspace G_0(\cdot) = \frac{1}{\alpha_0} \alpha(\cdot)
$$

## 3.2 Properties

위에서 정의한 것과 같은 Dirichlet process의 존재성과 construct하는 방법에 대해서는 뒤에서 설명하기로 하고, 먼저 Dirichlet process의 성질에 대해 알아보자. 이하의 설명에서는 Dirichlet process의 두 equivalent한 정의의 notation를 모두 사용하겠다.

### 3.2.1 Expectations, Variances, and Co-variances

정의에 따르면, 임의의 Borel set $A$에 대하여 $(P(A), P(A^c))$는 $\text{Dirichlet}( \alpha(A) , \alpha(A^c))$의 분포를 따른다. 즉 $P(A)$는 $\text{Beta}(\alpha(A) , \alpha(A^c))$ 분포를 따른다. 따라서 임의의 Borel set $A$, $B$에 대하여 다음이 만족한다.

$$
\text{E}(P(A)) = \frac{\alpha(A)}{\alpha(\mathfrak X)} =  \frac{1}{\alpha_0} \alpha(A) = G_0(A)
$$

$$
\text{Var}(P(A)) = \frac{\alpha(A)\alpha(A^c)}{\alpha(\mathfrak X)^2 (\alpha(\mathfrak X)+1)} = \frac{G_0(A)G_0(A^c)}{\alpha_0+1}
$$

$$
\text{Cov}(P(A),P(B))=\frac{G_0(A \cap B)-G_0(A)G_0(B)}{\alpha_0+1}
$$

첫 두 결과는 $P(A) \sim \text{Beta}(\alpha(A) , \alpha(A^c))$의 성질을 이용하면 쉽게 얻을 수 있다. 세 번째 결과를 얻기 위해서는 아래와 같은 증명과정이 필요하다.  
  
먼저 $A \cap B = \emptyset$이라고 가정하자. 이 때 $\{ A, B, A^c \cap B^c \}$는 $\mathfrak X$의 partition이 되고, $(P(A), P(B), P(A^c \cap B^c))$은 $\text{Dirichlet}( \alpha(A) , \alpha(B), \alpha(A^c \cap B^c))$를 따른다. 따라서 다음이 만족한다.

$$
\text{Cov}(P(A),P(B))=-\frac{\alpha(A)\alpha(B)}{\alpha_0^2(\alpha_0+1)}=-\frac{G_0(A)G_0(B)}{\alpha_0+1}
$$

따라서 $A \cap B = \emptyset$일 때, 위 결과의 식이 만족하는 것을 확인할 수 있다. 일반적인 Borel set $A$, $B$에 대해서 증명을 하기 위해, 먼저 $A$, $B$를 $A=(A \cap B) \cup (A \cap B^c)$, $B=(A \cap B) \cup (A^c \cap B)$로 분해한다.

$$
\text{Cov}(P(A),P(B))=\text{Cov}(P(A \cap B) + P(A \cap B^c),P(A \cap B) + P(A^c \cap B)) \\
=\text{Var}(P(A \cap B),P(A \cap B) ) + \text{Cov}(P(A \cap B),P(A^c \cap B)) +\text{Cov}( P(A \cap B^c),P(A \cap B)) +\text{Cov}(P(A \cap B^c),P(A^c \cap B)) 
$$

네 개의 Covariance 항을 모두 계산하면 위의 세 번째 결과 식을 얻을 수 있다. $\square$  
  
첫 번째 결과의 의미를 좀더 살펴보자. Parametric 가정이 없는 상황에서, prior가 $\text{DP}(\alpha)$를 따르는 random measure $P$라고 하자. 만약 observation X 역시 random distribution(measure) $P$로부터 나왔다고 가정하자. 즉, $P \sim \text{DP}(\alpha)$, $X \mid P \sim P $의 상황을 가정한 것이다. 위의 Expectation 식을 적분 꼴로 나타내면 다음과 같다.

$$
\int P(A) d \text{DP}_\alpha(P) =  G_0(A)
$$

이는 observation $X$의 marginal 분포가 $G_0$이라는 것을 의미한다. 

$$
\text{if }P \sim \text{DP}(\alpha) \text{ and } X \mid P \sim P , \text{ then }X \sim G_0
$$

따라서 이 상황에서 우리는 $G_0$을 다음과 같이 두 가지로 이해할 수 있다.

 * Prior mean
 * "Dirichlet process(에서 생성된 분포)로부터 생성된 observation"의 marginal 분포

Probability measure $G_0$을 Dirichlet process의 *center measure*라고도 부른다. 또한, 분산과 공분산의 식을 보면 $\alpha_0$가 $P(A)$의 variability를 조절하는 것을 볼 수 있다. 따라서 $\alpha_0$를 *precision parameter*라고 부른다. 이에 대한 간단한 예시를 들자면, 만약 우리의 모형이 $N(0,1)$를 따를 것으로 기대하지만 확실하지는 않다면, center measure가 $G_0 \sim N(0,1)$분포인 Dirichlet process prior를 부여할 수 있다. 그 때, $\alpha_0$은 우리의 prior guess의 확실한 정도를 나타낸다. 다만 $\alpha_0$는 prior guess의 confidence말고도 다른 의미를 가지고 있기 때문에 해석에 유의하여야 하며, 다른 의미는 후에 다룰 posterior distribution of Dirichlet distribution에서 알 수 있다.  
  
또한 일반적인 경우에, base measure $\alpha$가 다르면 다른 Dirichlet process를 얻을 것이라는 것을 알 수 있다. 즉,

$$
\alpha \neq \alpha ' \enspace \Longrightarrow \enspace \text{DP}(\alpha) \neq \text{DP}(\alpha ') \text{ , unless }G_0 = G_0 ' = \delta_x \text{ for some } x
$$

그 이유는 다음과 같다.

 * $\alpha \neq \alpha \prime \text{ & } G_0 \neq G_0 \prime$인 경우, $\text{DP}(\alpha)$와 $\text{DP}(\alpha \prime)$는 서로 다른 expectation을 갖는다.
 * $\alpha \neq \alpha \prime \text{ & } G_0 = G_0 \prime$인 경우, $\alpha_0 \neq \alpha_0 \prime$이다. 그러면 임의의 $A$에 대하여 $ 0 < G_0(A) = G_0 \prime(A) < 1$를 가정하면 $\text{DP}(\alpha)$와 $\text{DP}(\alpha \prime)$는 서로 다른 variance를 갖는다.
 * 즉, base measure가 nondegenerate하다면, 서로 다른 base measure에 대해 다른 Dirichlet process를 얻게 된다.

### 3.2.2 Tail-freeness

***tail-free*** random measure에 대해 소개하고자 한다. 그를 위해서 random measure $P$를 construct하는 방법 중 하나인 tree-based method를 소개한다. Tree를 이용해 random measure $P$를 construct한다는 것은, 쉽게 말해서 total probability mass $1$를 반복적인 partitioning을 통해 $\mathfrak X$의 subset들로 분배하는 것으로 이해할 수 있다. 다만 그 mass의 분배를 조절하는 proportion들이 random variable인 것이다.  
  
다음과 같이, 직전 partition의 모든 set을 각각 새로운 두 개의 set으로 나눔으로써 얻어지는, $\mathfrak X$의 partition들의 sequence $\{ \mathcal T_m \}$을 생각해보자.

$$
\begin{align*}
\mathcal T_0 &= \{ \mathfrak X \} \\
\mathcal T_1 &= \{ A_0, A_1 \} \\
\mathcal T_2 &= \{ A_{00}, A_{01}, A_{10}, A_{11} \} \\
\mathcal T_3 &= \{ A_{000}, A_{001}, A_{010}, A_{011}, A_{100}, A_{101}, A_{110} , A_{111}\} \\
&\enspace \enspace \vdots
\end{align*}	
$$

$0$과 $1$로 이루어져 있고 길이가 $m$인 모든 string $\varepsilon_1 \cdots \varepsilon_m $의 set, $\mathcal E ^m$을 정의하자.

$$
\mathcal E ^m = \{ 00\cdots 00, 00\cdots01, \cdots , 11\cdots11 \} , \enspace \left\lvert \mathcal E ^m \right\rvert = 2^m
$$

$\mathcal E ^m$을 이용하여, $m$번째 partition $\mathcal T_m$의 $2^m$개의 set들에 다음과 같이 index를 부여할 수 있다.

$$
\mathcal E ^3 = \{ 000, 001, 010, 011, 100, 101, 110, 111 \} \\
\mathcal T_3 = \{ A_{000}, A_{001}, A_{010}, A_{011}, A_{100}, A_{101}, A_{110} , A_{111}\} 
$$

이와 같은 string을 모두 모은 set $\mathcal E ^\ast = \bigcup_{m=0}^{\infty} \mathcal E ^m$을 고려할 수 있다. 임의의 $\varepsilon \in \mathcal E ^\ast$에 대해, splitting probability $V_\varepsilon$을 정의하자.

$$
V_{\varepsilon0} = \text{Pr}(A_{\varepsilon0} \mid A_{\varepsilon}), \enspace V_{\varepsilon1} = \text{Pr}(A_{\varepsilon1} \mid A_{\varepsilon})
$$

$V_{\varepsilon0}, V_{\varepsilon1}$는 partition들의 sequence, $\mathcal T_0, \mathcal T_1, \mathcal T_2, \mathcal T_3, \cdots$를 모든 node가 둘로 쪼개지는 tree로 생각했을 때, $A_{\varepsilon}$ node를 둘로 쪼개는 conditional (random) probability와 같고, $V_{\varepsilon0} + V_{\varepsilon1}=1$를 만족한다.

![image](https://user-images.githubusercontent.com/45325895/61279858-e1471600-a7f1-11e9-98a0-9d1e8315aa60.png){: .center-image}

만약 refining partition들의 set, $\{ A_\varepsilon : \varepsilon \in \mathcal E^\ast \}$가 Borel $\sigma$-field를 generate한다면, 위의 성질을 만족하는 $[0,1]$-valued random variable의 collection $\{ V_\varepsilon : \varepsilon \in \mathcal E^\ast \}$이 주어졌을 때, 그에 따라 random measure $P$를 정의할 수 있다.

$$
P(A_{\varepsilon_1 \varepsilon_2 \cdots \varepsilon_m})= V_{\varepsilon_1} V_{\varepsilon_1 \varepsilon_2} \cdots V_{\varepsilon_1 \varepsilon_2 \cdots \varepsilon_m} , \enspace \enspace \varepsilon = \varepsilon_1 \varepsilon_2 \cdots \varepsilon_m \in \mathcal E^m
$$

이제 random measure $P$의 *tail-freeness*를 정의할 수 있다. 정의는 다음과 같다.

 > The random measure P is a **tail-free process** with respect to the sequence of partitions $\mathcal T_m$ if the following is true.
 > 
 > $$
 > \{ V_0 \} \perp\!\!\!\perp \{ V_{00} , V_{10} \} \perp\!\!\!\perp \{ V_{000} , V_{010}, V_{100}, V_{110} \} \perp\!\!\!\perp \cdots \perp\!\!\!\perp \{ V_{\varepsilon 0} : \varepsilon \in \mathcal E^m \} \perp\!\!\!\perp \cdots
 > $$

이를 조건부확률의 형태로 다시 쓰면 아래와 같다.

$$
\{ \text{Pr}(A_0) \} \perp\!\!\!\perp \{ \text{Pr}(A_{00} \mid A_0) ,\text{Pr}(A_{10} \mid A_0) \} \perp\!\!\!\perp \{ \text{Pr}(A_{000} \mid A_{00}), \text{Pr}(A_{010} \mid A_{01}),\text{Pr}(A_{100} \mid A_{10}), \text{Pr}(A_{110} \mid A_{11}) \} \perp\!\!\!\perp \cdots 
$$

Dirichlet process는 **tail-free random measure**이다. 즉, 

> Splitting variable $( V_{\varepsilon 0} : \varepsilon \in \mathcal E^m )$은 서로 다른 level $m$ 사이에서 mutually independent하고, $V_{\varepsilon 0} \sim \text{Beta}(\alpha(A_{\varepsilon 0}), \alpha(A_{\varepsilon 1}))$을 따른다. 

그 증명은 아래와 같다. 먼저 증명에 필요한 Dirichlet distribution의 성질 몇 가지를 소개하고 증명을 보이겠다.  

### Gamma Representation of Dirichlet Distribution / Aggregation Property

간단한 variable transformation을 거치면 Dirichlet distribution을 gamma distribution에 대해 나타낼 수 있다. 다음과 같은 확률변수 $Z_1, \cdots ,Z_k$를 생각해보자.

$$
Z_i \stackrel{ind}{\sim} \text{Gamma}(\alpha_i , 1) , \enspace i=1, \cdots, k
$$

증명하고자 하는 성질은 다음과 같다.

> $$
> \begin{align*}
> 1. \enspace &\left( \frac{Z_1}{\sum^{k}_{i=1} Z_i} , \cdots , \frac{Z_k}{\sum^{k}_{i=1} Z_i} \right) \sim \text{Dirichlet}(\alpha_1, \cdots, \alpha_k) \\ 
> 2. \enspace &\left( \frac{Z_1}{\sum^{k}_{i=1} Z_i} , \cdots , \frac{Z_k}{\sum^{k}_{i=1} Z_i} \right) \perp\!\!\!\perp \sum^{k}_{i=1} Z_i \\
> 3. \enspace &\text{ If } P = (P_1, \cdots, P_k)\sim \text{Dirichlet}(\alpha_1, \cdots , \alpha_k),\text{ then for any partition }A_1, \cdots , A_m \text{ of } \{ 1, \cdots, k \}, \\
> &\left( \sum_{i \in A_1} P_i , \cdots , \sum_{i \in A_m} P_i \right) \sim 	\text{Dirichlet} \left( \sum_{i \in A_1} \alpha_i, \cdots , \sum_{i \in A_k} \alpha_i \right) \\
> \end{align*}
> $$

증명은 다음과 같다.

$$
f_{\mathbf Z} (z_1, \cdots, z_k ) = e^{-\sum_{i=1}^{k} z_i} \prod_{i=1}^{k} \frac{z_i^{\alpha_i -1}}{\Gamma(\alpha_i)}
$$

다음과 같이 정의된 $(Z_1, \cdots ,Z_k) \rightarrow (Y_1, \cdots ,Y_k)$의 variable transformation을 고려해보자.

$$
\begin{align*}
Y_i &= \frac{Z_i}{\sum_{j=1}^{k} Z_j} , \enspace i=1, \cdots , k-1 \\
Y_k &= \sum_{i=1}^{k} Z_i
\end{align*}
$$

Variable transformation을 수행하면 다음과 같다.

$$
\begin{align*}
Z_i &= Y_i Y_k  , \enspace i=1, \cdots , k-1 \\
Z_k &= \left( 1- \sum_{i=1}^{k-1} Y_i \right)Y_k
\end{align*}
$$

$$
\lvert J \rvert = 
\left\lvert
\begin{matrix}
 Y_k & 0 & \cdots &0 & Y_1\\
 0 & Y_k & \cdots & 0  & Y_2\\
 \vdots & \vdots & \ddots &   & \vdots \\
 0 & 0 &  & Y_k   & Y_{k-1}\\
 -Y_k & -Y_k & \cdots & -Y_k   & 1-\sum_{i=1}^{k-1} Y_i\\
\end{matrix}
\right\rvert
=
\left\lvert
\begin{matrix}
 Y_k & 0 & \cdots &0 & Y_1\\
 0 & Y_k & \cdots & 0  & Y_2\\
 \vdots & \vdots & \ddots &   & \vdots \\
 0 & 0 &  & Y_k   & Y_{k-1}\\
 0 & 0 & \cdots & 0   & 1 \\
\end{matrix}
\right\rvert
=Y_k^{k-1}
$$

$$
\begin{align*}
\implies \enspace f_\mathbf Y (y_1 , \cdots ,y_k) &= e^{-\sum_{i=1}^{k-1} y_i y_k} e^{(-1+\sum_{i=1}^{k-1} y_i) y_k} \left\{ \prod_{i=1}^{k-1} \frac{(y_iy_k)^{\alpha_i -1}}{\Gamma(\alpha_i)} \right\} \frac{(1-\sum_{i=1}^{k-1} y_i)^{\alpha_k -1} y_k^{\alpha_k -1}}{\Gamma(\alpha_k)} y_k^{k-1} \\
&= \frac{ \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1}  }{\prod_{i=1}^{k} \Gamma(\alpha_i)} y_k^{\sum_{i=1}^{k} \alpha_i -1} e^{-y_k} ,\\
&(\text{where } 0 \leq y_1, \cdots, y_{k-1} \leq 1, \text{ and } y_k >0 )
\end{align*}
$$

이를 $y_k$에 대해 marginalize하면 다음과 같다.

$$
\begin{align*}
f(y_1, \cdots , y_{k-1}) &=\frac{ \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1}  }{\prod_{i=1}^{k} \Gamma(\alpha_i)}
\int_0^\infty y_k^{\sum_{i=1}^{k} \alpha_i -1} e^{-y_k} dy_k \\
&= \frac{ \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1}  }{\prod_{i=1}^{k} \Gamma(\alpha_i)} {\Gamma(\sum_{i=1}^{k} \alpha_i)} \\
&= \frac{\Gamma(\sum_{i=1}^{k} \alpha_i)}{\prod_{i=1}^{k} \Gamma(\alpha_i)} \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1} 
\end{align*}
$$

$Y_i ' = Y_i  \text{ for }i =1,\cdots, k-1, \text{ }Y_k '  = 1-\sum_{i=1}^{k-1} Y_i$로 한 번 더 transformation을 수행하면 1번 성질을 얻을 수 있다.

$$
(Y_1 ' , \cdots, Y_k ') = \left( \frac{Z_1}{\sum^{k}_{i=1} Z_i} , \cdots , \frac{Z_k}{\sum^{k}_{i=1} Z_i} \right) \sim \text{Dirichlet}(\alpha_1, \cdots, \alpha_k)
$$

또한, 2번 성질에서처럼 $i=1,\cdots,k$에 대해 $\frac{Z_i}{\sum^k_{j=1} Z_j}$과 $\sum^k_{j=1} Z_j$이 서로 독립인 것은 도출 과정 중간에 얻은 density 식이 factorize되는 것에서 알 수 있다.

$$
f_\mathbf Y (y_1 , \cdots ,y_k)  = \frac{ \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1}  }{\prod_{i=1}^{k} \Gamma(\alpha_i)} y_k^{\sum_{i=1}^{k} \alpha_i -1} e^{-y_k} = \left( \prod_{i=1}^{k-1} g_i(y_i) \right) g_k(y_k).
$$
  
3번 성질은 다음과 같은 과정으로 증명할 수 있다. 위에서와 같은 gamma representation을 사용하면 아래와 같다.

$$
\begin{align*}
\left( \sum_{i \in A_1} P_i , \cdots , \sum_{i \in A_m} P_i \right) &= \left( \sum_{i \in A_1} \frac{Z_i}{\sum^{k}_{j=1} Z_j} , \cdots , \sum_{i \in A_m} \frac{Z_i}{\sum^{k}_{j=1} Z_j}  \right) \\
&= \frac{1}{\sum^{k}_{j=1} Z_j} \left( \sum_{i \in A_1} Z_i , \cdots , \sum_{i \in A_m} Z_i \right) \\
&\stackrel{d}{=} \frac{1}{\sum^{m}_{\ell=1} \sum_{i \in A_\ell} Z_i} \left( \sum_{i \in A_1} Z_i , \cdots , \sum_{i \in A_m} Z_i \right) \\
&\stackrel{d}{=} \text{Dirichlet}\left( \sum_{i \in A_1} \alpha_i, \cdots , \sum_{i \in A_k} \alpha_i \right) \\
\end{align*}
$$

마지막 등호는 다음과 같은 gamma distribution의 성질에서 온 것이다.   $\square $

$$
\sum_{i \in A_j} Z_i \sim \Gamma \left( \sum_{i \in A_j} \alpha_i \right) \enspace , \enspace \enspace j=1, \cdots , m 
$$

### Proof of Tailfreeness of Dirichlet Process

먼저 우리는 random vector $( V_{\varepsilon 0} : \varepsilon \in \mathcal E^m )$이 모든 level $m$에 대해 서로 독립임을 보여야 한다. 그를 위해서 임의의 $m$에 대해, $( V_{\varepsilon 0} : \varepsilon \in \mathcal E^m )$이 자기자신보다 lower level에 해당하는 random vector들과 독립임을 보이면 된다. 더 간단히 이야기하면 임의의 $m \in \mathbb N$, 임의의 given $\varepsilon = \varepsilon_1 \cdots \varepsilon_m \in \mathcal E^m$에 대해, 다음을 보이면 된다.

$$
V_{\varepsilon_1 \cdots \varepsilon_m} \perp\!\!\!\perp \{ V_{\varepsilon_1 \cdots \varepsilon_m 0} , V_{\varepsilon_1 \cdots \varepsilon_m 1} \}
$$

귀납법을 이용하여 이를 증명할 것이다. 먼저 $m=1$인 경우 위 induction statement를 증명하자.

$$
V_{\varepsilon_1} = P(A_{\varepsilon_1})
$$

$$
V_{\varepsilon_1 0} = P(A_{\varepsilon_1 0} \mid A_{\varepsilon_1} )= \frac{P(A_{\varepsilon_1 0} )}{P(A_{\varepsilon_1})} \enspace , \enspace V_{\varepsilon_1 1} = P(A_{\varepsilon_1 1} \mid A_{\varepsilon_1} ) = \frac{P(A_{\varepsilon_1 1} )}{P(A_{\varepsilon_1})} 
$$

Dirichlet process의 정의에 따라 다음이 성립한다.

$$
\Big(P(A_{00}),P(A_{01}),P(A_{10}),P(A_{11})\Big) \sim \text{Dirichlet}\Big( \alpha(A_{00}),\alpha(A_{01}),\alpha(A_{10}),\alpha(A_{11}) \Big)
$$

Gamma representation을 이용하면 다음과 같은 결과를 얻을 수 있다.
$$
\begin{align*}
\Big( \frac{P(A_{00})}{P(A_0)} ,\frac{P(A_{01})}{P(A_0)}\Big) &\stackrel{d}{=} \Big( \frac{Z_1}{Z_1+Z_2} , \frac{Z_2}{Z_1+Z_2} \Big)\\
&\stackrel{d}{=} \text{Dirichlet}\Big( \alpha(A_{00}),\alpha(A_{01}) \Big) \\
\Big( \frac{P(A_{10})}{P(A_1)}, \frac{P(A_{11})}{P(A_1)}\Big) &\stackrel{d}{=} \Big( \frac{Z_3}{Z_3+Z_4} , \frac{Z_4}{Z_3+Z_4} \Big)\\
&\stackrel{d}{=} \text{Dirichlet}\Big( \alpha(A_{10}),\alpha(A_{11}) \Big)
\end{align*}
$$

따라서, 위에서 보인 Dirichlet distribution의 성질에 의해, $m=1$인 경우에 induction statement가 성립하는 것을 보였다.

$$
P(A_0)  \perp\!\!\!\perp   \frac{P(A_{00})}{P(A_0)} = P(A_{0 0} \mid A_{0} ) , \enspace P(A_1)  \perp\!\!\!\perp \frac{P(A_{10})}{P(A_1)} =  P(A_{10} \mid A_{1} )  
$$

$$
P(A_{0 0} \mid A_{0} ) \sim \text{Beta}(\alpha(A_{00}),\alpha(A_{01})) , \enspace P(A_{10} \mid A_{1} )   \sim \text{Beta}(\alpha(A_{10}),\alpha(A_{11}))
$$

$m=1, \cdots , k-1$인 경우 위 induction statement가 성립한다고 가정했을 때, $m=k$인 경우도 성립함을 보이자. Dirichlet process의 정의에 의해 다음이 성립한다.

$$
\Big(\cdots , P(A_{\varepsilon_1 \cdots \varepsilon_k 0}),P(A_{\varepsilon_1 \cdots \varepsilon_k 1}), \cdots \Big) \sim \text{Dirichlet}\Big( 2^{k+1} ;\text{ } \cdots ,\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 0}),\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 1}), \cdots \Big)
$$

위에서와 같이 gamma representation을 이용하면 다음과 같은 결과를 얻는다.

$$
\begin{align*}
\Big( \frac{P(A_{\varepsilon_1 \cdots \varepsilon_k 0})}{P(A_{\varepsilon_1 \cdots \varepsilon_k})}, \frac{P(A_{\varepsilon_1 \cdots \varepsilon_k 1})}{P(A_{\varepsilon_1 \cdots \varepsilon_k})} \Big) 
&\stackrel{d}{=} \Big( \frac{Z_1}{Z_1+Z_2} , \frac{Z_2}{Z_1+Z_2} \Big)\\
&\stackrel{d}{=} \text{Dirichlet}\Big( \alpha(A_{\varepsilon_1 \cdots \varepsilon_k 0}),\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 1}) \Big)
\end{align*}
$$

위에서 보인 Dirichlet distribution의 성질에 의해 다음을 얻을 수 있다.

$$
P(A_{\varepsilon_1 \cdots \varepsilon_k})  \perp\!\!\!\perp   \frac{P(A_{\varepsilon_1 \cdots \varepsilon_k 0})}{P(A_{\varepsilon_1 \cdots \varepsilon_k })} = P(A_{\varepsilon_1 \cdots \varepsilon_k 0} \mid A_{\varepsilon_1 \cdots \varepsilon_k} )= V_{\varepsilon_1 \cdots \varepsilon_k 0 }
$$

$$
V_{\varepsilon_1 \cdots \varepsilon_k 0 } = P(A_{\varepsilon_1 \cdots \varepsilon_k 0} \mid A_{\varepsilon_1 \cdots \varepsilon_k}) \sim \text{Beta}(\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 0}),\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 1})) 
$$

또한, $m=1, \cdots , k-1$인 경우 induction statement가 성립하므로 

$$
P(A_{\varepsilon_1 \cdots \varepsilon_k} \mid A_{\varepsilon_1 \cdots \varepsilon_{k-1}}) \perp\!\!\!\perp  P(A_{\varepsilon_1 \cdots \varepsilon_{k-1}} \mid A_{\varepsilon_1 \cdots \varepsilon_{k-2}}) \perp\!\!\!\perp  \cdots \perp\!\!\!\perp  P(A_{\varepsilon_1 \varepsilon_2} \mid A_{\varepsilon_1 }) \perp\!\!\!\perp  P(A_{\varepsilon_1 }) 
$$

$$
\begin{align*}
P(A_{\varepsilon_1 \cdots \varepsilon_k}) &= P(A_{\varepsilon_1 \cdots \varepsilon_k} \mid A_{\varepsilon_1 \cdots \varepsilon_{k-1}})   P(A_{\varepsilon_1 \cdots \varepsilon_{k-1}} \mid A_{\varepsilon_1 \cdots \varepsilon_{k-2}}) \cdots P(A_{\varepsilon_1 \varepsilon_2} \mid A_{\varepsilon_1 }) P(A_{\varepsilon_1 }) \\
&=V_{\varepsilon_1 \cdots \varepsilon_k} V_{\varepsilon_1 \cdots \varepsilon_{k-1}} \cdots V_{\varepsilon_1}
\end{align*}
$$

즉, 

$$
V_{\varepsilon_1 \cdots \varepsilon_k 0 }  \perp\!\!\!\perp V_{\varepsilon_1 \cdots \varepsilon_k} V_{\varepsilon_1 \cdots \varepsilon_{k-1}} \cdots V_{\varepsilon_1}
$$

따라서, 

$$
\begin{align*}
&\therefore \enspace \enspace V_{\varepsilon_1 \cdots \varepsilon_k 0 }  \perp\!\!\!\perp V_{\varepsilon_1 \cdots \varepsilon_k} \enspace \enspace \\
&\therefore \enspace \enspace  V_{\varepsilon_1 \cdots \varepsilon_k 0 } \sim \text{Beta}(\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 0}),\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 1})) \enspace \enspace \square.
\end{align*}
$$



### 3.2.3 Self-similarity

Dirichlet process는 sample space의 successive partition으로 이루어진 임의의 sequence에 대해 tail-free하다는 것을 확인하였다. 이에 더해, Dirichlet process는 더 강한 conditional independence 성질을 갖는다. Notation을 정리하자면, measure $P$와 measurable set $B$에 대해,

$$
\text{restriction measure: }\enspace   P\mid _{B}(A)=P(A \cap B) \\
\text{conditional measure: }\enspace P_B (A) = P(A \mid B),\text{ for } B \text{ with } P(B)>0.
$$

> If $P \sim \text{DP}(\alpha)$, then $P_B \sim \text{DP}(\alpha \mid_{ B})$, and the variable and processes $P(B)$, $(P_B(A):A \in \mathcal X)$, $(P_{B^c}(A):A \in \mathcal X)$ are mutually independent, for any $B \in \mathcal X$ such that $\alpha(B)>0$

증명에 앞서 이 정리의 의미를 생각해보자. 

 * Dirichlet process는 먼저 어떤 set $B$로 conditioning을 함으로써 "localize"해도 여전히 Dirichlet process이다. 그리고 그 때의 base measure는 set $B$로 restrict된 measure이다.
 * 서로 disjoint한 set에 각각 localize된 stochastic process들은 서로 독립이다. 또한 확률변수 $P(B)$와도 독립이다.

어떤 locality($B$)가 주어지면, probability mass는 그 $B$ 밖의 사건들과는 상관없이 Dirichlet process에 의해 분배될 것이다. 즉 Dirichlet process는 local하게 자기자신과 유사하다는 점에서 이 성질을 self-similarity라고 부른다. $P(B)$, $(P_B(A):A \in \mathcal X)$, $(P_{B^c}(A):A \in \mathcal X)$가 서로 independent라는 것은 Dirichlet process의 tail-freeness에 의해 만족한다. Conditional measure가 base measure를 같은 set에 restrict시킨 것과 같은 Dirichlet process를 갖는 사실은 tail-freeness를 보일 때 사용했던 것과 같은 gamma representation을 이용하면 쉽게 보일 수 있다.



### 3.2.4 Conjugacy

Dirichlet process prior의 가장 유용한 성질 중 하나는 ***conjugacy***, 즉 posterior distribution 역시 Dirichlet process를 따른다는 점이다. Observation $X_1, \cdots , X_N$이 Dirichlet process prior로부터 생성된 distribution $P$로부터 independent하게 sample되었다고 하자.

$$
\begin{align*}
P &\sim \text{DP}(\alpha) \\
X_i \mid P &\stackrel{\text{iid}}{\sim} P , \enspace \enspace \enspace i=1, \cdots , N
\end{align*}
$$

뭉뚱그린 표현이지만, 이와 같이 생성된 observation을 편의상 주로 *sample from the Dirichlet process*라고 표현한다. 이 때 다음 정리가 성립한다.  
  
 > Given an i.i.d sample $X_1, \cdots , X_N $ from the $\text{DP}(\alpha)$-process, the $\text{DP}(\alpha + \sum_{i=1}^{N} \delta_{X_i})$-process is a version of the posterior distribution.

#### Necessary Concepts

Dirichlet process의 증명을 위해, 몇 가지 개념들에 대한 정확한 정의가 필요하다. Conditional expectation과 conditional probability에 대한 정의는 다음과 같다.

 > Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $\mathcal G$ be a sub-$\sigma$-field of $\mathcal F$, i.e. , $\mathcal G$ is a $\sigma$-algebra and $\mathcal G \subset \mathcal F$. 
 > 
 > The **conditional expectation** of a random variable $X$ given $\mathcal G$, $\mathbb E(X \mid \mathcal G )$, is any $\mathcal G$-measurable function, $Y$, satisfying the following.
 > 
 > $$
 > \mathbb E[ Y I_A ] = \mathbb E[ X I_A ], \text{ for all } A \in \mathcal G
 > $$
 > 
 > Any $Y$ satisfying the above definition is said to be a version of $\mathbb E(X \mid \mathcal G )$.
 > 
 > The **conditional probability** of an event $A \in \mathcal F$ given $\mathcal G$, $\mathbb P(A \mid \mathcal G )$, is a conditional expectation of a indicator random variable $I_A$, $\mathbb E(I_A \mid \mathcal G )$, i.e., any $\mathcal G$-measurable function, $Y$, satisfying the following.
 > 
 > $$
 > \mathbb E[ Y I_B ] = \mathbb E[ I_A I_B ]=\mathbb E[ I_{A\cap B} ]=\mathbb P(A \cap B), \text{ for all } B \in \mathcal G
 > $$
 > 
 > Any $Y$ satisfying the above definition is said to be a version of $\mathbb P( A \mid \mathcal G )$.
 > 
 > Also, we define
 > 
 > $$
 > \mathbb E(X \mid Y)= \mathbb E(X \mid \sigma(Y))
 > $$
 > 
 > where $\sigma(Y)$ is the $\sigma$-algebra generated by a random variable $Y$.
 > 

Regular conditional probability에 대한 정의는 다음과 같다.
 > Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $X:(\Omega, \mathcal F) \rightarrow (S, \mathcal S)$ be a $\mathcal F$-measurable function. Let $\mathcal G$ be a $\sigma$-algebra such that $\mathcal G \subset \mathcal F$. A mapping $\mu : \Omega \times \mathcal S \rightarrow [0,1]$ is a **regular conditional distribution** for $X$ given $\mathcal G$, if the following holds:
 > 1. $\forall A \in \mathcal S$, the map $\omega \mapsto \mu(\omega, A)$ is a version of $\mathbb P(X \in A \mid \mathcal G)$.  
 > 2. For a.e. $\omega \in \Omega$, the map $A \mapsto \mu(\omega, A)$ is a probability measure on $(S, \mathcal S)$.
 > 
 > When $S = \Omega$ and $X$ is the identity map, then a mapping $\mu : \Omega \times \mathcal F \rightarrow [0,1]$ is a **regular conditional probability** given $ \mathcal G$, if the following holds:
 > 1. $\forall A \in \mathcal F$, the map $\omega \mapsto \mu(\omega, A)$ is a version of $\mathbb P(X \in A \mid \mathcal G)$.  
 > 2. For a.e. $\omega \in \Omega$, the map $A \mapsto \mu(\omega, A)$ is a probability measure on $(\Omega, \mathcal F)$.
 > 

마지막으로, dirichlet process의 conjugacy를 증명하는데 사용될 monotone class theorem for functions를 소개하겠다. 먼저 일반적인 monotone class theorem은 다음과 같다. 자세한 내용은 [Durrett - Probability: Theory and Examples](https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf)의 Theorem 5.2.2를 참고하자.

> **(Monotone Class Theorem)** Let $\mathcal A$ be a $\pi$-system that contains $\Omega$ and let $\mathcal H$ be a collection of real-valued functions that satisfies:
> 
> 1. If $A \in \mathcal A$, then $1_A \in \mathcal H$.
> 2. If $f,g \in \mathcal H$, then $f+g, cf \in \mathcal H$ for any $c \in \mathbb R$.
> 3. If $f_n \in \mathcal H$ are nonnegative and increase to a bounded function $f$, then $f \in \mathcal H$
> 
> Then $\mathcal H$ contains all bounded functions which are measurable with respect to $\sigma(\mathcal A)$.
>

이는 다음과 같은 증명 과정을 따른다.

 * $\mathcal A$ be a $\pi$-system that contains $\Omega$.
 * $\mathcal G= \left\{ A : 1_A \in \mathcal H \right\}$ is a $\lambda$-system.
	 * $\mathcal A$ contains $\Omega$, $\mathcal H$ contains $1_\Omega$, then $\mathcal G$ contains $\Omega$.
	 * If $A,B \in \mathcal G$ satisfying $A \subset B$, $\mathcal H$ contains $1_A, 1_B$, and $1_{B \backslash A} = 1_B - 1_A$. Then $B \backslash A \in \mathcal G$
     * $A_1, A_2 , \cdots \in \mathcal G$ satisfying $A_n \subset A_{n+1}$, $\forall n$, then $1_{A_1}, 1_{A_2} , \cdots \in \mathcal H$ satisfying $1_{A_n} \leq 1_{A_{n+1}}$, $\forall n$. By the condition 3, $\lim_{n\rightarrow\infty}1_{A_n} = 1_{\cup_{n=1}^\infty A_n} \in \mathcal H$ because $\lim_{n\rightarrow\infty}1_{A_n} \leq 1 $ a.e.. Then $\cup_{n=1}^\infty A_n \in \mathcal G$.
 * Then, by Dynkin's $\pi-\lambda$ theorem, $\sigma(\mathcal A) \subset \mathcal G=\left\{ A : 1_A \in \mathcal H \right\}$.
 * The condition 2 implies $\mathcal H$ contains all simple functions, with respect to $\sigma(\mathcal A)$.
 * The condition 3 implies $\mathcal H$ contains all bounded measurable functions, with respect to $\sigma(\mathcal A)$. $\square$
  
이를 이용하여 다음과 같은 monotone class theorem for functions를 얻을 수 있다.

> A collection of real-valued functions, $\mathcal M$, is a **multiplicative class** if
> 
> $$
> f,g \in \mathcal M \implies f \cdot g \in \mathcal M
> $$
> 
> A collection of real-valued bounded functions, $\mathcal H$, is a **monotone vector space** if the following hold.
> 
> 1. $\mathcal H$ is a vector space over $\mathbb R$.
> 2. $1 = 1_\Omega \in \mathcal H$.
> 3. $f_n \geq 0, f_n \in \mathcal H, f_n \uparrow f, f: \text{bdd} \implies f \in \mathcal H$.
> 
> **(Monotone Class Theorem For Functions)** Let $\mathcal M$ be a multiplicative class of bounded real-valued functions on $\Omega$ and Let $\mathcal H$ be a monotone vector space of real-valued bounded functions. Let $\mathcal B$ be a $\sigma$-algebra that makes all elements of $\mathcal M$ measurable, i.e., $\mathcal B = \sigma (\mathcal M)$. Then the following holds.
> 
> $$
> \mathcal M \subset \mathcal H \implies \mathcal H \text{ contains all bounded } \mathcal B \text{-measurable real-valued functions}
> $$
> 

이는 다음과 같은 증명 과정을 따른다.

 * We may assume that $\text 1_\Omega \in \mathcal M$, because if $\mathcal M$ is a multiplicative class then $\mathcal M \cup \{ 1_{\Omega} \}$ is also a multiplicative class.
 * Let $\mathcal A$ be a $\pi$-system on $\Omega$ generating $\mathcal B = \sigma (\mathcal M)$.
 * If we show that $1_A \in \mathcal H$ for all $A \in \mathcal A$, then, by monotone class theorem, $\mathcal H$ contains all bounded functions which are measurable with respect to $\sigma(\mathcal A)=\mathcal B$.
	 * Let 
 * **(보충 필요)**

<br>


### Proof of Conjugacy of Dirichlet Process

이제 Dirichlet process의 posterior 분포를 알아보자.

 > **(Conjugacy)** Given an i.i.d sample $X_1, \cdots , X_N $ from the $\text{DP}(\alpha)$-process, 
 > 
 > $$
\begin{align*}
P &\sim \text{DP}(\alpha) \\
X_i \mid P &\stackrel{\text{iid}}{\sim} P , \enspace \enspace \enspace i=1, \cdots , N
\end{align*}
$$
 >
 > the $\text{DP}(\alpha + \sum_{i=1}^{N} \delta_{X_i})$-process is a version of the posterior distribution.
 > 

우리는 이를 증명하기 위해서 sample의 갯수가 $1$개일 때의 상황을 증명하면 된다. $N$개의 sample일 때의 결과는 같은 과정을 $N$번 반복하여 얻을 수 있다. 

$$
\begin{align*}
P \mid X_1 , \cdots , X_{N-1} &\sim \text{DP} \left( \alpha+ \sum_{i=1}^{N-1} \delta_{X_i} \right) \\
X \mid P, X_1 , \cdots , X_{N-1} &\sim P\\
P \mid X_1 , \cdots , X_N &\sim \text{DP} \left( \alpha + \sum_{i=1}^{N} \delta_{X_i} \right)
\end{align*}
$$

따라서 다음의 증명을 소개하겠다.

$$
\begin{align*}
P &\sim \text{DP} \left( \alpha \right) \\
X \mid P &\sim P\\
\implies P \mid X &\sim \text{DP} \left( \alpha +  \delta_{X} \right)
\end{align*}
$$

보다 정확히 이야기하자면, **$\text{DP}(\alpha + \delta_X)$의 distribution을 따르는 어떤 random probability measure $\tilde P$가, $X$가 주어졌을 때의 $P$의 regular conditional probability의 한 version이 된다**는 것이다. 

$$
\mathbb P(P \in M \mid X=x)=\text{DP}_{\alpha +\delta_x}(M), \enspace \text{ for all }x \in \mathcal X, M \in \mathcal M
$$

여기서 주의할 점은 $M$은 The Borel $\sigma$-algebra generated by weakly open sets on $\mathfrak M =M(\mathfrak X)$,  $\mathcal M$의 원소이므로, probability measure들을 원소로 갖는 measurable set이라는 점이다. 다시 소개하자면, regular conditional probability의 정의는 다음과 같다.

 > A mapping $\mu : \Omega \times \mathcal F \rightarrow [0,1]$ is a **regular conditional probability** given sub-$\sigma$-algebra $ \mathcal G$, if the following holds:
 > 1. For a.e. $\omega \in \Omega$, the map $A \mapsto \mu(\omega, A)$ is a probability measure on $(\Omega, \mathcal F)$.
 > 2. $\forall A \in \mathcal F$, the map $\omega \mapsto \mu(\omega, A)$ is a version of $\mathbb P(A \mid \mathcal G)$.  
 >  

우리는 $\text{DP}(\alpha + \delta_X)$가, $\mathbb P(P \in M \mid X)$에 대해, 위 두 조건을 만족시키는 것을 보이고자 한다. 정확한 의미를 이해하기 위해 notation을 엄밀하게 적으면 다음과 같다.

> Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $X : (\Omega, \mathcal F) \rightarrow (\mathfrak X, \mathcal X)$ be a random variable, and $P:(\Omega, \mathcal F) \rightarrow (\mathfrak M, \mathcal M)$ a random probability measure, where $\mathfrak M$ is the space of probability measures on $(\mathfrak X, \mathcal X)$ and $\mathcal M$ is the Borel $\sigma$-algebra on $\mathfrak M$ generated by weak topology. For $M \in \mathcal M$,
> 
> $$
> \begin{align*}
> \mathbb P(P \in M \mid X) &= \mathbb P \Big( \{ \omega: P(\omega) \in M \} \mid \sigma(X) \Big) \\
> &= \mathbb P \Big( P^{-1}(M) \mid \sigma(X) \Big)
> \end{align*}
> $$

일반적인 *probability measure* $(\mathbb P)$와 어떤 random variable(여기서는 "probability measure"-valued random variable, 즉 random probability measure)에 대한 *induced probability measure* $(P^X = \mathbb P \cdot X^{-1})$의 관계를 이용하여, 우리의 상황에 맞게 regular conditional probability의 조건을 다시 적으면 아래와 같다.

 > A mapping $\mu : \Omega \times \mathcal M \rightarrow [0,1]$ is a **(induced) regular conditional probability** given sub-$\sigma$-algebra $ \sigma(X)$, if the following holds:
 > 1. For a.e. $\omega \in \Omega$, the map $M \mapsto \mu(\omega, M)$ is a probability measure on $(\mathfrak M, \mathcal M)$.
 > 2. $\forall M \in \mathcal M$, the map $\omega \mapsto \mu(\omega, M)$ is a version of $\mathbb P( P^{-1}(M) \mid \mathcal \sigma(X))$.
 >

따라서, 다음 두 가지 사실을 보이면 증명이 끝난다.  

$$
1. \forall \omega \in \Omega, \text{ DP}_{\alpha + \delta_{X(\omega)}}(\cdot) \text{ is a probability measure on } (\mathfrak M, \mathcal M). \\
2. \forall M \in \mathcal M, \text{ DP}_{\alpha + \delta_{X(\omega)}}(M) \text{ is a version of the conditional probability } \mathbb P(P \in M \mid X).
$$

### Fact 1

Fact 1은 쉽게 보일 수 있다. 어떤 fixed $\omega \in \Omega$에 대해서, $\alpha(\cdot)+\delta_{X(\omega)}(\cdot)$는 finite measure이다. 따라서, 그에 대해 $\text{ DP}_{\alpha + \delta_{X(\omega)}}(\cdot)$는 Dirichlet process이며, 즉 probability measure on the space of probability measure이기 때문이다.  
  
### Fact 2

Fact 2는 어떤 임의의 $ M \in \mathcal M$에 대하여, $\text{ DP}_{\alpha + \delta_{X(\omega)}}(M)$이 conditional probability의 조건을 만족하는지를 확인함으로써 보일 수 있다. 다시 소개하자면, conditional probability의 정의는 다음과 같다.

 > The **conditional probability** of an event $A \in \mathcal F$ given $\mathcal G$, $\mathbb P(A \mid \mathcal G )$, is a conditional expectation of a indicator random variable $I_A$, $\mathbb E(I_A \mid \mathcal G )$, i.e., any $\mathcal G$-measurable function, $Y$, satisfying the following.
 > 
 > $$
 > \mathbb E[ Y I_B ] = \mathbb E[ I_A I_B ]=\mathbb E[ I_{A\cap B} ]=\mathbb P(A \cap B), \text{ for all } B \in \mathcal G
 > $$
 > 
 > Any $Y$ satisfying the above definition is said to be a version of $\mathbb P( A \mid \mathcal G )$.
 >

즉, 임의의 $M \in \mathcal M$에 대해서 $\text{ DP}_{\alpha + \delta_{X(\omega)}}(M)$이 a version of $\mathbb P( P^{-1}(M) \mid \sigma(X))$이기 위해서는,

 * $(\text{Fact } 2.1) \enspace \forall M \in \mathcal M, \text{ DP}_{\alpha + \delta_{X(\omega)}}(M) \text{ is } \sigma(X) \text{-measurable.}$
 * $(\text{Fact }2.2) \enspace \forall M \in \mathcal M, \forall B \in \mathcal X,$

$$
\mathbb E[DP_{\alpha+\delta_X}(M) \cdot I(X \in B)]=\mathbb P(P \in M, X \in B)
$$

### Fact 2.1

$$
\forall M \in \mathcal M, \text{ DP}_{\alpha + \delta_{X(\omega)}}(M) \text{ is } \sigma(X) \text{-measurable.}
$$

**(보충 필요)**

### Fact 2.2

For all $M \in \mathcal M$, and $B \in \mathcal X$,

$$
\mathbb E[DP_{\alpha+\delta_X}(M) \cdot I(X \in B)]=\mathbb P(P \in M, X \in B)
$$

먼저 좌변을 정리하면 아래와 같다. 세 번째 등호는 "Dirichlet process(에서 생성된 $P$)로부터 생성된 observation" $X$의 marginal 분포가 Dirichlet process의 center measure $G_0 = \frac{1}{\alpha_0} \alpha$라는 점을 이용한 것이다.

$$
\begin{align*}
\text{LHS} &= \mathbb E[DP_{\alpha+\delta_X}(M) \cdot I(X \in B)]\\
&= \int_\left\{ \omega : X(\omega) \in B \right\} DP_{\alpha+\delta_{X(\omega)}}(M)) \text{ } \mathbb P(d \omega) \\
&= \int_B DP_{\alpha+\delta_x}(M)) \text{ } G_0( dx) \\
&= \int_B \int I(P \in M) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) 
\end{align*}
$$


우변을 정리하면 아래와 같다. 

$$
\begin{align*}
\text{RHS} &= \mathbb P(P \in M, X \in B) \\
&= \mathbb P(P(B_1) \leq t_1, \cdots , P(B_k) \leq t_k,  X \in B) \\
&= \int_M P(B) \text{ DP}_\alpha(dP) \\
&= \int I(P \in M) P(B) \text{ DP}_\alpha(dP) \\
\end{align*}
$$

즉, Fact 2.2를 보이기 위해서는 다음 등식이 성립함을 보여야 한다.

$$
\int_B \int I(P \in M) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int I(P \in M) P(B) \text{ DP}_\alpha(dP)
$$

이를 위해 위에서 소개한 monotone class theorem for functions를 사용할 것이다. 다시 소개하자면, 이 정리는 bounded real-valued function on $\Omega$로 이루어진 multiplicative class $\mathcal C$에 대해서, 이 multiplicative class가 bounded real-valued function으로 이루어진 monotone vector space $\mathcal H$의 부분집합이 될 때, $\mathcal C$로 generate된 $\sigma$-algebra $\sigma(\mathcal C)$에 대해, monotone vector space $\mathcal H$는 $\sigma(\mathcal C)$-measurable한 모든 bounded real-valued을 포함한다는 것을 의미한다.

다음과 같이 multiplicative class $\mathcal C$과 monotone vector space $\mathcal H$를 설정하겠다.

$$
\mathcal C = \left\{ f(P)= P(B_1)^{r_1} \cdots P(B_k)^{r_k} : k \in \mathbb N , B_i \in  \mathcal B, r_i \in \mathbb N, i= 1, \cdots, k\right\} \\
\mathcal H = \left\{ f : \mathfrak M \rightarrow \mathbb R\text{, bounded, }\mathcal M \text{- measurable, }\int_B \int f(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int f(P) P(B) \text{ DP}_\alpha(dP), \forall B \in \mathcal B \right\}
$$

위와 같이 정의한 $\mathcal C$가 multiplicative class of bounded real function이라는 것은 자명하다. 또한 $\mathcal C$는 다음과 같은 사실들을 만족한다. $\mathcal M = \sigma(\mathcal C_0)$는 Ghosal & Van der Vaart, proposition A.5.(i)에 따른 결과이다.

> $$
> \pi_B : \mathfrak M \rightarrow \mathbb R \text{ such that }P \mapsto P(B)
> $$
> 
> $\sigma(\{ \pi_B : B \in \mathcal B \})$ is the smallest $\sigma$-algebra on $\mathfrak M$ making $\pi_B$ measurable for all $B \in \mathcal B$. $\mathcal M$ is the Borel $\sigma$-algebra on $\mathfrak M$ for the weak topology. Then
>
> $$
> \sigma(\{ \pi_B : B \in \mathcal B \}) =\mathcal M 
> $$

$\mathcal H$가 monotone vector space라는 것을 보이자. 
 * $f_1 , f_2 \in \mathcal H, c_1, c_2 \in \mathbb R \implies c_1f_1 +c_2f_2 \in \mathcal H$

$$
\begin{align*}
&\int_B \int \{ c_1f_1(P) + c_2f_2(P) \} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&=c_1 \int_B \int f_1(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) +c_2 \int_B \int f_2(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx)\\
&= c_1 \int f_1(P) P(B) \text{ DP}_\alpha(dP)+c_2 \int f_2(P) P(B) \text{ DP}_\alpha(dP)\\
&= \int \{ c_1f_1(P) + c_2f_2(P) \}  P(B) \text{ DP}_\alpha(dP)
\end{align*}
$$


 * $1 \in \mathcal H \text{ (constant function of value }1)$

$$
\int_B \int 1(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int_B \left( \int \text{ }DP_{\alpha+\delta_x}(dP) \right) G_0( dx) = \int_B 1 \text{ } G_0( dx) = G_0(B)
$$

$$
\int 1(P) P(B) \text{ DP}_\alpha(dP) = \int  P(B) \text{ DP}_\alpha(dP) =\mathbb E[P(B)] = G_0(B)
$$

 * $f_n \geq 0, f_n \in \mathcal H, f_n \uparrow f, f: \text{bdd} \implies f \in \mathcal H$

$$
\begin{align*}
&\int_B \int f(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&= \int_B \int \lim_{n\rightarrow \infty} f_n(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&= \lim_{n\rightarrow \infty} \int_B \int  f_n(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \text{ (} \because \text{ DCT}) \\
&= \lim_{n\rightarrow \infty} \int f_n(P) P(B) \text{ DP}_\alpha(dP) \text{ (} \because \text{ } f_n \in \mathcal H) \\
&=  \int  \lim_{n\rightarrow \infty} f_n(P) P(B) \text{ DP}_\alpha(dP) \text{ (} \because \text{ DCT}) \\
&= \int  f(P) P(B) \text{ DP}_\alpha(dP)  \\
\end{align*}
$$

이제 $\mathcal C \subset \mathcal H$를 보이면 우리가 정의한 $\mathcal C$, $\mathcal H$에 대해 monotone class theorem을 적용할 수 있다. 다음과 같이 multiplicative class $\mathcal C$의 subcollection $\mathcal C^\ast$를 정의하자. $\mathcal C$의 조건에 $(B_1, \cdots , B_k)$이 $\mathfrak X$의 partition이 된다는 조건을 추가한 것이다.

$$
\mathcal C^\ast = \left\{ f(P)= P(B_1)^{r_1} \cdots P(B_k)^{r_k} : k \in \mathbb N , (B_1, \cdots , B_k) \text{ : partition of }\mathfrak X , B_i \in \mathcal B, r_i \in \mathbb N, i= 1, \cdots, k\right\} 
$$

임의의 $B_i \in \mathcal B$에 대해, $B_1 \cup \cdots \cup B_k$를 disjoint한 조각으로 나누는 partition, $\tilde B_1 , \cdots, \tilde B_l$을 고려하면, $\mathcal C$의 모든 원소들은 $\mathcal C^\ast$의 원소들의 선형결합으로 나타낼 수 있다. monotone vector space $\mathcal H$는 선형결합에 닫혀있기 때문에, $\mathcal C^\ast$이 $\mathcal H$에 들어가는 것을 보이면 된다.  
  
$\mathcal B$의 원소로 구성된 $\mathfrak X$의 임의의 finite partition $(B_1, \cdots , B_k)$를 생각해보자. 또한 그에 대해 다음과 같이 $\alpha ' \in \mathbb R^k$를 정의하자.

$$
\alpha ' = (\alpha(B_1), \cdots , \alpha(B_k)) = (\alpha_1, \cdots , \alpha_k)
$$

만약 $\mathfrak X$의 임의의 finite partition $(B_1, \cdots , B_k)$, $r_1 , \cdots, r_k \in \mathbb R$에 대해 $\prod_{i=1}^{k} P(B_i)^{r_i}$가 아래 식을 만족한다면, $\mathcal C^\ast \subset \mathcal H$이 된다.

$$
\int_B \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int \prod_{i=1}^{k} P(B_i)^{r_i} P(B) \text{ DP}_\alpha(dP)
$$

좌변을 정리하면 아래와 같다.

$$
\begin{align*}
\text{LHS } &= \int_B \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&=  \sum_{j=1}^{k} \int_{B \cap B_j} \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
\end{align*}
$$

Dirichlet process의 정의에 따라, $(P(B_1),\cdots, P(B_k))$는 Dirichlet distribution를 따른다. 또한 첫 번째 적분기호를 $x$가 어느 partition에 들어가는지에 따라 적분 항을 쪼갰으므로, $\int_{B \cap B_j} \cdots G_0(dx)$ 안에서는 $(P(B_1),\cdots, P(B_k))$가 $\text{Dirichlet}(\alpha ' + e_j)$의 분포를 갖는다. 그러므로,

$$
\begin{align*}
\text{LHS } &= \int_B \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&=  \sum_{j=1}^{k} \int_{B \cap B_j} \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&=  \sum_{j=1}^{k} \int_{B \cap B_j} \mathbb E \left[ \prod_{i=1}^{k} P(B_i)^{r_i} \right] G_0( dx) \\
&=  \sum_{j=1}^{k} \int_{B \cap B_j} \mathbb E \left[ \prod_{i=1}^{k} Y_i^{r_i} \right] G_0( dx)  \\
&\text{ (where } Y \sim \text{Dirichlet}(\alpha ' +e_j)=\text{Dirichlet}(\alpha_1 , \cdots , \alpha_j+1, \cdots, \alpha_k))\\
&= \sum_{j=1}^{k}  \mathbb E \left[ \prod_{i=1}^{k} Y_i^{r_i} \right] G_0(B \cap B_j) \\
&= \sum_{j=1}^{k}  G_0(B \cap B_j) \int y_1^{r_1} \cdots y_j^{r_j} \cdots y_k^{r_k} \frac{\Gamma(\sum_{i=1}^{k}\alpha_i +1)}{\Gamma(\alpha_1)\cdots \Gamma(\alpha_j+1) \cdots \Gamma(\alpha_k)} y_1^{\alpha_1-1} \cdots y_j^{\alpha_j} \cdots y_k^{\alpha_k-1} dy \\
&= \sum_{j=1}^{k}  G_0(B \cap B_j) 
\frac{\sum_{i=1}^{k}\alpha_i}{\alpha_j}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \frac{\Gamma(\sum_{i=1}^{k}\alpha_i )}{\Gamma(\alpha_1)\cdots \Gamma(\alpha_j) \cdots \Gamma(\alpha_k)} y_1^{\alpha_1-1} \cdots y_j^{\alpha_j-1} \cdots y_k^{\alpha_k-1} dy \\
&= \sum_{j=1}^{k}  G_0(B \cap B_j) 
\frac{\sum_{i=1}^{k}\alpha_i}{\alpha_j}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \text{Dirichlet}_{\alpha '} (dy) \\
&= \sum_{j=1}^{k}  G_0(B \cap B_j) 
\frac{\alpha(\mathfrak X)}{\alpha(B_j)}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \text{Dirichlet}_{\alpha '} (dy) \\
&= \sum_{j=1}^{k} \frac{\alpha(B \cap B_j)}{\alpha(B_j)}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \text{Dirichlet}_{\alpha '} (dy) \\
\end{align*}
$$

우변을 정리하면 아래와 같다. 5번째 등호는 Dirichlet process의 tail-free property에 의해 $P(B_1),\cdots, P(B_k)$와 $P( B \mid B_j), P( B^c \mid B_j)$가 서로 독립인 것에서 온 결과이다. 또한, 6번째 등호는 Dirichlet process를 partition으로 refine하며 무한히 쪼갤 때의 splitting variable이 Beta분포를 따르는 것에서 온 결과이다. 이에 대한 자세한 내용은 [3.2 Properties](#32-properties)의 tail-freeness section에서 소개하였다.



$$
\begin{align*}
\text{RHS } &= \int \prod_{i=1}^{k} P(B_i)^{r_i} P(B) \text{ DP}_\alpha(dP) \\
&= \int \left( \prod_{i=1}^{k}  P(B_i)^{r_i} \right) \sum_{j=1}^k P(B \cap B_j) \text{ DP}_\alpha(dP) \\
&= \sum_{j=1}^k \int \left( \prod_{i=1}^{k} P(B_i)^{r_i} \right) P(B \cap B_j) \text{ DP}_\alpha(dP) \\
&= \sum_{j=1}^k \int P(B_1)^{r_1} \cdots P(B_j)^{r_j+1} \cdots P(B_k)^{r_k} \frac{P(B \cap B_j)}{P(B_j)} \text{ DP}_\alpha(dP) \\
&= \sum_{j=1}^k \Bigg[ \int P(B_1)^{r_1} \cdots P(B_j)^{r_j+1} \cdots P(B_k)^{r_k} \text{ DP}_\alpha(dP) \Bigg] \Bigg[ \int \frac{P(B \cap B_j)}{P(B_j)} \text{ DP}_\alpha(dP) \Bigg] \\
&= \sum_{j=1}^{k} \frac{\alpha(B \cap B_j)}{\alpha(B_j)}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \text{Dirichlet}_{\alpha '} (dy) \\
&\left( \because \enspace P(B \mid B_j)=\frac{P(B \cap B_j)}{P(B_j)} \sim \text{Beta}\big(\alpha(B \cap B_j),\alpha(B^c \cap B_j) \big)  \right)\\
\end{align*}
$$

즉 $\mathcal C^\ast$이 $\mathcal H$의 부분집합이 되며, 그에 따라 $\mathcal C$ 역시 $\mathcal H$의 부분집합이 된다.  
  
따라서, monotone class theorem for function에 의해, $\mathcal H$는 모든 bounded real valued $\sigma(\mathcal C)$-measurable 함수를 포함한다. 또한 위에서 확인했듯이, $\mathcal M = \sigma(\mathcal C)$이므로, $\mathcal H$는 모든 bounded real valued $\mathcal M$-measurable 함수를 포함한다. 다시 말해서, 임의의 bounded real valued $\mathcal M$-measurable function $f$에 대하여, 아래의 식이 성립한다는 것이 된다.


$$
\int_B \int f(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int f(P) P(B) \text{ DP}_\alpha(dP)
$$

그런데 $I(P \in M)$는 임의의 $M \in \mathcal M$에 대해 항상 bounded real valued $\mathcal M$-measurable function이다. 따라서 우리가 보이고자 했던 Fact 2.2가 증명되었다.

$$
\int_B \int I(P \in M) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int I(P \in M) P(B) \text{ DP}_\alpha(dP)
$$

따라서, Fact 1, Fact 2, Fact 2.1, Fact 2.2에 의해,  **$\text{DP}(\alpha + \delta_X)$의 distribution을 따르는 어떤 random probability measure $\tilde P$가, $X$가 주어졌을 때의 $P$의 regular conditional probability의 한 version이 된다**. $\square$  
  
긴 증명이었지만 결론은, 어떤 random measure의 prior distribution이 $\text{DP}(\alpha)$였다면, $N$개의 자료를 관찰한 후의 posterior distribution은 $\text{DP}(\alpha + \sum_{i=1}^{N} \delta_{X_i})$라는 것이다. 

$$
P \mid X_1, \cdots , X_N \sim \text{DP}\left( \alpha + \sum_{i=1}^{N} \delta_{X_i} \right)
$$

Dirichlet process의 base measure를 $\alpha_0$와 probability measure $G_0$를 이용해 parametrize한 경우는 posterior distribution이 아래와 같다.

$$
\begin{align*}
P \mid X_1, \cdots , X_N &\sim \text{DP}\left( \frac{1}{\alpha_0 + N} \left\{ \alpha + \sum_{i=1}^{N} \delta_{X_i} \right\} \right) \\
&=\text{DP}\left( \frac{\alpha_0}{\alpha_0 + N} G_0 + \frac{N}{\alpha_0 + N} \mathbb P_N  \right) ,\\
\text{where }\mathbb P_N = \frac{1}{N} &\sum_{i=1}^{N} \delta_{X_i} \text{ is the empirical distribution of } X_1, \cdots, X_N \\ 
\end{align*}
$$

위에서 Dirichlet process의 mean과 variance를 도출했던 것과 같은 방식으로, posterior mean, variance을 구하면 다음과 같다.

$$
\begin{align*}
\mathbb E [P(A) \mid X_1, \cdots , X_N ] &= \frac{\alpha_0}{\alpha_0 + N} G_0(A) + \frac{N}{\alpha_0 + N} \mathbb P_N (A)  \\
&\stackrel{\text{let}}{=} \tilde{\mathbb P}_N (A)\\
\text{Var} [P(A) \mid X_1, \cdots , X_N ] &= \frac{\tilde{\mathbb P}_N (A) \tilde{\mathbb P}_N (A^c)}{\alpha_0 + N +1} 
\end{align*}
$$

의미를 살펴보자면 다음과 같다.

 * Dirichlet process의 posterior mean measure는 prior mean measure$(G_0)$와 empirical distribution$(\mathbb P_N)$의 convex combination이다.
  * Weight는 각각 $\frac{\alpha_0}{\alpha_0 + N}, \frac{N}{\alpha_0 + N}$이다.
  * 따라서, base measure $\alpha$의 전체집합에 대한 measure 값인 $\alpha_0 = \alpha(\mathfrak X)$가 클 수록, 자료의 sample 수$(N)$가 작을 수록, posterior mean measure가 prior mean measure에 가깝게 된다.
  * 이러한 관점에서 $\alpha_0 = \alpha(\mathfrak X)$를 ***prior sample size***, 그리고 $\alpha_0 + N$을 ***posterior sample size***라고 부른다.
  * 또한 자료의 수 $N$이 커짐에 따라, posterior mean measure는 almost sure하게 empirical distribution과 같은 asymptotic behavior를 보인다. 따라서 적절한 조건이 만족된다면, posterior mean measure 역시 empirical distribution과 같이 consistency, asymptotic normality와 같은 성질을 갖는다.
  * 마찬가지로 만약 우리의 자료가 true distribution $P_0$에서 sample되었다면, posterior mean measure은 자료의 수가 커짐에 따라 almost sure하게 $P_0$로 수렴할 것이다.

 * Posterior mean measure뿐만 아니라, $P$의 full posterior distribution 역시 mean measure로 수축할 것이다. 이는 다음과 같이 posterior variance measure가 bound되는 것을 이용해서 확인할 수 있다.

$$
\begin{align*}
\text{Var} [P(A) \mid X_1, \cdots , X_N ] &= \frac{\tilde{\mathbb P}_N (A) \tilde{\mathbb P}_N (A^c)}{\alpha_0 + N +1} \leq  \frac{1}{4(\alpha_0 + N +1)} \\ 
&\longrightarrow 0 \enspace ,\enspace \enspace \text{as }N \rightarrow \infty
\end{align*}
$$

  
<br>

### 3.2.5 Marginal and Conditional Distribution
### 3.2.6 Discreteness


## 3.3 Constructions
### 3.3.1 Construction via a Stochastic Process
Kolmogorov-consistency theorem

### 3.3.2 Construction through a Distribution Function
### 3.3.3 Construction through a Gamma Process

CRM

### 3.3.4 Construction through Polya Urn Scheme

de Finetti measure of polya urn scheme is DP?

### 3.3.5 Stick-Breaking Representation(Sethuraman Representation)

Stick-Breaking Representation, 혹은 Sethuraman Representation은 Beta 분포에 기반한 stick-breaking 과정을 통해 Dirichlet process를 construct하는 방법을 제시한다. 
> If $\theta_1, \theta_2, \cdots \stackrel{\textit{iid}}{\sim} G_0$ and $V_1, V_2, \cdots \stackrel{\textit{iid}}{\sim} \text{Beta}(1,M)$ are independent random variables and $W_j = V_j \prod_{l=1}^{j-1} (1-V_l) $, then $\sum_{j=1}^{\infty} W_J \delta_{\theta_j} \sim \text{DP}(M G_0)$.

증명은 아래와 같다.

$$
\begin{align*}
 1 - \sum_{j=1}^{\infty} W_j &= 1- V_1 + V_2(1-V_1) + V_3(1-V_2)(1-V_1) + \cdots \\
 &= (1-V_1)\left[  V_2 + V_3(1-V_2) + \cdots \right] \\
 &= \prod_{l=1}^{\infty} (1-V_l) \\
\text{E}\left[ \prod_{l=1}^{j} (1-V_l) \right] &\stackrel{ind}{=} \prod_{l=1}^{j} \text{E}\left[  (1-V_l) \right] \\
 &= \left( \frac{M}{M+1} \right)^j \\
 &\longrightarrow 0, \enspace \text{ as } j \rightarrow \infty \\
 \\
 \therefore \enspace \sum_{j=1}^{\infty} W_j &= 1- \prod_{l=1}^{\infty} (1-V_l) = 1,\enspace \text{ almost surely}
\end{align*}
$$

즉 stick-breaking weights $W_j$는 probability vector이다, with probability $1$. 따라서, 다음과 같이 정의된 random measure $P$는 probability measure이다, with probability $1$.

$$
P= \sum_{j=1}^{\infty} W_j \delta_{\theta_j}
$$

$j \geq 1$에 대해, $W_j \prime = V_{j+1} \prod_{l=2}^{j}(1-V_l)$와 $\theta_j \prime = \theta_{j+1}$을 정의하자. $j \geq 1$에 대해, 다음이 만족한다.

$$
W_{j+1}=V_{j+1} \prod_{l=1}^{j}(1-V_l)=(1-V_1)\cdot V_{j+1} \prod_{l=1}^{j}(1-V_l)=(1-V_1)W_j '
$$

$$
P= W_1 \delta_{\theta_1} + \sum_{j=2}^{\infty} W_j \delta_{\theta_j}=V_1 \delta_{\theta_1} + (1-V_1) \sum_{j=1}^{\infty} W_j ' \delta_{\theta_j '}
$$

Random measure $P'$를 $P'= \sum_{j=1}^{\infty} W_j ' \delta_{\theta_j '}$로 정의한다면, $P'$는 $P$와 정확히 같은 structure를 가지고 있기 때문에 $P$와 분포가 같다. 또한 $P'$는 $V_1$와 $\theta_1$를 포함하지 않기 때문에 $(V_1 ,\theta_1)$과 독립이다. 따라서 stick-breaking으로 construct된 random measure $P$는 다음과 같은 distributional equation을 만족한다.

$$
P \stackrel{d}{=} V \delta_{\theta} + (1-V) P
$$

다음 lemma를 이용하여 증명을 끝낼 수 있다.

 > For given independent $\theta \sim G_0$ and $V \sim \text{Beta}(1, M)$, the Dirichlet process $\text{DP}(M G_0)$ is the unique solution of the above distributional equation.  
 >   
 > **proof of the lemma**  
 >   
 > Let $P \sim \text{DP}(M G_0)$. Let $A_1, \cdots , A_k$ be any finite partition of $\mathfrak X$. Then, 
 > 
 > $$
 > (P(A_1), \cdots , P(A_k)) \sim \text{Dirichlet}(MG_0(A_1), \cdots , MG_0(A_k))
 > $$
 > 
 > For $\theta \sim G_0$, 
 > 
 > $$
 > (\delta_{\theta}(A_1), \cdots , \delta_{\theta}(A_k)) \sim \text{Multinomial}[1;(G_0(A_1), \cdots , G_0(A_k))]
 > $$
 > 
 > Then, for $Y_0 , Y_1, \cdots , Y_k \stackrel{ind}{\sim} \text{Gamma}(\alpha_i,1)$, and $\alpha_0 = 1, \alpha_i = MG_0(A_i)$, Then,
 > 
 > 
 > $$
 > \tilde P := \left( \frac{Y_1}{\sum_{i=1}^{k} Y_i} , \cdots, \frac{Y_k}{\sum_{i=1}^{k} Y_i} \right) \sim  \text{Dirichlet}(MG_0(A_1), \cdots , MG_0(A_k))
 > $$
 > 
 > $$
 > \tilde P  \perp\!\!\!\perp \left( Y_0, \sum_{i=1}^{k} Y_i \right)  
 > $$
 > 
 > $$
 > \tilde V := \frac{Y_0}{Y_0 +\sum_{i=1}^{k} Y_i  } \sim \text{Beta}(1, M)
 > $$
 > 
 > $$
 > (\tilde V , (1-\tilde V )\tilde P )=\frac{1}{Y_0 +\sum_{i=1}^{k} Y_i} \left( Y_0, Y_1, \cdots , Y_k \right)  \sim  \text{Dirichlet}(1,MG_0(A_1), \cdots , MG_0(A_k))
 > $$
 > 
 > Then,for $i=1,\cdots, k$,
 > 
 > $$
 > \tilde V e_i + (1-\tilde V )\tilde P  \sim \text{Dirichlet}(MG_0(A_1), \cdots , MG_0(A_i) + 1, \cdots , MG_0(A_k))
 > $$
 > 
 > Then, by the fact that $V \stackrel{d}{=} \tilde V$, $P \stackrel{d}{=} \tilde P$, letting $N=(\delta_{\theta}(A_1), \cdots , \delta_{\theta}(A_k))$,
 > 
 > $$
 > V N + (1-V)P  \sim \text{Dirichlet}(MG_0(A_1),  \cdots , MG_0(A_k))
 > $$
 > 
 > We have chosen a finite partition of $\mathfrak X$, $A_1, \cdots , A_k$ arbitrarily. Then, $P \sim \text{DP}(M G_0)$ is a solution for the distributional equation above.

**((uniqueness 보충 필요))**

따라서, random measure $P$는 $\text{DP}(M G_0)$을 따른다. $\square$


# Reference
> Ghosal, Subhashis, and Aad Van der Vaart. Fundamentals of nonparametric Bayesian inference. Vol. 44. Cambridge University Press, 2017.  
> K. Ghosh, J & Ramamoorthi, R. Bayesian Nonparametrics. Springer Series in Statistics. 16. 2011.  
> 이재용 교수님, 디리클레 과정 lecture note.
