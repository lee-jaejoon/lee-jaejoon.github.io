---
layout: post
title: "3. Dirichlet Process (1)"
tags: [Bayesian Nonparametrics]
comments: true
---

# 3.1 Definition

위에서와 마찬가지로 $\mathfrak M$은 Polish space $(\mathfrak X, \mathcal X)$ 위에서 정의된 probability measure들의 set이다. Dirichlet process의 정의는 아래와 같다.

 > A random measure $P$ on $(\mathfrak X, \mathcal X)$ is said to possess a Dirichlet process distribution $\text{DP}(\alpha)$ with a base measure $\alpha$, if for every finite measurable partition $A_1, \cdots , A_k$ of $\mathfrak X$,
 > 
 > $$
 > (P(A_1), \cdots , P(A_k)) \sim \text{Dirichlet}(\alpha(A_1), \cdots , \alpha(A_k)).
 > $$
 > 

위 정의에서는 $\alpha$가 임의의 finite positive Borel measure on $(\mathfrak X, \mathcal X)$이다. 이를 양수 $\alpha_0$와 probability measure $G_0$를 이용하여 아래와 같이 정의하기도 한다.

 > $$
 > (P(A_1), \cdots , P(A_k)) \sim \text{Dirichlet}( \alpha_0 G_0(A_1), \cdots , \alpha_0 G_0(A_k)).
 > $$

이 때 양수 $\alpha_0$와 probability measure $G_0$는 첫 번째 정의의 base measure $\alpha$와 아래와 같은 관계를 가진다.

$$
\alpha_0 = \alpha(\mathfrak X) , \enspace G_0(\cdot) = \frac{1}{\alpha_0} \alpha(\cdot)
$$

# 3.2 Properties

위에서 정의한 것과 같은 Dirichlet process의 존재성과 construct하는 방법에 대해서는 뒤에서 설명하기로 하고, 먼저 Dirichlet process의 성질에 대해 알아보자. 이하의 설명에서는 Dirichlet process의 두 equivalent한 정의의 notation를 모두 사용하겠다.

## 3.2.1 Expectations, Variances, and Co-variances

정의에 따르면, 임의의 Borel set $A$에 대하여 $(P(A), P(A^c))$는 $\text{Dirichlet}( \alpha(A) , \alpha(A^c))$의 분포를 따른다. 즉 $P(A)$는 $\text{Beta}(\alpha(A) , \alpha(A^c))$ 분포를 따른다. 따라서 임의의 Borel set $A$, $B$에 대하여 다음이 만족한다.

$$
\text{E}(P(A)) = \frac{\alpha(A)}{\alpha(\mathfrak X)} =  \frac{1}{\alpha_0} \alpha(A) = G_0(A)
$$

$$
\text{Var}(P(A)) = \frac{\alpha(A)\alpha(A^c)}{\alpha(\mathfrak X)^2 (\alpha(\mathfrak X)+1)} = \frac{G_0(A)G_0(A^c)}{\alpha_0+1}
$$

$$
\text{Cov}(P(A),P(B))=\frac{G_0(A \cap B)-G_0(A)G_0(B)}{\alpha_0+1}
$$

첫 두 결과는 $P(A) \sim \text{Beta}(\alpha(A) , \alpha(A^c))$의 성질을 이용하면 쉽게 얻을 수 있다. 세 번째 결과를 얻기 위해서는 아래와 같은 증명과정이 필요하다.  
  
먼저 $A \cap B = \emptyset$이라고 가정하자. 이 때 $\{ A, B, A^c \cap B^c \}$는 $\mathfrak X$의 partition이 되고, $(P(A), P(B), P(A^c \cap B^c))$은 $\text{Dirichlet}( \alpha(A) , \alpha(B), \alpha(A^c \cap B^c))$를 따른다. 따라서 다음이 만족한다.

$$
\text{Cov}(P(A),P(B))=-\frac{\alpha(A)\alpha(B)}{\alpha_0^2(\alpha_0+1)}=-\frac{G_0(A)G_0(B)}{\alpha_0+1}
$$

따라서 $A \cap B = \emptyset$일 때, 위 결과의 식이 만족하는 것을 확인할 수 있다. 일반적인 Borel set $A$, $B$에 대해서 증명을 하기 위해, 먼저 $A$, $B$를 $A=(A \cap B) \cup (A \cap B^c)$, $B=(A \cap B) \cup (A^c \cap B)$로 분해한다.

$$
\text{Cov}(P(A),P(B))=\text{Cov}(P(A \cap B) + P(A \cap B^c),P(A \cap B) + P(A^c \cap B)) \\
=\text{Var}(P(A \cap B),P(A \cap B) ) + \text{Cov}(P(A \cap B),P(A^c \cap B)) +\text{Cov}( P(A \cap B^c),P(A \cap B)) +\text{Cov}(P(A \cap B^c),P(A^c \cap B)) 
$$

네 개의 Covariance 항을 모두 계산하면 위의 세 번째 결과 식을 얻을 수 있다. $\square$  
  
첫 번째 결과의 의미를 좀더 살펴보자. Parametric 가정이 없는 상황에서, prior가 $\text{DP}(\alpha)$를 따르는 random measure $P$라고 하자. 만약 observation X 역시 random distribution(measure) $P$로부터 나왔다고 가정하자. 즉, $P \sim \text{DP}(\alpha)$, $X \mid P \sim P $의 상황을 가정한 것이다. 위의 Expectation 식을 적분 꼴로 나타내면 다음과 같다.

$$
\int P(A) d \text{DP}_\alpha(P) =  G_0(A)
$$

이는 observation $X$의 marginal 분포가 $G_0$이라는 것을 의미한다. 

$$
\text{if }P \sim \text{DP}(\alpha) \text{ and } X \mid P \sim P , \text{ then }X \sim G_0
$$

따라서 이 상황에서 우리는 $G_0$을 다음과 같이 두 가지로 이해할 수 있다.

 * Prior mean
 * "Dirichlet process(에서 생성된 분포)로부터 생성된 observation"의 marginal 분포

Probability measure $G_0$을 Dirichlet process의 *center measure*라고도 부른다. 또한, 분산과 공분산의 식을 보면 $\alpha_0$가 $P(A)$의 variability를 조절하는 것을 볼 수 있다. 따라서 $\alpha_0$를 *precision parameter*라고 부른다. 이에 대한 간단한 예시를 들자면, 만약 우리의 모형이 $N(0,1)$를 따를 것으로 기대하지만 확실하지는 않다면, center measure가 $G_0 \sim N(0,1)$분포인 Dirichlet process prior를 부여할 수 있다. 그 때, $\alpha_0$은 우리의 prior guess의 확실한 정도를 나타낸다. 다만 $\alpha_0$는 prior guess의 confidence말고도 다른 의미를 가지고 있기 때문에 해석에 유의하여야 하며, 다른 의미는 후에 다룰 posterior distribution of Dirichlet distribution에서 알 수 있다.  
  
또한 일반적인 경우에, base measure $\alpha$가 다르면 다른 Dirichlet process를 얻을 것이라는 것을 알 수 있다. 즉,

$$
\alpha \neq \alpha ' \enspace \Longrightarrow \enspace \text{DP}(\alpha) \neq \text{DP}(\alpha ') \text{ , unless }G_0 = G_0 ' = \delta_x \text{ for some } x
$$

그 이유는 다음과 같다.

 * $\alpha \neq \alpha^\prime \text{ & } G_0 \neq G_0^\prime$인 경우, $\text{DP}(\alpha)$와 $\text{DP}(\alpha^\prime)$는 서로 다른 expectation을 갖는다.
 * $\alpha \neq \alpha^\prime \text{ & } G_0 = G_0^\prime$인 경우, $\alpha_0 \neq \alpha_0^\prime$이다. 그러면 임의의 $A$에 대하여 $ 0 < G_0(A) = G_0^\prime(A) < 1$를 가정하면 $\text{DP}(\alpha)$와 $\text{DP}(\alpha^\prime)$는 서로 다른 variance를 갖는다.
 * 즉, base measure가 nondegenerate하다면, 서로 다른 base measure에 대해 다른 Dirichlet process를 얻게 된다.

## 3.2.2 Tail-freeness

***tail-free*** random measure에 대해 소개하고자 한다. 그를 위해서 random measure $P$를 construct하는 방법 중 하나인 tree-based method를 소개한다. Tree를 이용해 random measure $P$를 construct한다는 것은, 쉽게 말해서 total probability mass $1$를 반복적인 partitioning을 통해 $\mathfrak X$의 subset들로 분배하는 것으로 이해할 수 있다. 다만 그 mass의 분배를 조절하는 proportion들이 random variable인 것이다. Tail-free 성질의 의미는, 만약 $\mathfrak X$의 한 부분, $A \subset \mathfrak X$로 conditioning이 된다면 그 $A$를 다시 partition하는 조건부 확률의 random variable은 $A$의 확률의 random variable과 상호 독립이라는 의미이다. 간단히 적자면 다음과 같다.

$$
P(A) \perp\!\!\!\perp \frac{P(A \cap B)}{P(A)} \text{ , for } B \subset A \subset \mathfrak X
$$
  
다음과 같이, 직전 partition의 모든 set을 각각 새로운 두 개의 set으로 나눔으로써 얻어지는, $\mathfrak X$의 partition들의 sequence $\{ \mathcal T_m \}$을 생각해보자.

$$
\begin{align*}
\mathcal T_0 &= \{ \mathfrak X \} \\
\mathcal T_1 &= \{ A_0, A_1 \} \\
\mathcal T_2 &= \{ A_{00}, A_{01}, A_{10}, A_{11} \} \\
\mathcal T_3 &= \{ A_{000}, A_{001}, A_{010}, A_{011}, A_{100}, A_{101}, A_{110} , A_{111}\} \\
&\enspace \enspace \vdots
\end{align*}	
$$

$0$과 $1$로 이루어져 있고 길이가 $m$인 모든 string $\varepsilon_1 \cdots \varepsilon_m $의 set, $\mathcal E ^m$을 정의하자.

$$
\mathcal E ^m = \{ 00\cdots 00, 00\cdots01, \cdots , 11\cdots11 \} , \enspace \left\lvert \mathcal E ^m \right\rvert = 2^m
$$

$\mathcal E ^m$을 이용하여, $m$번째 partition $\mathcal T_m$의 $2^m$개의 set들에 다음과 같이 index를 부여할 수 있다.

$$
\mathcal E ^3 = \{ 000, 001, 010, 011, 100, 101, 110, 111 \} \\
\mathcal T_3 = \{ A_{000}, A_{001}, A_{010}, A_{011}, A_{100}, A_{101}, A_{110} , A_{111}\} 
$$

이와 같은 string을 모두 모은 set $\mathcal E ^\ast = \bigcup_{m=0}^{\infty} \mathcal E ^m$을 고려할 수 있다. 임의의 $\varepsilon \in \mathcal E ^\ast$에 대해, splitting probability $V_\varepsilon$을 정의하자.

$$
V_{\varepsilon0} = \text{Pr}(A_{\varepsilon0} \mid A_{\varepsilon}), \enspace V_{\varepsilon1} = \text{Pr}(A_{\varepsilon1} \mid A_{\varepsilon})
$$

$V_{\varepsilon0}, V_{\varepsilon1}$는 partition들의 sequence, $\mathcal T_0, \mathcal T_1, \mathcal T_2, \mathcal T_3, \cdots$를 모든 node가 둘로 쪼개지는 tree로 생각했을 때, $A_{\varepsilon}$ node를 둘로 쪼개는 conditional (random) probability와 같고, $V_{\varepsilon0} + V_{\varepsilon1}=1$를 만족한다.

![image](https://user-images.githubusercontent.com/45325895/61279858-e1471600-a7f1-11e9-98a0-9d1e8315aa60.png){: .center-image}

만약 refining partition들의 set, $\{ A_\varepsilon : \varepsilon \in \mathcal E^\ast \}$가 Borel $\sigma$-field를 generate한다면, 위의 성질을 만족하는 $[0,1]$-valued random variable의 collection $\{ V_\varepsilon : \varepsilon \in \mathcal E^\ast \}$이 주어졌을 때, 그에 따라 random measure $P$를 정의할 수 있다.

$$
P(A_{\varepsilon_1 \varepsilon_2 \cdots \varepsilon_m})= V_{\varepsilon_1} V_{\varepsilon_1 \varepsilon_2} \cdots V_{\varepsilon_1 \varepsilon_2 \cdots \varepsilon_m} , \enspace \enspace \varepsilon = \varepsilon_1 \varepsilon_2 \cdots \varepsilon_m \in \mathcal E^m
$$

이제 random measure $P$의 *tail-freeness*를 정의할 수 있다. 정의는 다음과 같다.

 > The random measure P is a **tail-free process** with respect to the sequence of partitions $\mathcal T_m$ if the following is true.
 > 
 > $$
 > \{ V_0 \} \perp\!\!\!\perp \{ V_{00} , V_{10} \} \perp\!\!\!\perp \{ V_{000} , V_{010}, V_{100}, V_{110} \} \perp\!\!\!\perp \cdots \perp\!\!\!\perp \{ V_{\varepsilon 0} : \varepsilon \in \mathcal E^m \} \perp\!\!\!\perp \cdots
 > $$

이를 조건부확률의 형태로 다시 쓰면 아래와 같다.

$$
\{ \text{Pr}(A_0) \} \perp\!\!\!\perp \{ \text{Pr}(A_{00} \mid A_0) ,\text{Pr}(A_{10} \mid A_0) \} \perp\!\!\!\perp \{ \text{Pr}(A_{000} \mid A_{00}), \text{Pr}(A_{010} \mid A_{01}),\text{Pr}(A_{100} \mid A_{10}), \text{Pr}(A_{110} \mid A_{11}) \} \perp\!\!\!\perp \cdots 
$$

Dirichlet process는 **tail-free random measure**이다. 즉, 

> Splitting variable $( V_{\varepsilon 0} : \varepsilon \in \mathcal E^m )$은 서로 다른 level $m$ 사이에서 mutually independent하고, $V_{\varepsilon 0} \sim \text{Beta}(\alpha(A_{\varepsilon 0}), \alpha(A_{\varepsilon 1}))$을 따른다. 

그 증명은 아래와 같다. 먼저 증명에 필요한 Dirichlet distribution의 성질 몇 가지를 소개하고 증명을 보이겠다.  

### Gamma Representation of Dirichlet Distribution / Aggregation Property

간단한 variable transformation을 거치면 Dirichlet distribution을 gamma distribution에 대해 나타낼 수 있다. 다음과 같은 확률변수 $Z_1, \cdots ,Z_k$를 생각해보자.

$$
Z_i \stackrel{ind}{\sim} \text{Gamma}(\alpha_i , 1) , \enspace i=1, \cdots, k
$$

증명하고자 하는 성질은 다음과 같다.

> $$
> \begin{align*}
> 1. \enspace &\left( \frac{Z_1}{\sum^{k}_{i=1} Z_i} , \cdots , \frac{Z_k}{\sum^{k}_{i=1} Z_i} \right) \sim \text{Dirichlet}(\alpha_1, \cdots, \alpha_k) \\ 
> 2. \enspace &\left( \frac{Z_1}{\sum^{k}_{i=1} Z_i} , \cdots , \frac{Z_k}{\sum^{k}_{i=1} Z_i} \right) \perp\!\!\!\perp \sum^{k}_{i=1} Z_i \\
> 3. \enspace &\text{ If } P = (P_1, \cdots, P_k)\sim \text{Dirichlet}(\alpha_1, \cdots , \alpha_k),\text{ then for any partition }A_1, \cdots , A_m \text{ of } \{ 1, \cdots, k \}, \\
> &\left( \sum_{i \in A_1} P_i , \cdots , \sum_{i \in A_m} P_i \right) \sim 	\text{Dirichlet} \left( \sum_{i \in A_1} \alpha_i, \cdots , \sum_{i \in A_k} \alpha_i \right) \\
> \end{align*}
> $$

증명은 다음과 같다.

$$
f_{\mathbf Z} (z_1, \cdots, z_k ) = e^{-\sum_{i=1}^{k} z_i} \prod_{i=1}^{k} \frac{z_i^{\alpha_i -1}}{\Gamma(\alpha_i)}
$$

다음과 같이 정의된 $(Z_1, \cdots ,Z_k) \rightarrow (Y_1, \cdots ,Y_k)$의 variable transformation을 고려해보자.

$$
\begin{align*}
Y_i &= \frac{Z_i}{\sum_{j=1}^{k} Z_j} , \enspace i=1, \cdots , k-1 \\
Y_k &= \sum_{i=1}^{k} Z_i
\end{align*}
$$

Variable transformation을 수행하면 다음과 같다.

$$
\begin{align*}
Z_i &= Y_i Y_k  , \enspace i=1, \cdots , k-1 \\
Z_k &= \left( 1- \sum_{i=1}^{k-1} Y_i \right)Y_k
\end{align*}
$$

$$
\lvert J \rvert = 
\left\lvert
\begin{matrix}
 Y_k & 0 & \cdots &0 & Y_1\\
 0 & Y_k & \cdots & 0  & Y_2\\
 \vdots & \vdots & \ddots &   & \vdots \\
 0 & 0 &  & Y_k   & Y_{k-1}\\
 -Y_k & -Y_k & \cdots & -Y_k   & 1-\sum_{i=1}^{k-1} Y_i\\
\end{matrix}
\right\rvert
=
\left\lvert
\begin{matrix}
 Y_k & 0 & \cdots &0 & Y_1\\
 0 & Y_k & \cdots & 0  & Y_2\\
 \vdots & \vdots & \ddots &   & \vdots \\
 0 & 0 &  & Y_k   & Y_{k-1}\\
 0 & 0 & \cdots & 0   & 1 \\
\end{matrix}
\right\rvert
=Y_k^{k-1}
$$

$$
\begin{align*}
\implies \enspace f_\mathbf Y (y_1 , \cdots ,y_k) &= e^{-\sum_{i=1}^{k-1} y_i y_k} e^{(-1+\sum_{i=1}^{k-1} y_i) y_k} \left\{ \prod_{i=1}^{k-1} \frac{(y_iy_k)^{\alpha_i -1}}{\Gamma(\alpha_i)} \right\} \frac{(1-\sum_{i=1}^{k-1} y_i)^{\alpha_k -1} y_k^{\alpha_k -1}}{\Gamma(\alpha_k)} y_k^{k-1} \\
&= \frac{ \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1}  }{\prod_{i=1}^{k} \Gamma(\alpha_i)} y_k^{\sum_{i=1}^{k} \alpha_i -1} e^{-y_k} ,\\
&(\text{where } 0 \leq y_1, \cdots, y_{k-1} \leq 1, \text{ and } y_k >0 )
\end{align*}
$$

이를 $y_k$에 대해 marginalize하면 다음과 같다.

$$
\begin{align*}
f(y_1, \cdots , y_{k-1}) &=\frac{ \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1}  }{\prod_{i=1}^{k} \Gamma(\alpha_i)}
\int_0^\infty y_k^{\sum_{i=1}^{k} \alpha_i -1} e^{-y_k} dy_k \\
&= \frac{ \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1}  }{\prod_{i=1}^{k} \Gamma(\alpha_i)} {\Gamma(\sum_{i=1}^{k} \alpha_i)} \\
&= \frac{\Gamma(\sum_{i=1}^{k} \alpha_i)}{\prod_{i=1}^{k} \Gamma(\alpha_i)} \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1} 
\end{align*}
$$

$Y_i ' = Y_i  \text{ for }i =1,\cdots, k-1, \text{ }Y_k '  = 1-\sum_{i=1}^{k-1} Y_i$로 한 번 더 transformation을 수행하면 1번 성질을 얻을 수 있다.

$$
(Y_1 ' , \cdots, Y_k ') = \left( \frac{Z_1}{\sum^{k}_{i=1} Z_i} , \cdots , \frac{Z_k}{\sum^{k}_{i=1} Z_i} \right) \sim \text{Dirichlet}(\alpha_1, \cdots, \alpha_k)
$$

또한, 2번 성질에서처럼 $i=1,\cdots,k$에 대해 $\frac{Z_i}{\sum^k_{j=1} Z_j}$과 $\sum^k_{j=1} Z_j$이 서로 독립인 것은 도출 과정 중간에 얻은 density 식이 factorize되는 것에서 알 수 있다.

$$
f_\mathbf Y (y_1 , \cdots ,y_k)  = \frac{ \left( \prod_{i=1}^{k-1} y_i^{\alpha_i-1} \right) \left( { 1-\sum_{i=1}^{k-1} y_i }\right)^{\alpha_k -1}  }{\prod_{i=1}^{k} \Gamma(\alpha_i)} y_k^{\sum_{i=1}^{k} \alpha_i -1} e^{-y_k} = \left( \prod_{i=1}^{k-1} g_i(y_i) \right) g_k(y_k).
$$
  
3번 성질은 다음과 같은 과정으로 증명할 수 있다. 위에서와 같은 gamma representation을 사용하면 아래와 같다.

$$
\begin{align*}
\left( \sum_{i \in A_1} P_i , \cdots , \sum_{i \in A_m} P_i \right) &= \left( \sum_{i \in A_1} \frac{Z_i}{\sum^{k}_{j=1} Z_j} , \cdots , \sum_{i \in A_m} \frac{Z_i}{\sum^{k}_{j=1} Z_j}  \right) \\
&= \frac{1}{\sum^{k}_{j=1} Z_j} \left( \sum_{i \in A_1} Z_i , \cdots , \sum_{i \in A_m} Z_i \right) \\
&\stackrel{d}{=} \frac{1}{\sum^{m}_{\ell=1} \sum_{i \in A_\ell} Z_i} \left( \sum_{i \in A_1} Z_i , \cdots , \sum_{i \in A_m} Z_i \right) \\
&\stackrel{d}{=} \text{Dirichlet}\left( \sum_{i \in A_1} \alpha_i, \cdots , \sum_{i \in A_k} \alpha_i \right) \\
\end{align*}
$$

마지막 등호는 다음과 같은 gamma distribution의 성질에서 온 것이다.   $\square $

$$
\sum_{i \in A_j} Z_i \sim \Gamma \left( \sum_{i \in A_j} \alpha_i \right) \enspace , \enspace \enspace j=1, \cdots , m 
$$

### Proof of Tailfreeness of Dirichlet Process

먼저 우리는 random vector $( V_{\varepsilon 0} : \varepsilon \in \mathcal E^m )$이 모든 level $m$에 대해 서로 독립임을 보여야 한다. 그를 위해서 임의의 $m$에 대해, $( V_{\varepsilon 0} : \varepsilon \in \mathcal E^m )$이 자기자신보다 lower level에 해당하는 random vector들과 독립임을 보이면 된다. 더 간단히 이야기하면 임의의 $m \in \mathbb N$, 임의의 given $\varepsilon = \varepsilon_1 \cdots \varepsilon_m \in \mathcal E^m$에 대해, 다음을 보이면 된다.

$$
V_{\varepsilon_1 \cdots \varepsilon_m} \perp\!\!\!\perp \{ V_{\varepsilon_1 \cdots \varepsilon_m 0} , V_{\varepsilon_1 \cdots \varepsilon_m 1} \}
$$

귀납법을 이용하여 이를 증명할 것이다. 먼저 $m=1$인 경우 위 induction statement를 증명하자.

$$
V_{\varepsilon_1} = P(A_{\varepsilon_1})
$$

$$
V_{\varepsilon_1 0} = P(A_{\varepsilon_1 0} \mid A_{\varepsilon_1} )= \frac{P(A_{\varepsilon_1 0} )}{P(A_{\varepsilon_1})} \enspace , \enspace V_{\varepsilon_1 1} = P(A_{\varepsilon_1 1} \mid A_{\varepsilon_1} ) = \frac{P(A_{\varepsilon_1 1} )}{P(A_{\varepsilon_1})} 
$$

Dirichlet process의 정의에 따라 다음이 성립한다.

$$
\Big(P(A_{00}),P(A_{01}),P(A_{10}),P(A_{11})\Big) \sim \text{Dirichlet}\Big( \alpha(A_{00}),\alpha(A_{01}),\alpha(A_{10}),\alpha(A_{11}) \Big)
$$

Gamma representation을 이용하면 다음과 같은 결과를 얻을 수 있다.

$$
\begin{align*}
\Big( \frac{P(A_{00})}{P(A_0)} ,\frac{P(A_{01})}{P(A_0)}\Big) &\stackrel{d}{=} \Big( \frac{Z_1}{Z_1+Z_2} , \frac{Z_2}{Z_1+Z_2} \Big)\\
&\stackrel{d}{=} \text{Dirichlet}\Big( \alpha(A_{00}),\alpha(A_{01}) \Big) \\
\Big( \frac{P(A_{10})}{P(A_1)}, \frac{P(A_{11})}{P(A_1)}\Big) &\stackrel{d}{=} \Big( \frac{Z_3}{Z_3+Z_4} , \frac{Z_4}{Z_3+Z_4} \Big)\\
&\stackrel{d}{=} \text{Dirichlet}\Big( \alpha(A_{10}),\alpha(A_{11}) \Big)
\end{align*}
$$

따라서, 위에서 보인 Dirichlet distribution의 성질에 의해, $m=1$인 경우에 induction statement가 성립하는 것을 보였다.

$$
P(A_0)  \perp\!\!\!\perp   \frac{P(A_{00})}{P(A_0)} = P(A_{0 0} \mid A_{0} ) , \enspace P(A_1)  \perp\!\!\!\perp \frac{P(A_{10})}{P(A_1)} =  P(A_{10} \mid A_{1} )  
$$

$$
P(A_{0 0} \mid A_{0} ) \sim \text{Beta}(\alpha(A_{00}),\alpha(A_{01})) , \enspace P(A_{10} \mid A_{1} )   \sim \text{Beta}(\alpha(A_{10}),\alpha(A_{11}))
$$

$m=1, \cdots , k-1$인 경우 위 induction statement가 성립한다고 가정했을 때, $m=k$인 경우도 성립함을 보이자. Dirichlet process의 정의에 의해 다음이 성립한다.

$$
\Big(\cdots , P(A_{\varepsilon_1 \cdots \varepsilon_k 0}),P(A_{\varepsilon_1 \cdots \varepsilon_k 1}), \cdots \Big) \sim \text{Dirichlet}\Big( 2^{k+1} ;\text{ } \cdots ,\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 0}),\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 1}), \cdots \Big)
$$

위에서와 같이 gamma representation을 이용하면 다음과 같은 결과를 얻는다.

$$
\begin{align*}
\Big( \frac{P(A_{\varepsilon_1 \cdots \varepsilon_k 0})}{P(A_{\varepsilon_1 \cdots \varepsilon_k})}, \frac{P(A_{\varepsilon_1 \cdots \varepsilon_k 1})}{P(A_{\varepsilon_1 \cdots \varepsilon_k})} \Big) 
&\stackrel{d}{=} \Big( \frac{Z_1}{Z_1+Z_2} , \frac{Z_2}{Z_1+Z_2} \Big)\\
&\stackrel{d}{=} \text{Dirichlet}\Big( \alpha(A_{\varepsilon_1 \cdots \varepsilon_k 0}),\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 1}) \Big)
\end{align*}
$$

위에서 보인 Dirichlet distribution의 성질에 의해 다음을 얻을 수 있다.

$$
P(A_{\varepsilon_1 \cdots \varepsilon_k})  \perp\!\!\!\perp   \frac{P(A_{\varepsilon_1 \cdots \varepsilon_k 0})}{P(A_{\varepsilon_1 \cdots \varepsilon_k })} = P(A_{\varepsilon_1 \cdots \varepsilon_k 0} \mid A_{\varepsilon_1 \cdots \varepsilon_k} )= V_{\varepsilon_1 \cdots \varepsilon_k 0 }
$$

$$
V_{\varepsilon_1 \cdots \varepsilon_k 0 } = P(A_{\varepsilon_1 \cdots \varepsilon_k 0} \mid A_{\varepsilon_1 \cdots \varepsilon_k}) \sim \text{Beta}(\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 0}),\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 1})) 
$$

또한, $m=1, \cdots , k-1$인 경우 induction statement가 성립하므로 

$$
P(A_{\varepsilon_1 \cdots \varepsilon_k} \mid A_{\varepsilon_1 \cdots \varepsilon_{k-1}}) \perp\!\!\!\perp  P(A_{\varepsilon_1 \cdots \varepsilon_{k-1}} \mid A_{\varepsilon_1 \cdots \varepsilon_{k-2}}) \perp\!\!\!\perp  \cdots \perp\!\!\!\perp  P(A_{\varepsilon_1 \varepsilon_2} \mid A_{\varepsilon_1 }) \perp\!\!\!\perp  P(A_{\varepsilon_1 }) 
$$

$$
\begin{align*}
P(A_{\varepsilon_1 \cdots \varepsilon_k}) &= P(A_{\varepsilon_1 \cdots \varepsilon_k} \mid A_{\varepsilon_1 \cdots \varepsilon_{k-1}})   P(A_{\varepsilon_1 \cdots \varepsilon_{k-1}} \mid A_{\varepsilon_1 \cdots \varepsilon_{k-2}}) \cdots P(A_{\varepsilon_1 \varepsilon_2} \mid A_{\varepsilon_1 }) P(A_{\varepsilon_1 }) \\
&=V_{\varepsilon_1 \cdots \varepsilon_k} V_{\varepsilon_1 \cdots \varepsilon_{k-1}} \cdots V_{\varepsilon_1}
\end{align*}
$$

즉, 

$$
V_{\varepsilon_1 \cdots \varepsilon_k 0 }  \perp\!\!\!\perp V_{\varepsilon_1 \cdots \varepsilon_k} V_{\varepsilon_1 \cdots \varepsilon_{k-1}} \cdots V_{\varepsilon_1}
$$

따라서, 

$$
\begin{align*}
&\therefore \enspace \enspace V_{\varepsilon_1 \cdots \varepsilon_k 0 }  \perp\!\!\!\perp V_{\varepsilon_1 \cdots \varepsilon_k} \enspace \enspace \\
&\therefore \enspace \enspace  V_{\varepsilon_1 \cdots \varepsilon_k 0 } \sim \text{Beta}(\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 0}),\alpha(A_{\varepsilon_1 \cdots \varepsilon_k 1})) \enspace \enspace \square.
\end{align*}
$$



## 3.2.3 Self-similarity & Neutral-to-the-right Property
### Self-similarity

Dirichlet process는 sample space의 successive partition으로 이루어진 임의의 sequence에 대해 tail-free하다는 것을 확인하였다. 이에 더해, Dirichlet process는 더 강한 conditional independence 성질을 갖는다. Notation을 정리하자면, measure $P$와 measurable set $B$에 대해,

$$
\text{restriction measure: }\enspace   P\mid _{B}(A)=P(A \cap B) \\
\text{conditional measure: }\enspace P_B (A) = P(A \mid B),\text{ for } B \text{ with } P(B)>0.
$$

> If $P \sim \text{DP}(\alpha)$, then $P_B \sim \text{DP}(\alpha \mid_{ B})$, and the variable and processes $P(B)$, $(P_B(A):A \in \mathcal X)$, $(P_{B^c}(A):A \in \mathcal X)$ are mutually independent, for any $B \in \mathcal X$ such that $\alpha(B)>0$

증명에 앞서 이 정리의 의미를 생각해보자. 

 * Dirichlet process는 먼저 어떤 set $B$로 conditioning을 함으로써 "localize"해도 여전히 Dirichlet process이다. 그리고 그 때의 base measure는 set $B$로 restrict된 measure이다.
 * 서로 disjoint한 set에 각각 localize된 stochastic process들은 서로 독립이다. 또한 확률변수 $P(B)$와도 독립이다.

어떤 locality($B$)가 주어지면, probability mass는 그 $B$ 밖의 사건들과는 상관없이 Dirichlet process에 의해 분배될 것이다. 즉 Dirichlet process는 local하게 자기자신과 유사하다는 점에서 이 성질을 self-similarity라고 부른다. $P(B)$, $(P_B(A):A \in \mathcal X)$, $(P_{B^c}(A):A \in \mathcal X)$가 서로 independent라는 것은 Dirichlet process의 tail-freeness에 의해 만족한다. Conditional measure가 base measure를 같은 set에 restrict시킨 것과 같은 Dirichlet process를 갖는 사실은 tail-freeness를 보일 때 사용했던 것과 같은 gamma representation을 이용하면 쉽게 보일 수 있다.  
  
또한 같은 맥락으로, Dirichlet process는 Neutral-to-the-right 성질을 만족한다. 그 정의는 아래와 같다.

### Neutral-to-the-right Property

Dirichlet process는 ***neutral-to-the-right***이라는 성질을 만족한다. 그 정의는 다음과 같다.

> Let $F \sim \text{DP}(\alpha)$ be a distribution function of a random probability measure. $F$ is ***neutral-to-the-right process*** if, for every finite partition $0=t_0 \leq t_1 < \cdots < t_k < \infty $, and $k \in \mathbb N$, the random variables
> 
> $$
> 1-F(t_1), \frac{1-F(t_2)}{1-F(t_1)}, \cdots , \frac{1-F(t_k)}{1-F(t_{k-1})}
> $$
> 
> are mutually independent.

Dirichlet process에 대해 위에서 정의된 random variable들을 적으면 다음과 같다.

$$
\begin{align*}
1-F(t_1) &= P(t_1, \infty) \\ 
\frac{1-F(t_2)}{1-F(t_1)} &= \frac{P(t_2, \infty)}{P(t_1, \infty)}= \frac{P((t_2, \infty) \cap (t_1, \infty))}{P(t_1, \infty)} = P_{(t_1,\infty)}\Big((t_2,\infty) \Big)\\
\end{align*}
$$

Dirichlet process의 self-similarity 성질에 의해 $ P(t_1, \infty)$와 $P_{(t_1,\infty)}((t_2,\infty) )$는 서로 독립이다. 또한,

$$
\begin{align*}
\frac{1-F(t_3)}{1-F(t_2)} &= \frac{P(t_3, \infty)}{P(t_2, \infty)} \\
&= \frac{P(t_3, \infty)/P(t_1,\infty)}{P(t_2, \infty)/P(t_1,\infty)} \\
&= \frac{P_{(t_1,\infty)}\Big( (t_3, \infty) \cap (t_2,\infty) \Big)}{P_{(t_1,\infty)}\Big( (t_2, \infty) \Big)} \\
&= \Big( P_{(t_1,\infty)} \Big)_{(t_2, \infty)} \Big( (t_3, \infty)\Big)
\end{align*}
$$

$\tilde P = P_{(t_1,\infty)}$이라고 하면, 다시 한 번 Dirichlet process의 self-similarity 성질에 의해 $\tilde P((t_2,\infty) )$와 $\tilde P_{(t_2, \infty)} ( (t_3, \infty) )$는 서로 독립이다. 따라서 같은 방법으로 다음 random variable이 모두 독립임을 보일 수 있다.

$$
1-F(t_1), \frac{1-F(t_2)}{1-F(t_1)}, \cdots , \frac{1-F(t_k)}{1-F(t_{k-1})}
$$

 
