---
layout: post
title: "ESL: Ch 5. Basis Expansions and Regularization"
tags: [Elements of Statistical Learning]
comments: true
---
> Contents  
> [5.1 Introduction](#51-introduction)  
> [5.2 Piecewise Polynomials and Splines](#52-piecewise-polynomials-and-splines)  
> [5.3 Filtering and Feature Extraction](#53-filtering-and-feature-extraction)  
> [5.4 Smoothing Splines](#54-smoothing-splines)  
> [5.5 Automatic Selection of the Smoothing Parameters](#55-automatic-selection-of-the-smoothing-parameters)  
> [5.6 Nonparametric Logistic Regression](#56-nonparametric-logistic-regression)  
> [5.7 Multidimensional Splines](#57-multidimensional-splines)  
> [5.8 Regularization and Reproducing Kernel Hilbert Spaces](#58-regularization-and-reproducing-kernel-hilbert-spaces)  
> [5.9 Wavelet Smoothing](#59-wavelet-smoothing)  


## 5.1 Introduction

기본적인 통계 모형에서는 input feature들 사이의 선형성(linearity)을 가정하는 경우가 많다. 이는 예측변수와 목적변수 사이의 true relationship에 대한 근사적인 모형이다.  
  
 * Regression problem의 예를 들면, 선형회귀분석은 실제 true function $f(X)$를 "예측변수 $X$에 대한 목적변수 $Y$의 조건부 평균"으로 다음과 같이 설정한다. 
 
 $$
 f(X)=E[Y|X]=\beta_0+\beta_1 X_1+...+\beta_p X_p
 $$
 
 * true function $f(X)$는 $X$에 대해 nonlinear, nonadditive일 것이지만, 선형성을 가정했을 때 분석 결과 해석이 더 용이하다는 점, 그리고 $\beta_0+\beta_1 X_1+...+\beta_p X_p$는 $f(X)$의 1차 Taylor approximation이라는 점 등의 이유 때문에, $X$의 선형성을 가정하는 경우가 많다.  
  
 * 또한, Classification problem을 예로 들면, posterior 확률 $P(Y=1 \mid X)$의 logit transformation은 $X$에 대해 선형이다.

$$
log(\frac{P(Y=1|X)}{1-P(Y=1|X)})=\beta_0+\beta_1 X_1+...+\beta_p X_p
$$

 * 이 때, 그 decision boundary 역시 선형성을 갖는데, 실제 true boundary는 nonlinear일 것.

### What is "basis" and "basis expansion"?
어떤 부분집합 $B= \\{ b_1, b_2, ...,b_n \\} \subset V$의 원소들의 선형결합으로 $B$가 속한 벡터공간 $V$의 모든 원소들을 나타낼 수 있다면, 우리는 $B$가 $V$를 span한다고 나타내거나, 혹은 $B$를 $V$의 spanning set이라고 부른다. 한 벡터공간을 span하는 집합은 무수히 많을 수 있다. 기저(basis)는 어떤 벡터 공간의 minimal spanning set, 즉 선형독립이면서 spanning set을 말한다.

예를 들면, $a\cdot1+b\cdot x+c\cdot x^2+d\cdot x^3$의 꼴로 3차 이하의 다항식을 모두 나타낼 수 있고,  $\\{ 1,x,x^2,x^3 \\}$는 서로 선형독립이기 때문에, 3차 이하의 다항식의 집합 $P_3$는 $ \\{ 1,x,x^2,x^3 \\} $을 basis로 갖는다. (물론 이것이 유일한 basis는 아니다.)

basis에 대한 위의 설명이 다소 흐름에 맞지 않다고 느껴질 수도 있겠지만, 위의 linear model의 예의 경우 true function $f(X)$를 나타내는 basis는 각 input feature $ \\{ 1,X_1,X_2,...,X_p \\} $가 된다.  

따라서 이 챕터에서 다루는 basis expansion의 의미는 더이상 input feature $X_1,X_2,...,X_p$를 그대로 basis로 쓰지 않고, $X$의 transformation인 새로운 변수들을 기존 linear model의 basis에 추가하거나, 새로운 변수들로 이를 대체하는 것을 말한다. 이를 linear basis expansion이라고 부르고, 다음과 같이 나타낸다.

$$
h_m:\mathbb{R}^p \rightarrow \mathbb{R}\:\:, m=1,2,...,M
$$  

$$
f(X)=\sum_{m=1}^{M} \beta_m h_m(X)=\beta_1 h_1(X)+\beta_2 h_2(X)+...+\beta_M h_M(X)
$$  

이제 true function $f(X)$는 input feature $X$에 대해서는 nonlinear일 수 있지만, 새로운 feature들 $h_1(X),..,h_M(X)$에 대해서는 linear model이다.  
  
회귀분석에서 특정 예측변수를 log transformation하거나, 이차항, interaction 항을 추가하는 작업은 이와 같은 linear basis expansion의 한 예이다. 그 외에도 $h_m(X)$를 어떻게 설정하는지에 따라 다양한 모형이 있는데 이는 아래에서 다루도록 하겠다.  
  
또한, basis function $h_m(X)$를 적절하게 설정하는 것도 중요하지만, 모형의 복잡도를 조절하는 방법 역시 요구된다. 이에는 크게 다음과 같은 세 가지 방법이 있다.
 * Restriction method : basis function $h_m$의 class를 사전에 결정하는 방법.
 * Selection method : 모형의 fit에 유의미한 기여를 하는 basis function만 모형에 포함시키는 방법. ex) CART, MARS
 * Regularization method : 가능한 basis function $h_m(X)$를 모두 모형에 포함시키지만, 그 계수 $\beta_m(X)$에 제약을 거는 방법. ex) ridge regression
 * lasso regression은 selection method와 regularization method를 동시에 사용하는 모형이다.


## 5.2 Piecewise Polynomials and Splines
## 5.3 Filtering and Feature Extraction
## 5.4 Smoothing Splines
## 5.5 Automatic Selection of the Smoothing Parameters
## 5.6 Nonparametric Logistic Regression
## 5.7 Multidimensional Splines
## 5.8 Regularization and Reproducing Kernel Hilbert Spaces
## 5.9 Wavelet Smoothing
