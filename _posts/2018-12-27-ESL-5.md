---
layout: post
title: "ESL: Ch 5. Basis Expansions and Regularization"
tags: [Elements of Statistical Learning]
comments: true
---
> Contents  
> [5.1 Introduction](#51-introduction)  
> [5.2 Piecewise Polynomials and Splines](#52-piecewise-polynomials-and-splines)  
> [5.3 Filtering and Feature Extraction](#53-filtering-and-feature-extraction)  
> [5.4 Smoothing Splines](#54-smoothing-splines)  
> [5.5 Automatic Selection of the Smoothing Parameters](#55-automatic-selection-of-the-smoothing-parameters)  
> [5.6 Nonparametric Logistic Regression](#56-nonparametric-logistic-regression)  
> [5.7 Multidimensional Splines](#57-multidimensional-splines)  
> [5.8 Regularization and Reproducing Kernel Hilbert Spaces](#58-regularization-and-reproducing-kernel-hilbert-spaces)  
> [5.9 Wavelet Smoothing](#59-wavelet-smoothing)  

# 5.1 Introduction

기본적인 통계 모형에서는 input feature들 사이의 선형성(linearity)을 가정하는 경우가 많다. 실제 예측변수와 목적변수 사이의 true relationship은 linear하지 않겠지만, 이를 linear한 모형으로 근사한 것이다.  
  
 * 예를 들면, 선형회귀분석은 true function $f(X)$를 "예측변수 $X$에 대한 목적변수 $Y$의 조건부 평균"으로 다음과 같이 설정한다. 
 
 $$
 f(X)=E[Y|X]=\beta_0+\beta_1 X_1+...+\beta_p X_p
 $$
  
 * 예측변수와 목적변수 사이의 true relationship $f(X)$는 $X$에 대해 linear, additive하다는 보장이 없다.
 * 다만, 분석 결과의 해석이 더 용이하다는 점, 그리고 $\beta_0+\beta_1 X_1+...+\beta_p X_p$는 $f(X)$의 1차 Taylor approximation이라는 점 등의 이유 때문에, $X$의 선형성을 가정하는 경우가 많다.  
<br>

## What is "basis" and "basis expansion"?
어떤 부분집합 $B= \\{ b_1, b_2, ...,b_n \\} \subset V$의 원소들의 선형결합으로 $B$가 속한 벡터공간 $V$의 모든 원소들을 나타낼 수 있다면, 우리는 $B$가 $V$를 span한다고 나타내거나, 혹은 $B$를 $V$의 spanning set이라고 부른다. 한 벡터공간을 span하는 집합은 무수히 많을 수 있다. 그 중에서도 **Basis**는 선형독립이면서 어떤 벡터공간을 span하는 set, 즉 어떤 벡터 공간의 **minimal spanning set**을 말한다.

예를 들면, $a\cdot1+b\cdot x+c\cdot x^2+d\cdot x^3$의 꼴로 3차 이하의 다항식을 모두 나타낼 수 있고,  $\\{ 1,x,x^2,x^3 \\}$는 서로 선형독립이기 때문에, $ \\{ 1,x,x^2,x^3 \\} $는 3차 이하의 모든 다항식들의 집합 $P_3$의 **basis**가 될 수 있다. 위의 linear model의 예의 경우 true function $f(X)$를 나타내는 **basis**는 각 input feature $ \\{ 1,X_1,X_2,...,X_p \\} $가 된다.  

따라서 이 챕터에서 다루는 **basis expansion**의 의미는 더이상 input feature $X_1,X_2,...,X_p$를 그대로 basis로 쓰지 않고, $X$의 transformation인 새로운 변수들을 모형의 basis로 사용하는 것을 말한다. 이를 **linear basis expansion**이라고 부르고, 다음과 같이 나타낸다.

$$
h_m:\mathbb{R}^p \rightarrow \mathbb{R}\:\:, m=1,2,...,M
$$  

$$
f(X)=\sum_{m=1}^{M} \beta_m h_m(X)=\beta_1 h_1(X)+\beta_2 h_2(X)+...+\beta_M h_M(X)
$$  

이제 true function $f(X)$는 input feature $X$에 대해서는 nonlinear일 수 있지만, 새로운 feature들 $h_1(X),..,h_M(X)$에 대해서는 linear, additive한 model이다.  
  
회귀분석에서 특정 예측변수를 log transformation하거나, 이차항 및 interaction 항을 추가하는 작업은 이와 같은 linear basis expansion의 한 예이다. 그 외에도 $h_m(X)$를 어떻게 설정하는지에 따라 다양한 모형이 있는데 이는 아래에서 다루도록 하겠다.  
  
또한, basis function $h_m(X)$를 적절하게 설정하는 것도 중요하지만, 모형의 복잡도를 조절하는 방법 역시 요구된다. 이에는 크게 다음과 같은 세 가지 방법이 있다.
 * Restriction method : basis function $h_m$의 class를 사전에 결정하는 방법.
 * Selection method : 모형의 fit에 유의미한 기여를 하는 basis function만 모형에 포함시키는 방법. ex) CART, MARS
 * Regularization method : 가능한 basis function $h_m(X)$를 모두 모형에 포함시키지만, 그 계수 $\beta_m(X)$에 제약을 거는 방법. ex) ridge regression
 * lasso regression은 selection method와 regularization method를 동시에 사용하는 모형이다.
<br>  
  
<br>
  

# 5.2 Piecewise Polynomials and Splines
우선 Section 5.7까지는 $X$가 1개의 예측변수를 갖는 것으로 가정한다.<br>

## Piecewise polynomial function  
**Piecewise polynomial** 함수, $f(X)$는 $X$의 정의역을 (겹치지 않고 서로 인접하는) 구간들로 잘라, 각 구간들 내에서 polynomial로 나타낼 수 있는 함수를 말한다. 아래의 그림과 같은 piecewise polynomial은 다음과 같은 basis로 나타낼 수 있다. 이 때, $f(X)$는 6개의 basis function으로 나타내어지므로 자유도(degrees of freedom)는 6이다.  

$$
f(X)=\theta_1 h_1(X)+\theta_2 h_2(X)+...+\theta_6 h_6(X)
$$

<p align="center"> 
<img src="https://user-images.githubusercontent.com/45325895/50602618-11d0fd80-0efb-11e9-82ce-b6d16643ce66.png" >
</p>

$$
h_1(X)=I(X< \xi_1), \enspace h_2(X)=I(X< \xi_1)\cdot X
$$

$$
h_3(X)=I(\xi_1 \leq X< \xi_2), \enspace h_4(X)=I(\xi_1 \leq X< \xi_2)\cdot X
$$

$$
h_5(X)=I(\xi_2 \leq X), \enspace h_6(X)=I(\xi_1 \leq X)\cdot X
$$


위와 같은 piecewise **linear** polynomial $f(X)$에 정의역 전체에서 연속이 될 조건을 추가하면 어떻게 될까? (책에서는 표현 상 이 부분에서 linear라는 표현을 affine과 구분하여 사용하지 않았다.) 추가되는 연속성 조건은 다음과 같다.  

$$
f(\xi^-_1 ) = f(\xi^+_1) \enspace \Rightarrow \enspace \theta_1+\theta_2 \xi_1 = \theta_3+\theta_4 \xi_1
$$

$$
f(\xi^-_2 ) = f(\xi^+_2) \enspace \Rightarrow \enspace \theta_3+\theta_4 \xi_2 = \theta_5+\theta_6 \xi_2
$$

piecewise linear polynomial $f(X)$에 연속의 조건인 두 개의 제약식이 추가되면, $6-2=4$개의 basis function으로 $f(X)$를 나타낼 수 있게 된다. 그 basis는 다음과 같다.

$$
f(X)=\theta_1 h_1(X)+\theta_2 h_2(X)+\theta_3 h_3(X)+\theta_4 h_4(X)
$$

<p align="center"> 
<img src="https://user-images.githubusercontent.com/45325895/50603776-ff58c300-0efe-11e9-8b6e-ee5987b87e13.png" >
</p>


$$
h_1(X)=1, \enspace h_2(X)=X
$$

$$
h_3(X)=(X-\xi_1)_+, \enspace h_4(X)=(X-\xi_2)_+
$$

 * $h_3(X)=(X-\xi_1)_+=max \\{ 0, X-\xi_1 \\}$는 다음 그림과 같은 함수를 의미한다.
<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/50604317-e00f6500-0f01-11e9-8210-e29bae21d3ed.png" >
</p>


이외에도 각 구간마다 일차함수가 아닌 $M$차 이하의 polynomial을, 그리고 연속 뿐만 아니라 $C^k$ class 조건 ($k$계 도함수까지 연속)도 부여할 수 있다. 예를 들면, 세 개의 knot ($\xi_1,\xi_2,\xi_3$)으로 나뉜 각 구간마다, 3차 polynomial을 이용하여, 2계 도함수까지 연속인 $f(X)$의 basis를 생각해보자. 먼저, 세 개의 knot은 정의역을 네 개의 구간으로 나누며 각 구간마다 4개, 16개의 basis function이 필요하다. ($4 \times 4 = 16$) 여기에 각 knot마다 연속, 1계도함수 연속, 2계도함수 연속과 같이 세 개의 제약식이 필요하므로, 9개의 basis function이 사라진다. ($3 \times 3 = 9$) 따라서, $f(X)$는 총 $16-9=7$개의 basis function으로 나타낼 수 있을 것이다.  
<br>
 
## Regression spline
<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/50833879-f439e300-1395-11e9-9852-d17456432c17.png">
</p>

> A “**spline**" is a thin strip of wood that can be easily bent to follow a curved line. Historically, it was used in drafting for drawing smooth curves. Regression splines, a statistical translation of this idea, are a way to represent nonlinear, but unknown, mean functions.

위의 사진과 같이, **spline**은 부드러운 곡선을 그리기 위해 사용된 얇고 긴 나무조각을 가리키는 말이다. 부드러운 곡선을 그리는 spline wood와 같이, 통계학에서 **spline**은 nonlinear한 (unknown) mean function을 그리는 방법을 의미하며, 크게 regression spline과 smoothing spline으로 나눌 수 있다. 이 절에서는 knot의 개수, 위치, 차수를 사전적으로 결정한 후 mean function을 그리는 regression spline을 다룬다.

**order-$M$ spline**은 $M-1$차 polynomial들로 이루어진 $C^{M-2}$ class의 piecewise polynomial function로 정의된다. 즉, $M-2$번 미분한 도함수까지 연속인 $M-1$차 이하의 piecewise polynomial을 (order-$M$) spline이라고 부른다. 만약 $M-1$차 piecewise polynomial이 ($M-2$번이 아닌) $M-1$번 미분한 도함수까지 연속이라면, 이는 더이상 piecewise polynomial이 아니라 global $M-1$차 polynomial이 될 것이다. 가장 많이 쓰이는 Cubic spline은 order-$4$ spline이다. $K$개의 knot를 갖는 **order-$M$ spline**의 basis는 다음과 같이 나타낼 수 있다.

$$
h_j(X)=X^{j-1}, \enspace j=1,...,M\enspace,  \enspace\enspace\enspace h_{M+l}(X)=(X-\xi_l)^{M-1}_+, \enspace l=1,...,K
$$

$$
f(X)=\theta_1 h_1(X)+\theta_2 h_2(X)+ ... +\theta_{M+K} h_{M+K}(X)
$$

다음 그림의 초록색 선은 두 개의 knot를 갖는 Cubic spline, 즉 order-$4$ spline을 나타낸 것이다. 구간과 구간 사이의 경계에서 연속이며 부드럽게 연결되는 것을 확인할 수 있다.

<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/50605745-9aa16680-0f06-11e9-99fa-9ce01f646b8b.png">
</p>

<br>

## Natural cubic spline
데이터를 piecewise polynomial로 fit하는 경우, 양 끝 값에서는 연속성 제약이 없고 그 주변에는 data도 적을 것이기 때문에, 정의역의 양 끝 값 주변에서 fitted value가 **불안정**해질 가능성이 있다. 여기서 **불안정**하다는 것은 데이터가 어떻게 뽑히는가에 따라서 fitted value가 크게 달라지게 되는 것을 의미한다. 우리가 관측한 observation은 데이터의 한 random realization일 것이다. 그런데 이 randomness에 따라 fitted value(여기서는 spline curve)가 그때그때 크게 달라진다면, 이는 큰 문제가 된다. 이를 **모형의 variance가 크다**고 하는데, **Natural cubic spline**은 이를 해결하고자 한 모형이다.

<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/50606266-80688800-0f08-11e9-9c7f-6b4576606169.png" >
</p>

**Natural cubic spline**은 좌우의 양 끝 boundary knot($\xi_1$과 $\xi_K$) 밖에서는 linear(일차함수 꼴)이고 그 이외의 구간에서는 3차 polynomial로 나타내어지는 spline을 말한다. 다시 말해서, **Natural cubic spline**은 "좌우 양 끝 구간에서, 차수가 2차 이상인 항의 계수$=0$"의 제약이 추가된 cubic spline이다. $K$개의 knot을 갖는 cubic spline이 $K+4$의 자유도를 갖는 것에 반해($M=4$), $K$개의 knot을 갖는 **Natural cubic spline**은 $(K+4)-4=K$개의 자유도를 갖는다. 양 끝 구간에서 각각 2 개씩, 총 4 개의 자유도를 아낄 수 있기 때문이다. 같은 차수의 cubic spline과 비교했을 때, **Natural cubic spline**은 다음과 같은 이점이 있다.

 * 같은 자유도를 갖는 cubic spline에 비해, interior region에서 4개의 knot을 더 가져갈 수 있다는 뜻이 된다.
 * Boundary 근처에서 fitting model의 bias가 약간 올라가는 대신, model의 variance를 낮출 수 있다.
	 * 어떤 데이터가 관측되더라도, spline curve가 크게 달라지지 않는다.
	 * Boundary knot 밖 구간에서는 어차피 많은 정보가 없으므로, linear 가정을 하는 것이 큰 문제가 없다.
  
## Example: South African heart disease



<br>


# 5.3 Filtering and Feature Extraction
# 5.4 Smoothing Splines

Regression spline을 그릴 때는 knot의 개수 및 위치를 임의로, 혹은 fit에 대한 어떤 measure를 통해 사전에 결정하여야 했다. 이 절에서는 사전에 knot의 개수 및 위치를 결정할 필요가 없는 **smoothing spline**에 대해 알아보고자 한다. 이 방법의 특징은 penalty항을 통해 각 knot들이 fitted value에 영향을 주는 정도에 제약을 걸어 overfitting을 방지한다는 것이다. 몇몇 knot으로부터의 영향은 penalty항에 의해 완전히 사라질 수도 있다. 이처럼 penalty를 준다는 점에서 ridge regression과 lasso와 닮아있지만, **smoothing spline**은 새로운 종류의 penalty를 적용함으로써 $X$와 $Y$ 사이의 nonlinear 관계를 허용한다.  

$(x_1,y_1),(x_2,y_2),...,(x_N,y_N)$과 같은 $N$개의 observed data가 있을 때, 다음과 같은 식을 최소화하는 함수 $f$를 생각해보자.  

$$
RSS(f,\lambda)=\sum_{i=1}^{N} { \{ y_i-f(x_i) \} }^2 + \lambda \int { \{ f''(t) \} }^2 dt
$$

 * 첫 번째 항은 **fit이 observed data와 얼마나 가까운지**를 측정하는 기준이 된다.
 * 두 번째 항은 함수 $f$의 **curvature**, 즉 $f$의 굴곡진 정도를 제한하고 spline을 smoothing하는 기준이 된다.
	 * $f$의 이계도함수, $f'' $의 $L^2$-norm의 제곱으로 이해할 수도 있다.
	 * $f(x)=a+bx$와 같이 $f$의 curvature가 전혀 없을 때, 즉 이계도함수 $f'' $이 $y=0$일 때, 아래와 같이 두번째 항은 0이 된다.

$$
||f''||_{L^2}^2 = \int { \{ f''(t) \} }^2 dt = 0
$$

 * $\lambda$는 위 식 전체를 최소화함에 있어, 두 기준들 간의 비중을 정해주는 smoothing parameter이다.
	 * $\lambda=0$일 때, $f$는 전혀 smoothing되지 않고 모든 observed points $(x_1,y_1),...,(x_N,y_N)$를 지나가는 곡선이 될 것.
	 * $\lambda=\infty$일 때, $f$는 이계도함수가 $0$이 되어야하므로 least square로 직선(일차함수)을 fit한 결과가 나올 것.

따라서, 주어진 $\lambda$에 대해서, 위 식을 최소화하는 함수 $f$는 observed data와의 fit도 좋으면서, curvature도 크지 않은 smoothing spline이 될 것이다.  

$x$에 대해 선형이라는 가정 없이, $f$는 $\int { \{ f'' (t) \} }^2 dt$ (두번째 항)의 값이 정의되는 이 세상 모든 함수가 위 최소화문제의 해가 될 수 있다. 그렇다면 어떻게 위 최소화문제의 해 $f^*$를 찾을 것인가? 그런데 놀랍게도, 위 최소화문제는 **$x_1,x_2, ...,x_N$에서 knot을 갖는 natural cubic spline**을 유일한 해로 갖는다는 것이 밝혀져 있다 (증명 [Reinsch (1967)](https://link.springer.com/content/pdf/10.1007/BF02162161.pdf)). 따라서, $f$를 **$x_1,x_2, ...,x_N$에서 knot을 갖는 natural cubic spline**으로 둔다면, 함수 $f$는 $N$개의 basis function을 갖는 다음과 같은 natural cubic spline이 된다.

$$
f(x)=\sum_{i=1}^{N} N_j(x)\theta_j
$$

$$
where \enspace \enspace N_1(x),...,N_N(x):basis functions \enspace \enspace / \enspace \enspace \theta_1,...,\theta_N : coefficients
$$

이 때, smoothing spline의 최소화문제는 아래와 같이 나타낼 수 있다.

$$
RSS( \theta,\lambda)=(y-N\theta)^T (y-N\theta) + \lambda \theta^T \Omega_N \theta
$$

$$
where \enspace \enspace y=
  \left[ {\begin{array}{c}
   y_1 \\
   y_2 \\
   \vdots \\
   y_N \\
  \end{array} } \right] \enspace , \enspace  \theta=
  \left[ {\begin{array}{c}
   \theta_1 \\
   \theta_2 \\
   \vdots \\
   \theta_N \\
  \end{array} } \right]
$$

$$
N=\begin{bmatrix}
  N_1(x_1) & N_2(x_1) & \cdots & N_N(x_1) \\
  N_1(x_2) & N_2(x_2) & \cdots & N_N(x_2) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  N_1(x_N) & N_2(x_N) & \cdots & N_N(x_N) 
 \end{bmatrix}  \enspace , \enspace \Omega_N = \begin{bmatrix}
  \ddots & \vdots & \enspace \\
  \cdots & \int N''_j(t)N''_k(t)  dt & \cdots \\
  \enspace & \vdots  & \ddots  \\
 \end{bmatrix}
$$

위 식을 최소화하는 Natural cubic spline을 찾기 위해서는 위 $RSS$식을 $\theta$에 대해 최소화하면 된다. $\theta$의 해를 찾기 위해 일계조건을 풀면 다음과 같다.

$$
\frac{\partial RSS}{\partial \theta}=\frac{\partial}{\partial \theta} \Big(  
y^T y - y^T N \theta - \theta^T N^T y + \theta^T N^T N \theta + \lambda \theta^T \Omega_N \theta
\Big)
$$

$$
=-2 N^T y +2 N^T N \theta +2 \lambda \Omega_N \theta = 0
$$

$$
 \hat{\theta} = \Big(  N^T N+ \lambda \Omega_N \Big) ^{-1}  N^T y
$$

따라서 도출한 smoothing spline은 아래와 같은 형태의 Natural cubic spline이다.

$$
\hat{f}(x)=\sum_{i=1}^{N} N_j(x)\hat{\theta}_j
$$

$$
where \enspace \enspace N_1(x),...,N_N(x):basis functions \enspace \enspace / \enspace \enspace \hat{\theta}_1,...,\hat{\theta}_N : coefficients
$$

$N$개의 observation에 대한 smoothing spline의 fitted value를 벡터를 이용하여 나타내면 다음과 같다. $\hat{ y}$, 는 아래와 같이 $y$에 $S_\lambda=N \Big(  N^T N+ \lambda \Omega_N \Big) ^{-1}  N^T$를 곱한형태인데, 이 때, smoothing spline의 fitted value를 구하기 위해 반응변수 벡터 $y$에 곱해주는 행렬 $S_\lambda$를 **Smoother matrix**라고 부른다.

$$
\hat{ y}=N\hat{\theta} = N \Big(  N^T N+ \lambda \Omega_N \Big) ^{-1}  N^T y = S_\lambda y
$$

그리고 Smoother matrix, $S_\lambda$, 는 N개 observation의 input 변수들 $x_1, x_2, ..., x_N$과 $\lambda$의 값에 의해 결정된다는 것을 알 수 있다.
<br>

## Smoother Matrix & Degrees of Freedom
$\lambda$는 위와 같이 Smoothing spline을 구하는 과정에서, curvature 제약의 강도를 결정하는 최소화문제의 tuning parameter이다. 지금까지는 $\lambda$가 smoothing spline을 구하기 앞서, 사전에 주어진 상수라고 놓고 논의를 전개했다. 먼저 Smoother matrix와 degrees of freedom에 대한 몇 가지 성질을 알아보고, 그에 따라 $\lambda$를 사전에 결정하는 방법에 대해 알아보고자 한다.

$\lambda$가 사전에 결정되어 있다면, $y$에 $S_\lambda$라는 행렬을 곱함으로써 얻은 Smoothing spline, $ \hat{ y}$은 $y$에 대해 **linear**한 smoother이다. (Matrix multiplication은 linear transformation이기 때문.) 이와 같은 linear smoother의 다른 예는 선형회귀분석의 **Hat matrix**, **$H$**가 있다. Hat matrix는 다음과 같이 회귀모형의 fitted value를 구해주는 linear operator이다.

$$
H=X(X^T X)^{-1}X^T \enspace , \enspace \enspace \hat{y}_{reg}=X(X^T X)^{-1}X^Ty=Hy
$$


회귀분석의 Hat matrix와의 비교를 통해, Smoother matrix $S_\lambda$의 성질에 대해 몇 가지 알아보자.

 * $H$와 $S_\lambda$는 대칭행렬, positive semidefinite행렬이라는 특징이 있다.
 * $H$는 $H^2=HH=H$, 즉 idempotent한 특징이 있다. $S_\lambda$는 $S_\lambda^2=S_\lambda S_\lambda \preceq S_\lambda$인 성질이 있다. 이는 $S_\lambda $가 $S_\lambda S_\lambda$보다 어떤 positive semidefinite행렬만큼 더 크다는 것을 의미한다. 이는 smoother matrix가 curvature 제약에 의해 **shrinking(수축하는, 쪼그라드는)** 된 것 때문인데, 이에 대해서는 뒤에 그 의미와 이유를 더 설명하겠다.
 * rank란 어떤 linear operator의 column space의 차원, 다시 말해서 linear operator의 결과로 이루어진 공간(치역:range)의 차원을 의미한다. 회귀모형이 $p$개의 항을 가질 때 $X$는 $p$개의 열로 이루어진 행렬이 되며, $H$는 $p$의 rank를 갖는다. $S_\lambda$는 $N$의 rank를 갖는다.  

여기서 눈여겨볼 부분이 있다. 회귀분석의 경우 $N$개의 자료가 있고 $p$개의 변수로 이를 설명하고자 할 때, fitted value는 $p$개의 설명변수 $X_1, X_2, ..., X_p$ 가 이루는 공간, 즉 $p$차원 공간 위에 놓이게 된다. 그렇기 때문에 반응변수 벡터 $y$를 fitted value인 $\hat{y}$로 변환하는 linear operator, $H$의 rank가 $p$인 것이다.  



Smoothing spline의 최소화문제는 **$x_1,x_2, ...,x_N$에서 knot을 갖는 natural cubic spline**을 그 해로 갖는다. 5.3에 서술된 대로 $N$개의 knot를 가지는 natural cubic spline은 $N$개의 basis로 표현할 수 있으며, 다시 말해서 $N$을 자유도를 갖는다. 자유도가 $N$이라는 것은 모형 내 추정해야 할 parameter($\hat{\theta}_1,...,\hat{\theta}_N$) 가 $N$개임을 의미한다.

$$
\hat{f}(x)=\sum_{i=1}^{N} N_j(x)\hat{\theta}_j
$$

그런데 smoothing spline이 $N$의 rank를 갖는다면, ($N$개의 자료를 통해 얻은 fitted curve가 $N$개의 basis로 나타내어진다면,) **smoothing spline은 아래와 같이 그저 $N$개의 점을 잇는 의미없는 선일 뿐일까?**  

<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/51170409-bc7cef00-18f1-11e9-8974-08a8a588fe63.png">
<p/>

렇지 않다. Smoothing spline을 처음 소개할 때, 아래와 같이 서술하였다.  

 > 이 절에서는 사전에 knot의 개수 및 위치를 결정할 필요가 없는 smoothing spline에 대해 알아보고자 한다. 이 방법의 특징은 **penalty항을 통해 각 knot들이 fitted value에 영향을 주는 정도에 제약을 걸어 overfitting을 방지한다**는 것이다. 몇몇 knot으로부터의 영향은 penalty항에 의해 완전히 사라질 수도 있다.  
 > (중략)  
 > 그런데 놀랍게도, 위 최소화문제는 $x_1,x_2, ...,x_N$에서 knot을 갖는 natural cubic spline을 유일한 해로 갖는다는 것이 밝혀져 있다 (증명 Reinsch (1967)). 따라서, $f$를$x_1,x_2, ...,x_N$에서 knot을 갖는 natural cubic spline으로 둔다면, 함수 $f$는 $N$개의 basis function을 갖는 다음과 같은 natural cubic spline이 된다.  

$N$개의 basis의 합으로 나타내어지지만, curvature 제약 하의 최소화의 해로 도출하였기 때문에, smoothing spline은 더 적은 수의 자유도로 나타내어질 수 있다. 그럼 smoothing spline의 "실제" 자유도(Effective degrees of freedom)는 어떻게 구할까? 우리는 회귀분석의 Hat matrix와의 비교를 통해 그 아이디어를 얻을 수 있다. 회귀분석의 smoother matrix, $H$의 자유도는 다음과 같이 나타낼 수 있다.

$$
trace(H)=trace(X(X^TX)^{-1}X^T)=trace(X^TX(X^TX)^{-1})=trace(I_p)=p
$$

$$
rank(H)=p=trace(H)
$$

따라서 우리는 $S_\lambda$의 "실제" 자유도(Effective degrees of freedom)를 다음과 같이 정의한다.

$$
df_\lambda=trace(S_\lambda)= N \Big(  N^T N+ \lambda \Omega_N \Big) ^{-1}  N^T
$$

이와 같이 $S_\lambda$의 자유도를 정의하는 것은 특정 자유도 값의 smoothing spline을 도출하고자 할 때 유용하다. 예를 들어, 자유도가 $12$인 smoothing spline을 구하고 싶다면, 간단하게 $trace(S_\lambda)=12$를 만족하는 $\lambda$의 값을 numerical한 방법으로 구하면 된다. 자유도의 개념은 회귀분석 등 다른 smoothing 모형들에도 적용되는 개념이기 때문에, 그와 일관적인 방법으로 smoothing spline을 parameterize하는 데 유용하다.
<br>
#### Shrinking Nature of Smoother Matrix $S_\lambda$

왜 smoothing spline의 smoother matrix, $S_\lambda$가 **수축하는(shrinking)** smoother로 불리는 것일까? 그 이유를 간단히 확인해 보자.  
$S_\lambda$의 고유값을 $\rho_1(\lambda), \rho_2(\lambda), ..., \rho_N(\lambda)$이라고 하자. ($\lambda$가 변하면 $S_\lambda$도 변하고 그에 따라 $S_\lambda$의 고유값도 변하므로, $\lambda$에 대한 함수로 나타내었다.) $S_\lambda$는 symmetric, positive semidefinite한 행렬이므로, 0 이상의 실수 고유값(eigenvalue)으로 다음과 같이 eigen-decomposition할 수 있다 ([Wikipedia](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix#Eigenvalues)). 

$$
S_\lambda = \sum_{k=1}^N \rho_k(\lambda)u_k u_k^T
$$

$S_\lambda=N \Big(  N^T N+ \lambda \Omega_N \Big) ^{-1} N^T$를 Reinsch form으로 나타내면 다음과 같다.

$$
N \Big(  N^T N+ \lambda \Omega_N \Big) ^{-1} N^T= N N^{-1} \Big( I+ \lambda (N^T)^{-1} \Omega_N N^{-1} \Big)^{-1} (N^T)^{-1} N^T =  \Big( I+ \lambda (N^T)^{-1} \Omega_N N^{-1} \Big)^{-1}
$$

$$
S_\lambda=N \Big(  N^T N+ \lambda \Omega_N \Big) ^{-1}  N^T=\Big( I+ \lambda (N^T)^{-1} \Omega_N N^{-1} \Big)^{-1}=(I+\lambda K)^{-1} 
$$

$$
where \enspace K =(N^T)^{-1} \Omega_N N^{-1}
$$

$K$는 $\lambda$의 영향을 받지 않는 symmetric 행렬이 된다. $S_\lambda=(I+\lambda K)^{-1}$와 $K$의 고유값($d_1,...,d_N$)를 이용하여, $S_\lambda$의 고유값 $\rho_1(\lambda),  ..., \rho_N(\lambda)$를 나타내면 다음과 같다.

$$
\rho_k(\lambda)=\frac{1}{1+\lambda d_k} \enspace , \enspace \enspace where \enspace k=1,2,...,N
$$

<br>

$$
S_\lambda = \sum_{k=1}^N \frac{1}{1+\lambda d_k} u_k u_k^T
$$

위와 같은 변형을 거치고 나면, $\lambda$를 통해 제약을 가하여 도출한 smoothing spline이 **수축하는(shrinking)** 특징을 갖는 이유를 한 눈에 확인할 수 있다. smoothing spline 최소화 문제에서 fitted curve의 구불구불한 정도(curvature)를 제한하는 penalty가 강하게 부과될 수록, $\lambda$의 값은 더 커지게 되고, **$S_\lambda$ 행렬의 값들의 절대값은 전체적으로 줄어들게 된다.** 즉 $\lambda$의 값이 클 수록, 더 smooth한 spline이 도출될 것을 예상해 볼 수 있다.  

아래의 사진은 같은 데이터에 $\lambda$의 값을 달리하여 smoothing spline을 적용한 결과를 나타낸 그림이다. 더 높은 값의 $\lambda$가 적용된 (즉, 더 curvature 제약이 강하게 적용된) 빨간색 smoothing spline이 초록색 smoothing spline보다 더 smooth한 곡선을 이루고 있는 것을 볼 수 있다. 따라서 이와 같은 curvature 제약 때문에, smoothing spline은 $N$개의 데이터로부터 $N$개의 basis로 이루어진 Natural cubic spline을 도출하는데도, 단순히 $N$개의 점을 모두 지나는 곡선이 아니라, 점들의 추세를 나타내는 smoothing 곡선이 될 수 있는 것이다.  
<br>
<br>
# 5.5 Automatic Selection of the Smoothing Parameters


그렇다면 데이터가 주어졌을 때, 위와 같이 Smoothing spline을 구하는 과정에서, curvature 제약의 강도를 결정하는 최소화문제의 tuning parameter, $\lambda$의 값은 어떻게 결정할까?

# 5.6 Nonparametric Logistic Regression
# 5.7 Multidimensional Splines
# 5.8 Regularization and Reproducing Kernel Hilbert Spaces
# 5.9 Wavelet Smoothing
  
# Reference
> Hastie, T., Tibshirani, R.,, Friedman, J. (2001). The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc.. 

-
