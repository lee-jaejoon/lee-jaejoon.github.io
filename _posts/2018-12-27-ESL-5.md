---
layout: post
title: "ESL: Ch 5. Basis Expansions and Regularization"
tags: [Elements of Statistical Learning]
comments: true
---
> Contents  
> [5.1 Introduction](#51-introduction)  
> [5.2 Piecewise Polynomials and Splines](#52-piecewise-polynomials-and-splines)  
> [5.3 Filtering and Feature Extraction](#53-filtering-and-feature-extraction)  
> [5.4 Smoothing Splines](#54-smoothing-splines)  
> [5.5 Automatic Selection of the Smoothing Parameters](#55-automatic-selection-of-the-smoothing-parameters)  
> [5.6 Nonparametric Logistic Regression](#56-nonparametric-logistic-regression)  
> [5.7 Multidimensional Splines](#57-multidimensional-splines)  
> [5.8 Regularization and Reproducing Kernel Hilbert Spaces](#58-regularization-and-reproducing-kernel-hilbert-spaces)  
> [5.9 Wavelet Smoothing](#59-wavelet-smoothing)  


# 5.1 Introduction

기본적인 통계 모형에서는 input feature들 사이의 선형성(linearity)을 가정하는 경우가 많다. 실제 예측변수와 목적변수 사이의 true relationship은 linear하지 않겠지만, 이를 linear한 모형으로 근사한 것이다.  
  
 * 예를 들면, 선형회귀분석은 true function $f(X)$를 "예측변수 $X$에 대한 목적변수 $Y$의 조건부 평균"으로 다음과 같이 설정한다. 
 
 $$
 f(X)=E[Y|X]=\beta_0+\beta_1 X_1+...+\beta_p X_p
 $$
  
 * 예측변수와 목적변수 사이의 true relationship $f(X)$는 $X$에 대해 linear, additive하다는 보장이 없다.
 * 다만, 분석 결과의 해석이 더 용이하다는 점, 그리고 $\beta_0+\beta_1 X_1+...+\beta_p X_p$는 $f(X)$의 1차 Taylor approximation이라는 점 등의 이유 때문에, $X$의 선형성을 가정하는 경우가 많다.  
<br>

## What is "basis" and "basis expansion"?
어떤 부분집합 $B= \\{ b_1, b_2, ...,b_n \\} \subset V$의 원소들의 선형결합으로 $B$가 속한 벡터공간 $V$의 모든 원소들을 나타낼 수 있다면, 우리는 $B$가 $V$를 span한다고 나타내거나, 혹은 $B$를 $V$의 spanning set이라고 부른다. 한 벡터공간을 span하는 집합은 무수히 많을 수 있다. 그 중에서도 **Basis**는 선형독립이면서 어떤 벡터공간을 span하는 set, 즉 어떤 벡터 공간의 **minimal spanning set**을 말한다.

예를 들면, $a\cdot1+b\cdot x+c\cdot x^2+d\cdot x^3$의 꼴로 3차 이하의 다항식을 모두 나타낼 수 있고,  $\\{ 1,x,x^2,x^3 \\}$는 서로 선형독립이기 때문에, $ \\{ 1,x,x^2,x^3 \\} $는 3차 이하의 모든 다항식들의 집합 $P_3$의 **basis**가 될 수 있다. 위의 linear model의 예의 경우 true function $f(X)$를 나타내는 **basis**는 각 input feature $ \\{ 1,X_1,X_2,...,X_p \\} $가 된다.  

따라서 이 챕터에서 다루는 **basis expansion**의 의미는 더이상 input feature $X_1,X_2,...,X_p$를 그대로 basis로 쓰지 않고, $X$의 transformation인 새로운 변수들을 모형의 basis로 사용하는 것을 말한다. 이를 **linear basis expansion**이라고 부르고, 다음과 같이 나타낸다.

$$
h_m:\mathbb{R}^p \rightarrow \mathbb{R}\:\:, m=1,2,...,M
$$  

$$
f(X)=\sum_{m=1}^{M} \beta_m h_m(X)=\beta_1 h_1(X)+\beta_2 h_2(X)+...+\beta_M h_M(X)
$$  

이제 true function $f(X)$는 input feature $X$에 대해서는 nonlinear일 수 있지만, 새로운 feature들 $h_1(X),..,h_M(X)$에 대해서는 linear, additive한 model이다.  
  
회귀분석에서 특정 예측변수를 log transformation하거나, 이차항 및 interaction 항을 추가하는 작업은 이와 같은 linear basis expansion의 한 예이다. 그 외에도 $h_m(X)$를 어떻게 설정하는지에 따라 다양한 모형이 있는데 이는 아래에서 다루도록 하겠다.  
  
또한, basis function $h_m(X)$를 적절하게 설정하는 것도 중요하지만, 모형의 복잡도를 조절하는 방법 역시 요구된다. 이에는 크게 다음과 같은 세 가지 방법이 있다.
 * Restriction method : basis function $h_m$의 class를 사전에 결정하는 방법.
 * Selection method : 모형의 fit에 유의미한 기여를 하는 basis function만 모형에 포함시키는 방법. ex) CART, MARS
 * Regularization method : 가능한 basis function $h_m(X)$를 모두 모형에 포함시키지만, 그 계수 $\beta_m(X)$에 제약을 거는 방법. ex) ridge regression
 * lasso regression은 selection method와 regularization method를 동시에 사용하는 모형이다.
<br>  
  
<br>
  

# 5.2 Piecewise Polynomials and Splines
우선 Section 5.7까지는 $X$가 1개의 예측변수를 갖는 것으로 가정한다.<br>

## Piecewise polynomial function  
**Piecewise polynomial** 함수, $f(X)$는 $X$의 정의역을 (겹치지 않고 서로 인접하는) 구간들로 잘라, 각 구간들 내에서 polynomial로 나타낼 수 있는 함수를 말한다. 아래의 그림과 같은 piecewise polynomial은 다음과 같은 basis로 나타낼 수 있다. 이 때, $f(X)$는 6개의 basis function으로 나타내어지므로 자유도(degrees of freedom)는 6이다.  

$$
f(X)=\theta_1 h_1(X)+\theta_2 h_2(X)+...+\theta_6 h_6(X)
$$

<p align="center"> 
<img src="https://user-images.githubusercontent.com/45325895/50602618-11d0fd80-0efb-11e9-82ce-b6d16643ce66.png" >
</p>

$$
h_1(X)=I(X< \xi_1), \enspace h_2(X)=I(X< \xi_1)\cdot X
$$

$$
h_3(X)=I(\xi_1 \leq X< \xi_2), \enspace h_4(X)=I(\xi_1 \leq X< \xi_2)\cdot X
$$

$$
h_5(X)=I(\xi_2 \leq X), \enspace h_6(X)=I(\xi_1 \leq X)\cdot X
$$


위와 같은 piecewise **linear** polynomial $f(X)$에 정의역 전체에서 연속이 될 조건을 추가하면 어떻게 될까? (책에서는 표현 상 이 부분에서 linear라는 표현을 affine과 구분하여 사용하지 않았다.) 추가되는 연속성 조건은 다음과 같다.  

$$
f(\xi^-_1 ) = f(\xi^+_1) \enspace \Rightarrow \enspace \theta_1+\theta_2 \xi_1 = \theta_3+\theta_4 \xi_1
$$

$$
f(\xi^-_2 ) = f(\xi^+_2) \enspace \Rightarrow \enspace \theta_3+\theta_4 \xi_2 = \theta_5+\theta_6 \xi_2
$$

piecewise linear polynomial $f(X)$에 연속의 조건인 두 개의 제약식이 추가되면, $6-2=4$개의 basis function으로 $f(X)$를 나타낼 수 있게 된다. 그 basis는 다음과 같다.

$$
f(X)=\theta_1 h_1(X)+\theta_2 h_2(X)+\theta_3 h_3(X)+\theta_4 h_4(X)
$$

<p align="center"> 
<img src="https://user-images.githubusercontent.com/45325895/50603776-ff58c300-0efe-11e9-8b6e-ee5987b87e13.png" >
</p>


$$
h_1(X)=1, \enspace h_2(X)=X
$$

$$
h_3(X)=(X-\xi_1)_+, \enspace h_4(X)=(X-\xi_2)_+
$$

 * $h_3(X)=(X-\xi_1)_+=max \\{ 0, X-\xi_1 \\}$는 다음 그림과 같은 함수를 의미한다.
<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/50604317-e00f6500-0f01-11e9-8210-e29bae21d3ed.png" >
</p>


이외에도 각 구간마다 일차함수가 아닌 $M$차 이하의 polynomial을, 그리고 연속 뿐만 아니라 $C^k$ class 조건 ($k$계 도함수까지 연속)도 부여할 수 있다. 예를 들면, 세 개의 knot ($\xi_1,\xi_2,\xi_3$)으로 나뉜 각 구간마다, 3차 polynomial을 이용하여, 2계 도함수까지 연속인 $f(X)$의 basis를 생각해보자. 먼저, 세 개의 knot은 정의역을 네 개의 구간으로 나누며 각 구간마다 4개, 16개의 basis function이 필요하다. ($4 \times 4 = 16$) 여기에 각 knot마다 연속, 1계도함수 연속, 2계도함수 연속과 같이 세 개의 제약식이 필요하므로, 9개의 basis function이 사라진다. ($3 \times 3 = 9$) 따라서, $f(X)$는 총 $16-9=7$개의 basis function으로 나타낼 수 있을 것이다.  
<br>
 
## Regression spline
<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/50833879-f439e300-1395-11e9-9852-d17456432c17.png">
</p>

> A “**spline**" is a thin strip of wood that can be easily bent to follow a curved line. Historically, it was used in drafting for drawing smooth curves. Regression splines, a statistical translation of this idea, are a way to represent nonlinear, but unknown, mean functions.

위의 사진과 같이, **spline**은 부드러운 곡선을 그리기 위해 사용된 얇고 긴 나무조각을 가리키는 말이다. 부드러운 곡선을 그리는 spline wood와 같이, 통계학에서 **spline**은 nonlinear한 (unknown) mean function을 그리는 방법을 의미하며, 크게 regression spline과 smoothing spline으로 나눌 수 있다. 이 절에서는 knot의 개수, 위치, 차수를 사전적으로 결정한 후 mean function을 그리는 regression spline을 다룬다.

**order-$M$ spline**은 $M-1$차 polynomial들로 이루어진 $C^{M-2}$ class의 piecewise polynomial function로 정의된다. 즉, $M-2$번 미분한 도함수까지 연속인 $M-1$차 이하의 piecewise polynomial을 (order-$M$) spline이라고 부른다. 만약 $M-1$차 piecewise polynomial이 ($M-2$번이 아닌) $M-1$번 미분한 도함수까지 연속이라면, 이는 더이상 piecewise polynomial이 아니라 global $M-1$차 polynomial이 될 것이다. 가장 많이 쓰이는 Cubic spline은 order-$4$ spline이다. $K$개의 knot를 갖는 **order-$M$ spline**의 basis는 다음과 같이 나타낼 수 있다.

$$
h_j(X)=X^{j-1}, \enspace j=1,...,M\enspace,  \enspace\enspace\enspace h_{M+l}(X)=(X-\xi_l)^{M-1}_+, \enspace l=1,...,K
$$

$$
f(X)=\theta_1 h_1(X)+\theta_2 h_2(X)+ ... +\theta_{M+K} h_{M+K}(X)
$$

다음 그림의 초록색 선은 두 개의 knot를 갖는 Cubic spline, 즉 order-$4$ spline을 나타낸 것이다. 구간과 구간 사이의 경계에서 연속이며 부드럽게 연결되는 것을 확인할 수 있다.

<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/50605745-9aa16680-0f06-11e9-99fa-9ce01f646b8b.png">
</p>

<br>

## Natural cubic spline
데이터를 piecewise polynomial로 fit하는 경우, 양 끝 값에서는 연속성 제약이 없고 그 주변에는 data도 적을 것이기 때문에, 정의역의 양 끝 값 주변에서 fitted value가 **불안정**해질 가능성이 있다. 여기서 **불안정**하다는 것은 데이터가 어떻게 뽑히는가에 따라서 fitted value가 크게 달라지게 되는 것을 의미한다. 우리가 관측한 observation은 데이터의 한 random realization일 것이다. 그런데 이 randomness에 따라 fitted value(여기서는 spline curve)가 그때그때 크게 달라진다면, 이는 큰 문제가 된다. 이를 **모형의 variance가 크다**고 하는데, **Natural cubic spline**은 이를 해결하고자 한 모형이다.

<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/50606266-80688800-0f08-11e9-9c7f-6b4576606169.png" >
</p>

**Natural cubic spline**은 좌우의 양 끝 boundary knot($\xi_1$과 $\xi_K$) 밖에서는 linear(일차함수 꼴)이고 그 이외의 구간에서는 3차 polynomial로 나타내어지는 spline을 말한다. 다시 말해서, **Natural cubic spline**은 "좌우 양 끝 구간에서, 차수가 2차 이상인 항의 계수$=0$"의 제약이 추가된 cubic spline이다. $K$개의 knot을 갖는 cubic spline이 $K+4$의 자유도를 갖는 것에 반해($M=4$), $K$개의 knot을 갖는 **Natural cubic spline**은 $(K+4)-4=K$개의 자유도를 갖는다. 양 끝 구간에서 각각 2 개씩, 총 4 개의 자유도를 아낄 수 있기 때문이다. 같은 차수의 cubic spline과 비교했을 때, **Natural cubic spline**은 다음과 같은 이점이 있다.

 * 같은 자유도를 갖는 cubic spline에 비해, interior region에서 4개의 knot을 더 가져갈 수 있다는 뜻이 된다.
 * Boundary 근처에서 fitting model의 bias가 약간 올라가는 대신, model의 variance를 낮출 수 있다.
	 * 어떤 데이터가 관측되더라도, spline curve가 크게 달라지지 않는다.
	 * Boundary knot 밖 구간에서는 어차피 많은 정보가 없으므로, linear 가정을 하는 것이 큰 문제가 없다.
  
## Example: South African heart disease



<br>


# 5.3 Filtering and Feature Extraction
# 5.4 Smoothing Splines

Regression spline을 그릴 때는 knot의 개수 및 위치를 임의로, 혹은 fit에 대한 어떤 measure를 통해 사전에 결정하여야 했다. 이 절에서는 사전에 knot의 개수 및 위치를 결정할 필요가 없는 **smoothing spline**에 대해 알아보고자 한다. 이 방법의 특징은 knot 으로 예측변수 $X$의 정의역이 가득차도록 포화(saturate)시키고, 그 대신 penalty항을 통해 각 knot들이 fitted value에 영향을 주는 것에 제약을 걸어 overfitting을 방지한다는 것이다. 몇몇 knot으로부터의 영향은 penalty항에 의해 완전히 사라질 수도 있다. 이처럼 penalty를 준다는 점에서 ridge regression과 lasso와 닮아있지만, **smoothing spline**은 새로운 종류의 penalty를 적용함으로써 $X$와 $Y$ 사이의 nonlinear 관계를 허용한다.  

다음과 같은 식을 최소화하는 함수 $f$를 생각해보자.

$$
\sum_{i=1}^{N} { \{ y_i-f(x_i) \} }^2 + \lambda \int f''(t)^2 dt
$$



# 5.5 Automatic Selection of the Smoothing Parameters
# 5.6 Nonparametric Logistic Regression
# 5.7 Multidimensional Splines
# 5.8 Regularization and Reproducing Kernel Hilbert Spaces
# 5.9 Wavelet Smoothing
