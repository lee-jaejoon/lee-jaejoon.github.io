---
layout: post
title: "ESL: Ch 5. Basis Expansions and Regularization"
tags: [Elements of Statistical Learning]
comments: true
---
> Contents  
> [5.1 Introduction](#51-introduction)  
> [5.2 Piecewise Polynomials and Splines](#52-piecewise-polynomials-and-splines)  
> [5.3 Filtering and Feature Extraction](#53-filtering-and-feature-extraction)  
> [5.4 Smoothing Splines](#54-smoothing-splines)  
> [5.5 Automatic Selection of the Smoothing Parameters](#55-automatic-selection-of-the-smoothing-parameters)  
> [5.6 Nonparametric Logistic Regression](#56-nonparametric-logistic-regression)  
> [5.7 Multidimensional Splines](#57-multidimensional-splines)  
> [5.8 Regularization and Reproducing Kernel Hilbert Spaces](#58-regularization-and-reproducing-kernel-hilbert-spaces)  
> [5.9 Wavelet Smoothing](#59-wavelet-smoothing)  


# 5.1 Introduction

기본적인 통계 모형에서는 input feature들 사이의 선형성(linearity)을 가정하는 경우가 많다. 이는 예측변수와 목적변수 사이의 (nonlinear할 것으로 예상되는) true relationship을 linear한 모형으로 근사한 것이다.  
  
 * 예를 들면, 선형회귀분석은 true function $f(X)$를 "예측변수 $X$에 대한 목적변수 $Y$의 조건부 평균"으로 다음과 같이 설정한다. 
 
 $$
 f(X)=E[Y|X]=\beta_0+\beta_1 X_1+...+\beta_p X_p
 $$
 
 * 예측변수와 목적변수 사이의 true relationship $f(X)$는 $X$에 대해 linear, additive하다는 보장이 없다.
 * 다만, 분석 결과의 해석이 더 용이하다는 점, 그리고 $\beta_0+\beta_1 X_1+...+\beta_p X_p$는 $f(X)$의 1차 Taylor approximation이라는 점 등의 이유 때문에, $X$의 선형성을 가정하는 경우가 많다.  
  

## What is "basis" and "basis expansion"?
어떤 부분집합 $B= \\{ b_1, b_2, ...,b_n \\} \subset V$의 원소들의 선형결합으로 $B$가 속한 벡터공간 $V$의 모든 원소들을 나타낼 수 있다면, 우리는 $B$가 $V$를 span한다고 나타내거나, 혹은 $B$를 $V$의 spanning set이라고 부른다. 한 벡터공간을 span하는 집합은 무수히 많을 수 있다. 기저(basis)는 어떤 벡터 공간의 minimal spanning set, 즉 선형독립이면서 spanning set을 말한다.

예를 들면, $a\cdot1+b\cdot x+c\cdot x^2+d\cdot x^3$의 꼴로 3차 이하의 다항식을 모두 나타낼 수 있고,  $\\{ 1,x,x^2,x^3 \\}$는 서로 선형독립이기 때문에, $ \\{ 1,x,x^2,x^3 \\} $는 3차 이하의 모든 다항식들의 집합 $P_3$의 basis이다. (물론 이것이 유일한 basis는 아니다.)

위의 linear model의 예의 경우 true function $f(X)$를 나타내는 basis는 각 input feature $ \\{ 1,X_1,X_2,...,X_p \\} $가 된다.  

따라서 이 챕터에서 다루는 basis expansion의 의미는 더이상 input feature $X_1,X_2,...,X_p$를 그대로 basis로 쓰지 않고, $X$의 transformation인 새로운 변수들을 모형의 basis로 사용하는 것을 말한다. 이를 linear basis expansion이라고 부르고, 다음과 같이 나타낸다.

$$
h_m:\mathbb{R}^p \rightarrow \mathbb{R}\:\:, m=1,2,...,M
$$  

$$
f(X)=\sum_{m=1}^{M} \beta_m h_m(X)=\beta_1 h_1(X)+\beta_2 h_2(X)+...+\beta_M h_M(X)
$$  

이제 true function $f(X)$는 input feature $X$에 대해서는 nonlinear일 수 있지만, 새로운 feature들 $h_1(X),..,h_M(X)$에 대해서는 linear, additive한 model이다.  
  
회귀분석에서 특정 예측변수를 log transformation하거나, 이차항 및 interaction 항을 추가하는 작업은 이와 같은 linear basis expansion의 한 예이다. 그 외에도 $h_m(X)$를 어떻게 설정하는지에 따라 다양한 모형이 있는데 이는 아래에서 다루도록 하겠다.  
  
또한, basis function $h_m(X)$를 적절하게 설정하는 것도 중요하지만, 모형의 복잡도를 조절하는 방법 역시 요구된다. 이에는 크게 다음과 같은 세 가지 방법이 있다.
 * Restriction method : basis function $h_m$의 class를 사전에 결정하는 방법.
 * Selection method : 모형의 fit에 유의미한 기여를 하는 basis function만 모형에 포함시키는 방법. ex) CART, MARS
 * Regularization method : 가능한 basis function $h_m(X)$를 모두 모형에 포함시키지만, 그 계수 $\beta_m(X)$에 제약을 거는 방법. ex) ridge regression
 * lasso regression은 selection method와 regularization method를 동시에 사용하는 모형이다.
  
  
  

# 5.2 Piecewise Polynomials and Splines
++우선 Section 5.7까지는 $X$가 1개의 예측변수를 갖는 것으로 가정한다.++<br>

## Piecewise polynomial function  
**Piecewise polynomial** 함수, $f(X)$는 $X$의 정의역을 (겹치지 않고 서로 인접하는) 구간들로 잘라, 각 구간들 내에서 polynomial로 나타낼 수 있는 함수를 말한다. 아래의 그림과 같은 piecewise polynomial은 다음과 같은 basis로 나타낼 수 있다. 이 때, $f(X)$는 6개의 basis function으로 나타내어지므로 자유도(degrees of freedom)는 6이다.  

$$
f(X)=\theta_1 h_1(X)+\theta_2 h_2(X)+...+\theta_6 h_6(X)
$$

<p align="center"> 
<img src="https://user-images.githubusercontent.com/45325895/50602618-11d0fd80-0efb-11e9-82ce-b6d16643ce66.png" >
</p>

$$
h_1(X)=I(X< \xi_1), \enspace h_2(X)=I(X< \xi_1)\cdot X
$$

$$
h_3(X)=I(\xi_1 \leq X< \xi_2), \enspace h_4(X)=I(\xi_1 \leq X< \xi_2)\cdot X
$$

$$
h_5(X)=I(\xi_2 \leq X), \enspace h_6(X)=I(\xi_1 \leq X)\cdot X
$$


위와 같은 piecewise **linear** polynomial $f(X)$에 정의역 전체에서 연속이 될 조건을 추가하면 어떻게 될까? (책에서는 표현 상 이 부분에서 linear라는 표현을 affine과 구분하여 사용하지 않았다.) 추가되는 연속성 조건은 다음과 같다.  

$$
f(\xi^-_1 ) = f(\xi^+_1) \enspace \Rightarrow \enspace \theta_1+\theta_2 \xi_1 = \theta_3+\theta_4 \xi_1
$$

$$
f(\xi^-_2 ) = f(\xi^+_2) \enspace \Rightarrow \enspace \theta_3+\theta_4 \xi_2 = \theta_5+\theta_6 \xi_2
$$

piecewise linear polynomial $f(X)$에 연속의 조건인 두 개의 제약식이 추가되면, $6-2=4$개의 basis function으로 $f(X)$를 나타낼 수 있게 된다. 그 basis는 다음과 같다.

$$
f(X)=\theta_1 h_1(X)+\theta_2 h_2(X)+\theta_3 h_3(X)+\theta_4 h_4(X)
$$

<p align="center"> 
<img src="https://user-images.githubusercontent.com/45325895/50603776-ff58c300-0efe-11e9-8b6e-ee5987b87e13.png" >
</p>


$$
h_1(X)=I(X< \xi_1), \enspace h_2(X)=I(X< \xi_1)\cdot X
$$

$$
h_3(X)=(X-\xi_1)_+, \enspace h_4(X)=(X-\xi_2)_+
$$

 * $h_3(X)=(X-\xi_1)_+=max \\{ 0, X-\xi_1 \\}$는 다음 그림과 같은 함수를 의미한다.
<p align="center">
<img src="https://user-images.githubusercontent.com/45325895/50604317-e00f6500-0f01-11e9-8210-e29bae21d3ed.png" >
</p>


이외에도 각 구간마다 일차함수가 아닌 $M$차 이하의 polynomial을, 그리고 연속 뿐만 아니라 $C^k$ class 조건 ($k$계 도함수까지 연속)도 부여할 수 있다. 예를 들면, 세 개의 knot ($\xi_1,\xi_2,\xi_3$)으로 나뉜 각 구간마다, 3차 polynomial을 이용하여, 2계 도함수까지 연속인 $f(X)$의 basis를 생각해보자. 먼저, 세 개의 knot은 정의역을 네 개의 구간으로 나누며 각 구간마다 4개, 16개의 basis function이 필요하다. ($4 \times 4 = 16$) 여기에 각 knot마다 연속, 1계도함수 연속, 2계도함수 연속과 같이 세 개의 제약식이 필요하므로, 9개의 basis function이 사라진다. ($3 \times 3 = 9$) 따라서, $f(X)$는 총 $16-9=7$개의 basis function으로 나타낼 수 있을 것이다.
  
## Spline
**Spline**은 주어진 점들(knots)을 통과하는 부드러운 곡선으로, 인접한 두 점 사이의 구간마다 polynomial을 이용해 곡선을 정의한다. **order-$M$ spline**은 $M-1$차 polynomial들로 이루어진 $C^{M-2}$ class의 piecewise polynomial function로 정의된다. 만약 $M-1$차 piecewise polynomial이 ($M-2$번이 아닌) $M-1$번 미분한 도함수까지 연속이라면, 이는 더이상 piecewise polynomial이 아니라 global $M-1$차 polynomial이 될 것이다. 가장 많이 쓰이는 Cubic spline은 order-$4$ spline이다. **order-$M$ spline**의 basis는 다음과 같이 나타낼 수 있다.


# 5.3 Filtering and Feature Extraction
# 5.4 Smoothing Splines
# 5.5 Automatic Selection of the Smoothing Parameters
# 5.6 Nonparametric Logistic Regression
# 5.7 Multidimensional Splines
# 5.8 Regularization and Reproducing Kernel Hilbert Spaces
# 5.9 Wavelet Smoothing
