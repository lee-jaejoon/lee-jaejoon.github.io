---
layout: post
title: "3. Dirichlet Process (2)"
tags: [Bayesian Nonparametrics]
comments: true
---
# 3.2 Properties (cont.)
## 3.2.4 Conjugacy

Dirichlet process prior의 가장 유용한 성질 중 하나는 ***conjugacy***, 즉 posterior distribution 역시 Dirichlet process를 따른다는 점이다. Observation $X_1, \cdots , X_N$이 Dirichlet process prior로부터 생성된 distribution $P$로부터 independent하게 sample되었다고 하자.

$$
\begin{align*}
P &\sim \text{DP}(\alpha) \\
X_i \mid P &\stackrel{\text{iid}}{\sim} P , \enspace \enspace \enspace i=1, \cdots , N
\end{align*}
$$

뭉뚱그린 표현이지만, 이와 같이 생성된 observation을 편의상 주로 *sample from the Dirichlet process*라고 표현한다. 이 때 다음 정리가 성립한다.  
  
 > Given an i.i.d sample $X_1, \cdots , X_N $ from the $\text{DP}(\alpha)$-process, the $\text{DP}(\alpha + \sum_{i=1}^{N} \delta_{X_i})$-process is a version of the posterior distribution.

### Necessary Concepts

Dirichlet process의 증명을 위해, 몇 가지 개념들에 대한 정확한 정의가 필요하다. Conditional expectation과 conditional probability에 대한 정의는 다음과 같다.

 > Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $\mathcal G$ be a sub-$\sigma$-field of $\mathcal F$, i.e. , $\mathcal G$ is a $\sigma$-algebra and $\mathcal G \subset \mathcal F$. 
 > 
 > The **conditional expectation** of a random variable $X$ given $\mathcal G$, $\mathbb E(X \mid \mathcal G )$, is any $\mathcal G$-measurable function, $Y$, satisfying the following.
 > 
 > $$
 > \mathbb E[ Y I_A ] = \mathbb E[ X I_A ], \text{ for all } A \in \mathcal G
 > $$
 > 
 > Any $Y$ satisfying the above definition is said to be a version of $\mathbb E(X \mid \mathcal G )$.
 > 
 > The **conditional probability** of an event $A \in \mathcal F$ given $\mathcal G$, $\mathbb P(A \mid \mathcal G )$, is a conditional expectation of a indicator random variable $I_A$, $\mathbb E(I_A \mid \mathcal G )$, i.e., any $\mathcal G$-measurable function, $Y$, satisfying the following.
 > 
 > $$
 > \mathbb E[ Y I_B ] = \mathbb E[ I_A I_B ]=\mathbb E[ I_{A\cap B} ]=\mathbb P(A \cap B), \text{ for all } B \in \mathcal G
 > $$
 > 
 > Any $Y$ satisfying the above definition is said to be a version of $\mathbb P( A \mid \mathcal G )$.
 > 
 > Also, we define
 > 
 > $$
 > \mathbb E(X \mid Y)= \mathbb E(X \mid \sigma(Y))
 > $$
 > 
 > where $\sigma(Y)$ is the $\sigma$-algebra generated by a random variable $Y$.
 > 

Regular conditional probability에 대한 정의는 다음과 같다.
 > Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $X:(\Omega, \mathcal F) \rightarrow (S, \mathcal S)$ be a $\mathcal F$-measurable function. Let $\mathcal G$ be a $\sigma$-algebra such that $\mathcal G \subset \mathcal F$. A mapping $\mu : \Omega \times \mathcal S \rightarrow [0,1]$ is a **regular conditional distribution** for $X$ given $\mathcal G$, if the following holds:
 > 1. $\forall A \in \mathcal S$, the map $\omega \mapsto \mu(\omega, A)$ is a version of $\mathbb P(X \in A \mid \mathcal G)$.  
 > 2. For a.e. $\omega \in \Omega$, the map $A \mapsto \mu(\omega, A)$ is a probability measure on $(S, \mathcal S)$.
 > 
 > When $S = \Omega$ and $X$ is the identity map, then a mapping $\mu : \Omega \times \mathcal F \rightarrow [0,1]$ is a **regular conditional probability** given $ \mathcal G$, if the following holds:
 > 1. $\forall A \in \mathcal F$, the map $\omega \mapsto \mu(\omega, A)$ is a version of $\mathbb P(X \in A \mid \mathcal G)$.  
 > 2. For a.e. $\omega \in \Omega$, the map $A \mapsto \mu(\omega, A)$ is a probability measure on $(\Omega, \mathcal F)$.
 > 

마지막으로, dirichlet process의 conjugacy를 증명하는데 사용될 monotone class theorem for functions를 소개하겠다. 먼저 일반적인 monotone class theorem은 다음과 같다. 자세한 내용은 [Durrett - Probability: Theory and Examples](https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf)의 Theorem 5.2.2를 참고하자.

> **(Monotone Class Theorem)** Let $\mathcal A$ be a $\pi$-system that contains $\Omega$ and let $\mathcal H$ be a collection of real-valued functions that satisfies:
> 
> 1. If $A \in \mathcal A$, then $1_A \in \mathcal H$.
> 2. If $f,g \in \mathcal H$, then $f+g, cf \in \mathcal H$ for any $c \in \mathbb R$.
> 3. If $f_n \in \mathcal H$ are nonnegative and increase to a bounded function $f$, then $f \in \mathcal H$
> 
> Then $\mathcal H$ contains all bounded functions which are measurable with respect to $\sigma(\mathcal A)$.
>

이는 다음과 같은 증명 과정을 따른다.

 * $\mathcal A$ be a $\pi$-system that contains $\Omega$.
 * $\mathcal G= \{ A : 1_A \in \mathcal H \}$ is a $\lambda$-system.
	 * $\mathcal A$ contains $\Omega$, $\mathcal H$ contains $1_\Omega$, then $\mathcal G$ contains $\Omega$.
	 * If $A,B \in \mathcal G$ satisfying $A \subset B$, $\mathcal H$ contains $1_A, 1_B$, and $1_{B \backslash A} = 1_B - 1_A$. Then $B \backslash A \in \mathcal G$
     * $A_1, A_2 , \cdots \in \mathcal G$ satisfying $A_n \subset A_{n+1}$, $\forall n$, then $1_{A_1}, 1_{A_2} , \cdots \in \mathcal H$ satisfying $1_{A_n} \leq 1_{A_{n+1}}$, $\forall n$. By the condition 3, $\lim_{n\rightarrow\infty}1_{A_n} = 1_{\cup_{n=1}^\infty A_n} \in \mathcal H$ because $\lim_{n\rightarrow\infty}1_{A_n} \leq 1 $ a.e.. Then $\cup_{n=1}^\infty A_n \in \mathcal G$.
 * Then, by Dynkin's $\pi-\lambda$ theorem, $\sigma(\mathcal A) \subset \mathcal G=\{ A : 1_A \in \mathcal H \}$.
 * The condition 2 implies $\mathcal H$ contains all simple functions, with respect to $\sigma(\mathcal A)$.
 * The condition 3 implies $\mathcal H$ contains all bounded measurable functions, with respect to $\sigma(\mathcal A)$. $\square$
  
이를 이용하여 다음과 같은 monotone class theorem for functions를 얻을 수 있다.

> A collection of real-valued functions, $\mathcal M$, is a **multiplicative class** if
> 
> $$
> f,g \in \mathcal M \implies f \cdot g \in \mathcal M
> $$
> 
> A collection of real-valued bounded functions, $\mathcal H$, is a **monotone vector space** if the following hold.
> 
> 1. $\mathcal H$ is a vector space over $\mathbb R$.
> 2. $1 = 1_\Omega \in \mathcal H$.
> 3. $f_n \geq 0, f_n \in \mathcal H, f_n \uparrow f, f: \text{bdd} \implies f \in \mathcal H$.
> 
> **(Monotone Class Theorem For Functions)** Let $\mathcal M$ be a multiplicative class of bounded real-valued functions on $\Omega$ and Let $\mathcal H$ be a monotone vector space of real-valued bounded functions. Let $\mathcal B$ be a $\sigma$-algebra that makes all elements of $\mathcal M$ measurable, i.e., $\mathcal B = \sigma (\mathcal M)$. Then the following holds.
> 
> $$
> \mathcal M \subset \mathcal H \implies \mathcal H \text{ contains all bounded } \mathcal B \text{-measurable real-valued functions}
> $$
> 

이는 다음과 같은 증명 과정을 따른다.

 * We may assume that $1_\Omega \in \mathcal M$, because if $\mathcal M$ is a multiplicative class then $\mathcal M \cup \{ 1_{\Omega} \}$ is also a multiplicative class.
 * Let $\mathcal A$ be a $\pi$-system on $\Omega$ generating $\mathcal B = \sigma (\mathcal M)$.
 * If we show that $1_A \in \mathcal H$ for all $A \in \mathcal A$, then, by monotone class theorem, $\mathcal H$ contains all bounded functions which are measurable with respect to $\sigma(\mathcal A)=\mathcal B$.
	 * Let 
 * **(보충 필요)**

<br>


### Proof of Conjugacy of Dirichlet Process

이제 Dirichlet process의 posterior 분포를 알아보자.

 > **(Conjugacy)** Given an i.i.d sample $X_1, \cdots , X_N $ from the $\text{DP}(\alpha)$-process, 
 > 
 > $$
\begin{align*}
P &\sim \text{DP}(\alpha) \\
X_i \mid P &\stackrel{\text{iid}}{\sim} P , \enspace \enspace \enspace i=1, \cdots , N
\end{align*}
$$
 >
 > the $\text{DP}(\alpha + \sum_{i=1}^{N} \delta_{X_i})$-process is a version of the posterior distribution.
 > 

우리는 이를 증명하기 위해서 sample의 갯수가 $1$개일 때의 상황을 증명하면 된다. $N$개의 sample일 때의 결과는 같은 과정을 $N$번 반복하여 얻을 수 있다. 

$$
\begin{align*}
P \mid X_1 , \cdots , X_{N-1} &\sim \text{DP} \left( \alpha+ \sum_{i=1}^{N-1} \delta_{X_i} \right) \\
X \mid P, X_1 , \cdots , X_{N-1} &\sim P\\
P \mid X_1 , \cdots , X_N &\sim \text{DP} \left( \alpha + \sum_{i=1}^{N} \delta_{X_i} \right)
\end{align*}
$$

따라서 다음의 증명을 소개하겠다.

$$
\begin{align*}
P &\sim \text{DP} \left( \alpha \right) \\
X \mid P &\sim P\\
\implies P \mid X &\sim \text{DP} \left( \alpha +  \delta_{X} \right)
\end{align*}
$$

보다 정확히 이야기하자면, **$\text{DP}(\alpha + \delta_X)$의 distribution을 따르는 어떤 random probability measure $\tilde P$가, $X$가 주어졌을 때의 $P$의 regular conditional probability의 한 version이 된다**는 것이다. 

$$
\mathbb P(P \in M \mid X=x)=\text{DP}_{\alpha +\delta_x}(M), \enspace \text{ for all }x \in \mathcal X, M \in \mathcal M
$$

여기서 주의할 점은 $M$은 The Borel $\sigma$-algebra generated by weakly open sets on $\mathfrak M =M(\mathfrak X)$,  $\mathcal M$의 원소이므로, probability measure들을 원소로 갖는 measurable set이라는 점이다. 다시 소개하자면, regular conditional probability의 정의는 다음과 같다.

 > A mapping $\mu : \Omega \times \mathcal F \rightarrow [0,1]$ is a **regular conditional probability** given sub-$\sigma$-algebra $ \mathcal G$, if the following holds:
 > 1. For a.e. $\omega \in \Omega$, the map $A \mapsto \mu(\omega, A)$ is a probability measure on $(\Omega, \mathcal F)$.
 > 2. $\forall A \in \mathcal F$, the map $\omega \mapsto \mu(\omega, A)$ is a version of $\mathbb P(A \mid \mathcal G)$.  
 >  

우리는 $\text{DP}(\alpha + \delta_X)$가, $\mathbb P(P \in M \mid X)$에 대해, 위 두 조건을 만족시키는 것을 보이고자 한다. 정확한 의미를 이해하기 위해 notation을 엄밀하게 적으면 다음과 같다.

> Let $(\Omega, \mathcal F, \mathbb P)$ be a probability space. Let $X : (\Omega, \mathcal F) \rightarrow (\mathfrak X, \mathcal X)$ be a random variable, and $P:(\Omega, \mathcal F) \rightarrow (\mathfrak M, \mathcal M)$ a random probability measure, where $\mathfrak M$ is the space of probability measures on $(\mathfrak X, \mathcal X)$ and $\mathcal M$ is the Borel $\sigma$-algebra on $\mathfrak M$ generated by weak topology. For $M \in \mathcal M$,
> 
> $$
> \begin{align*}
> \mathbb P(P \in M \mid X) &= \mathbb P \Big( \{ \omega: P(\omega) \in M \} \mid \sigma(X) \Big) \\
> &= \mathbb P \Big( P^{-1}(M) \mid \sigma(X) \Big)
> \end{align*}
> $$

일반적인 *probability measure* $(\mathbb P)$와 어떤 random variable(여기서는 "probability measure"-valued random variable, 즉 random probability measure)에 대한 *induced probability measure* $(P^X = \mathbb P \cdot X^{-1})$의 관계를 이용하여, 우리의 상황에 맞게 regular conditional probability의 조건을 다시 적으면 아래와 같다.

 > A mapping $\mu : \Omega \times \mathcal M \rightarrow [0,1]$ is a **(induced) regular conditional probability** given sub-$\sigma$-algebra $ \sigma(X)$, if the following holds:
 > 1. For a.e. $\omega \in \Omega$, the map $M \mapsto \mu(\omega, M)$ is a probability measure on $(\mathfrak M, \mathcal M)$.
 > 2. $\forall M \in \mathcal M$, the map $\omega \mapsto \mu(\omega, M)$ is a version of $\mathbb P( P^{-1}(M) \mid \mathcal \sigma(X))$.
 >

따라서, 다음 두 가지 사실을 보이면 증명이 끝난다.  

$$
1. \forall \omega \in \Omega, \text{ DP}_{\alpha + \delta_{X(\omega)}}(\cdot) \text{ is a probability measure on } (\mathfrak M, \mathcal M). \\
2. \forall M \in \mathcal M, \text{ DP}_{\alpha + \delta_{X(\omega)}}(M) \text{ is a version of the conditional probability } \mathbb P(P \in M \mid X).
$$

### Fact 1

Fact 1은 쉽게 보일 수 있다. 어떤 fixed $\omega \in \Omega$에 대해서, $\alpha(\cdot)+\delta_{X(\omega)}(\cdot)$는 finite measure이다. 따라서, 그에 대해 $\text{ DP}_{\alpha + \delta_{X(\omega)}}(\cdot)$는 Dirichlet process이며, 즉 probability measure on the space of probability measure이기 때문이다.  
  
### Fact 2

Fact 2는 어떤 임의의 $ M \in \mathcal M$에 대하여, $\text{ DP}_{\alpha + \delta_{X(\omega)}}(M)$이 conditional probability의 조건을 만족하는지를 확인함으로써 보일 수 있다. 다시 소개하자면, conditional probability의 정의는 다음과 같다.

 > The **conditional probability** of an event $A \in \mathcal F$ given $\mathcal G$, $\mathbb P(A \mid \mathcal G )$, is a conditional expectation of a indicator random variable $I_A$, $\mathbb E(I_A \mid \mathcal G )$, i.e., any $\mathcal G$-measurable function, $Y$, satisfying the following.
 > 
 > $$
 > \mathbb E[ Y I_B ] = \mathbb E[ I_A I_B ]=\mathbb E[ I_{A\cap B} ]=\mathbb P(A \cap B), \text{ for all } B \in \mathcal G
 > $$
 > 
 > Any $Y$ satisfying the above definition is said to be a version of $\mathbb P( A \mid \mathcal G )$.
 >

즉, 임의의 $M \in \mathcal M$에 대해서 $\text{ DP}_{\alpha + \delta_{X(\omega)}}(M)$이 a version of $\mathbb P( P^{-1}(M) \mid \sigma(X))$이기 위해서는,

 * $(\text{Fact } 2.1) \enspace \forall M \in \mathcal M, \text{ DP}_{\alpha + \delta_{X(\omega)}}(M) \text{ is } \sigma(X) \text{-measurable.}$
 * $(\text{Fact }2.2) \enspace \forall M \in \mathcal M, \forall B \in \mathcal X,$

$$
\mathbb E[DP_{\alpha+\delta_X}(M) \cdot I(X \in B)]=\mathbb P(P \in M, X \in B)
$$

### Fact 2.1

$$
\forall M \in \mathcal M, \text{ DP}_{\alpha + \delta_{X(\omega)}}(M) \text{ is } \sigma(X) \text{-measurable.}
$$

**(보충 필요)**

### Fact 2.2

For all $M \in \mathcal M$, and $B \in \mathcal X$,

$$
\mathbb E[DP_{\alpha+\delta_X}(M) \cdot I(X \in B)]=\mathbb P(P \in M, X \in B)
$$

먼저 좌변을 정리하면 아래와 같다. 세 번째 등호는 "Dirichlet process(에서 생성된 $P$)로부터 생성된 observation" $X$의 marginal 분포가 Dirichlet process의 center measure $G_0 = \frac{1}{\alpha_0} \alpha$라는 점을 이용한 것이다.

$$
\begin{align*}
\text{LHS} &= \mathbb E[DP_{\alpha+\delta_X}(M) \cdot I(X \in B)]\\
&= \int_\left\{ \omega : X(\omega) \in B \right\} DP_{\alpha+\delta_{X(\omega)}}(M)) \text{ } \mathbb P(d \omega) \\
&= \int_B DP_{\alpha+\delta_x}(M)) \text{ } G_0( dx) \\
&= \int_B \int I(P \in M) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) 
\end{align*}
$$


우변을 정리하면 아래와 같다. 

$$
\begin{align*}
\text{RHS} &= \mathbb P(P \in M, X \in B) \\
&= \mathbb P(P(B_1) \leq t_1, \cdots , P(B_k) \leq t_k,  X \in B) \\
&= \int_M P(B) \text{ DP}_\alpha(dP) \\
&= \int I(P \in M) P(B) \text{ DP}_\alpha(dP) \\
\end{align*}
$$

즉, Fact 2.2를 보이기 위해서는 다음 등식이 성립함을 보여야 한다.

$$
\int_B \int I(P \in M) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int I(P \in M) P(B) \text{ DP}_\alpha(dP)
$$

이를 위해 위에서 소개한 monotone class theorem for functions를 사용할 것이다. 다시 소개하자면, 이 정리는 bounded real-valued function on $\Omega$로 이루어진 multiplicative class $\mathcal C$에 대해서, 이 multiplicative class가 bounded real-valued function으로 이루어진 monotone vector space $\mathcal H$의 부분집합이 될 때, $\mathcal C$로 generate된 $\sigma$-algebra $\sigma(\mathcal C)$에 대해, monotone vector space $\mathcal H$는 $\sigma(\mathcal C)$-measurable한 모든 bounded real-valued을 포함한다는 것을 의미한다.

다음과 같이 multiplicative class $\mathcal C$과 monotone vector space $\mathcal H$를 설정하겠다.

$$
\mathcal C = \left\{ f(P)= P(B_1)^{r_1} \cdots P(B_k)^{r_k} : k \in \mathbb N , B_i \in  \mathcal B, r_i \in \mathbb N, i= 1, \cdots, k\right\} \\
\mathcal H = \left\{ f : \mathfrak M \rightarrow \mathbb R\text{, bounded, }\mathcal M \text{- measurable, }\int_B \int f(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int f(P) P(B) \text{ DP}_\alpha(dP), \forall B \in \mathcal B \right\}
$$

위와 같이 정의한 $\mathcal C$가 multiplicative class of bounded real function이라는 것은 자명하다. 또한 $\mathcal C$는 다음과 같은 사실들을 만족한다. $\mathcal M = \sigma(\mathcal C_0)$는 Ghosal & Van der Vaart, proposition A.5.(i)에 따른 결과이다.

> $$
> \pi_B : \mathfrak M \rightarrow \mathbb R \text{ such that }P \mapsto P(B)
> $$
> 
> $\sigma(\{ \pi_B : B \in \mathcal B \})$ is the smallest $\sigma$-algebra on $\mathfrak M$ making $\pi_B$ measurable for all $B \in \mathcal B$. $\mathcal M$ is the Borel $\sigma$-algebra on $\mathfrak M$ for the weak topology. Then
>
> $$
> \sigma(\{ \pi_B : B \in \mathcal B \}) =\mathcal M 
> $$

$\mathcal H$가 monotone vector space라는 것을 보이자. 
 * $f_1 , f_2 \in \mathcal H, c_1, c_2 \in \mathbb R \implies c_1f_1 +c_2f_2 \in \mathcal H$

$$
\begin{align*}
&\int_B \int \{ c_1f_1(P) + c_2f_2(P) \} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&=c_1 \int_B \int f_1(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) +c_2 \int_B \int f_2(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx)\\
&= c_1 \int f_1(P) P(B) \text{ DP}_\alpha(dP)+c_2 \int f_2(P) P(B) \text{ DP}_\alpha(dP)\\
&= \int \{ c_1f_1(P) + c_2f_2(P) \}  P(B) \text{ DP}_\alpha(dP)
\end{align*}
$$


 * $1 \in \mathcal H \text{ (constant function of value }1)$

$$
\int_B \int 1(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int_B \left( \int \text{ }DP_{\alpha+\delta_x}(dP) \right) G_0( dx) = \int_B 1 \text{ } G_0( dx) = G_0(B)
$$

$$
\int 1(P) P(B) \text{ DP}_\alpha(dP) = \int  P(B) \text{ DP}_\alpha(dP) =\mathbb E[P(B)] = G_0(B)
$$

 * $f_n \geq 0, f_n \in \mathcal H, f_n \uparrow f, f: \text{bdd} \implies f \in \mathcal H$

$$
\begin{align*}
&\int_B \int f(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&= \int_B \int \lim_{n\rightarrow \infty} f_n(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&= \lim_{n\rightarrow \infty} \int_B \int  f_n(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \text{ (} \because \text{ DCT}) \\
&= \lim_{n\rightarrow \infty} \int f_n(P) P(B) \text{ DP}_\alpha(dP) \text{ (} \because \text{ } f_n \in \mathcal H) \\
&=  \int  \lim_{n\rightarrow \infty} f_n(P) P(B) \text{ DP}_\alpha(dP) \text{ (} \because \text{ DCT}) \\
&= \int  f(P) P(B) \text{ DP}_\alpha(dP)  \\
\end{align*}
$$

이제 $\mathcal C \subset \mathcal H$를 보이면 우리가 정의한 $\mathcal C$, $\mathcal H$에 대해 monotone class theorem을 적용할 수 있다. 다음과 같이 multiplicative class $\mathcal C$의 subcollection $\mathcal C^\ast$를 정의하자. $\mathcal C$의 조건에 $(B_1, \cdots , B_k)$이 $\mathfrak X$의 partition이 된다는 조건을 추가한 것이다.

$$
\mathcal C^\ast = \left\{ f(P)= P(B_1)^{r_1} \cdots P(B_k)^{r_k} : k \in \mathbb N , (B_1, \cdots , B_k) \text{ : partition of }\mathfrak X , B_i \in \mathcal B, r_i \in \mathbb N, i= 1, \cdots, k\right\} 
$$

임의의 $B_i \in \mathcal B$에 대해, $B_1 \cup \cdots \cup B_k$를 disjoint한 조각으로 나누는 partition, $\tilde B_1 , \cdots, \tilde B_l$을 고려하면, $\mathcal C$의 모든 원소들은 $\mathcal C^\ast$의 원소들의 선형결합으로 나타낼 수 있다. monotone vector space $\mathcal H$는 선형결합에 닫혀있기 때문에, $\mathcal C^\ast$이 $\mathcal H$에 들어가는 것을 보이면 된다.  
  
$\mathcal B$의 원소로 구성된 $\mathfrak X$의 임의의 finite partition $(B_1, \cdots , B_k)$를 생각해보자. 또한 그에 대해 다음과 같이 $\alpha ' \in \mathbb R^k$를 정의하자.

$$
\alpha ' = (\alpha(B_1), \cdots , \alpha(B_k)) = (\alpha_1, \cdots , \alpha_k)
$$

만약 $\mathfrak X$의 임의의 finite partition $(B_1, \cdots , B_k)$, $r_1 , \cdots, r_k \in \mathbb R$에 대해 $\prod_{i=1}^{k} P(B_i)^{r_i}$가 아래 식을 만족한다면, $\mathcal C^\ast \subset \mathcal H$이 된다.

$$
\int_B \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int \prod_{i=1}^{k} P(B_i)^{r_i} P(B) \text{ DP}_\alpha(dP)
$$

좌변을 정리하면 아래와 같다.

$$
\begin{align*}
\text{LHS } &= \int_B \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&=  \sum_{j=1}^{k} \int_{B \cap B_j} \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
\end{align*}
$$

Dirichlet process의 정의에 따라, $(P(B_1),\cdots, P(B_k))$는 Dirichlet distribution를 따른다. 또한 첫 번째 적분기호를 $x$가 어느 partition에 들어가는지에 따라 적분 항을 쪼갰으므로, $\int_{B \cap B_j} \cdots G_0(dx)$ 안에서는 $(P(B_1),\cdots, P(B_k))$가 $\text{Dirichlet}(\alpha ' + e_j)$의 분포를 갖는다. 그러므로,

$$
\begin{align*}
\text{LHS } &= \int_B \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&=  \sum_{j=1}^{k} \int_{B \cap B_j} \int \prod_{i=1}^{k} P(B_i)^{r_i} \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) \\
&=  \sum_{j=1}^{k} \int_{B \cap B_j} \mathbb E \left[ \prod_{i=1}^{k} P(B_i)^{r_i} \right] G_0( dx) \\
&=  \sum_{j=1}^{k} \int_{B \cap B_j} \mathbb E \left[ \prod_{i=1}^{k} Y_i^{r_i} \right] G_0( dx)  \\
&\text{ (where } Y \sim \text{Dirichlet}(\alpha ' +e_j)=\text{Dirichlet}(\alpha_1 , \cdots , \alpha_j+1, \cdots, \alpha_k))\\
&= \sum_{j=1}^{k}  \mathbb E \left[ \prod_{i=1}^{k} Y_i^{r_i} \right] G_0(B \cap B_j) \\
&= \sum_{j=1}^{k}  G_0(B \cap B_j) \int y_1^{r_1} \cdots y_j^{r_j} \cdots y_k^{r_k} \frac{\Gamma(\sum_{i=1}^{k}\alpha_i +1)}{\Gamma(\alpha_1)\cdots \Gamma(\alpha_j+1) \cdots \Gamma(\alpha_k)} y_1^{\alpha_1-1} \cdots y_j^{\alpha_j} \cdots y_k^{\alpha_k-1} dy \\
&= \sum_{j=1}^{k}  G_0(B \cap B_j) 
\frac{\sum_{i=1}^{k}\alpha_i}{\alpha_j}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \frac{\Gamma(\sum_{i=1}^{k}\alpha_i )}{\Gamma(\alpha_1)\cdots \Gamma(\alpha_j) \cdots \Gamma(\alpha_k)} y_1^{\alpha_1-1} \cdots y_j^{\alpha_j-1} \cdots y_k^{\alpha_k-1} dy \\
&= \sum_{j=1}^{k}  G_0(B \cap B_j) 
\frac{\sum_{i=1}^{k}\alpha_i}{\alpha_j}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \text{Dirichlet}_{\alpha '} (dy) \\
&= \sum_{j=1}^{k}  G_0(B \cap B_j) 
\frac{\alpha(\mathfrak X)}{\alpha(B_j)}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \text{Dirichlet}_{\alpha '} (dy) \\
&= \sum_{j=1}^{k} \frac{\alpha(B \cap B_j)}{\alpha(B_j)}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \text{Dirichlet}_{\alpha '} (dy) \\
\end{align*}
$$

우변을 정리하면 아래와 같다. 5번째 등호는 Dirichlet process의 tail-free property에 의해 $P(B_1),\cdots, P(B_k)$와 $P( B \mid B_j), P( B^c \mid B_j)$가 서로 독립인 것에서 온 결과이다. 또한, 6번째 등호는 Dirichlet process를 partition으로 refine하며 무한히 쪼갤 때의 splitting variable이 Beta분포를 따르는 것에서 온 결과이다. 이에 대한 자세한 내용은 [3.2 Properties](#32-properties)의 tail-freeness section에서 소개하였다.



$$
\begin{align*}
\text{RHS } &= \int \prod_{i=1}^{k} P(B_i)^{r_i} P(B) \text{ DP}_\alpha(dP) \\
&= \int \left( \prod_{i=1}^{k}  P(B_i)^{r_i} \right) \sum_{j=1}^k P(B \cap B_j) \text{ DP}_\alpha(dP) \\
&= \sum_{j=1}^k \int \left( \prod_{i=1}^{k} P(B_i)^{r_i} \right) P(B \cap B_j) \text{ DP}_\alpha(dP) \\
&= \sum_{j=1}^k \int P(B_1)^{r_1} \cdots P(B_j)^{r_j+1} \cdots P(B_k)^{r_k} \frac{P(B \cap B_j)}{P(B_j)} \text{ DP}_\alpha(dP) \\
&= \sum_{j=1}^k \Bigg[ \int P(B_1)^{r_1} \cdots P(B_j)^{r_j+1} \cdots P(B_k)^{r_k} \text{ DP}_\alpha(dP) \Bigg] \Bigg[ \int \frac{P(B \cap B_j)}{P(B_j)} \text{ DP}_\alpha(dP) \Bigg] \\
&= \sum_{j=1}^{k} \frac{\alpha(B \cap B_j)}{\alpha(B_j)}
\int y_1^{r_1} \cdots y_j^{r_j+1} \cdots y_k^{r_k} \text{Dirichlet}_{\alpha '} (dy) \\
&\left( \because \enspace P(B \mid B_j)=\frac{P(B \cap B_j)}{P(B_j)} \sim \text{Beta}\big(\alpha(B \cap B_j),\alpha(B^c \cap B_j) \big)  \right)\\
\end{align*}
$$

즉 $\mathcal C^\ast$이 $\mathcal H$의 부분집합이 되며, 그에 따라 $\mathcal C$ 역시 $\mathcal H$의 부분집합이 된다.  
  
따라서, monotone class theorem for function에 의해, $\mathcal H$는 모든 bounded real valued $\sigma(\mathcal C)$-measurable 함수를 포함한다. 또한 위에서 확인했듯이, $\mathcal M = \sigma(\mathcal C)$이므로, $\mathcal H$는 모든 bounded real valued $\mathcal M$-measurable 함수를 포함한다. 다시 말해서, 임의의 bounded real valued $\mathcal M$-measurable function $f$에 대하여, 아래의 식이 성립한다는 것이 된다.


$$
\int_B \int f(P) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int f(P) P(B) \text{ DP}_\alpha(dP)
$$

그런데 $I(P \in M)$는 임의의 $M \in \mathcal M$에 대해 항상 bounded real valued $\mathcal M$-measurable function이다. 따라서 우리가 보이고자 했던 Fact 2.2가 증명되었다.

$$
\int_B \int I(P \in M) \text{ }DP_{\alpha+\delta_x}(dP) G_0( dx) = \int I(P \in M) P(B) \text{ DP}_\alpha(dP)
$$

따라서, Fact 1, Fact 2, Fact 2.1, Fact 2.2에 의해,  **$\text{DP}(\alpha + \delta_X)$의 distribution을 따르는 어떤 random probability measure $\tilde P$가, $X$가 주어졌을 때의 $P$의 regular conditional probability의 한 version이 된다**. $\square$  
  
긴 증명이었지만 결론은, 어떤 random measure의 prior distribution이 $\text{DP}(\alpha)$였다면, $N$개의 자료를 관찰한 후의 posterior distribution은 $\text{DP}(\alpha + \sum_{i=1}^{N} \delta_{X_i})$라는 것이다. 

$$
P \mid X_1, \cdots , X_N \sim \text{DP}\left( \alpha + \sum_{i=1}^{N} \delta_{X_i} \right)
$$

Dirichlet process의 base measure를 $\alpha_0$와 probability measure $G_0$를 이용해 parametrize한 경우는 posterior distribution이 아래와 같다.

$$
\begin{align*}
P \mid X_1, \cdots , X_N &\sim \text{DP}\left( \frac{1}{\alpha_0 + N} \left\{ \alpha + \sum_{i=1}^{N} \delta_{X_i} \right\} \right) \\
&=\text{DP}\left( \frac{\alpha_0}{\alpha_0 + N} G_0 + \frac{N}{\alpha_0 + N} \mathbb P_N  \right) ,\\
\text{where }\mathbb P_N = \frac{1}{N} &\sum_{i=1}^{N} \delta_{X_i} \text{ is the empirical distribution of } X_1, \cdots, X_N \\ 
\end{align*}
$$

위에서 Dirichlet process의 mean과 variance를 도출했던 것과 같은 방식으로, posterior mean, variance을 구하면 다음과 같다.

$$
\begin{align*}
\mathbb E [P(A) \mid X_1, \cdots , X_N ] &= \frac{\alpha_0}{\alpha_0 + N} G_0(A) + \frac{N}{\alpha_0 + N} \mathbb P_N (A)  \\
&\stackrel{\text{let}}{=} \tilde{\mathbb P}_N (A)\\
\text{Var} [P(A) \mid X_1, \cdots , X_N ] &= \frac{\tilde{\mathbb P}_N (A) \tilde{\mathbb P}_N (A^c)}{\alpha_0 + N +1} 
\end{align*}
$$

의미를 살펴보자면 다음과 같다.

 * Dirichlet process의 posterior mean measure는 prior mean measure$(G_0)$와 empirical distribution$(\mathbb P_N)$의 convex combination이다.
  * Weight는 각각 $\frac{\alpha_0}{\alpha_0 + N}, \frac{N}{\alpha_0 + N}$이다.
  * 따라서, base measure $\alpha$의 전체집합에 대한 measure 값인 $\alpha_0 = \alpha(\mathfrak X)$가 클 수록, 자료의 sample 수$(N)$가 작을 수록, posterior mean measure가 prior mean measure에 가깝게 된다.
  * 이러한 관점에서 $\alpha_0 = \alpha(\mathfrak X)$를 ***prior sample size***, 그리고 $\alpha_0 + N$을 ***posterior sample size***라고 부른다.
  * 또한 자료의 수 $N$이 커짐에 따라, posterior mean measure는 almost sure하게 empirical distribution과 같은 asymptotic behavior를 보인다. 따라서 적절한 조건이 만족된다면, posterior mean measure 역시 empirical distribution과 같이 consistency, asymptotic normality와 같은 성질을 갖는다.
  * 마찬가지로 만약 우리의 자료가 true distribution $P_0$에서 sample되었다면, posterior mean measure은 자료의 수가 커짐에 따라 almost sure하게 $P_0$로 수렴할 것이다.

 * Posterior mean measure뿐만 아니라, $P$의 full posterior distribution 역시 mean measure로 수축할 것이다. 이는 다음과 같이 posterior variance measure가 bound되는 것을 이용해서 확인할 수 있다.

$$
\begin{align*}
\text{Var} [P(A) \mid X_1, \cdots , X_N ] &= \frac{\tilde{\mathbb P}_N (A) \tilde{\mathbb P}_N (A^c)}{\alpha_0 + N +1} \leq  \frac{1}{4(\alpha_0 + N +1)} \\ 
&\longrightarrow 0 \enspace ,\enspace \enspace \text{as }N \rightarrow \infty
\end{align*}
$$

  
<br>

## 3.2.5 Marginal and Conditional Distribution

Dirichlet process로부터 얻어진 sample $X_1, \cdots ,X_N$, 정확히는 $P \sim \text{DP}(\alpha)$, $X_1, \cdots , X_N \vert P \stackrel{iid}{\sim} P$로 얻어진 sample은 복잡한 joint distribution을 갖는다. 하지만 이 joint distribution은 다음과 같은 predictive distribution의 sequence로 간단하게 나타낼 수 있다.

$$
X_1, \enspace X_2 \vert X_1, \enspace X_3 \vert X_1 , X_2, \enspace \cdots 
$$

먼저 위에서 정의한 conditional probability의 정의에 따라, 

$$
\mathbb P(X_1 \in A) = \mathbb E [\mathbb P(X_1 \in A \mid P)]=\mathbb E[P(A)]=G_0(A)
$$

이므로, $X_1$의 marginal distribution은 $G_0$이다. $X_2 \vert X_1$의 marginal distribution의 경우, 다음과 같이 도출할 수 있다. 

$$
\begin{align*}
X_2 \vert P,X_1  &\sim \text{ } P \enspace ( \because \text{ }X_2 \vert P,X_1  \stackrel{d}{=} X_2 \vert P \text{ , due to conditional independence} )\\
P\vert X_1 &\sim \text{DP}(\alpha+\delta_{X_1}) \enspace ( \because \text{ conjugacy of Dirichlet process} )\\
\end{align*}
$$

$$
\begin{align*}
\mathbb P(X_2 \in A \vert X_1) &= \mathbb E [\mathbb P(X_2 \in A \mid P, X_1) \vert X_1] \\
&=\mathbb E[P(A)\vert X_1] \\
&= \frac{\alpha(A) + \delta_{X_1}(A)}{\alpha_0 + 1} \\
&= \frac{\alpha_0}{\alpha_0 + 1}G_0(A) +\frac{1}{\alpha_0 + 1} \delta_{X_1}(A)
\end{align*}
$$

이와 같이 Dirichlet process의 conjugacy를 이용해 같은 작업을 반복하면 다음과 같은 predictive distribution을 얻을 수 있다.

$$
X_i \vert X_1 , \cdots , X_{i-1} = 
\begin{cases}
\delta_{X_{1}} &\quad\text{with probability } \frac{1}{\alpha_0 +i-1}\\
\enspace \vdots &\quad \enspace \enspace \enspace \vdots\\
\delta_{X_{i-1}} &\quad\text{with probability } \frac{1}{\alpha_0 +i-1}\\
G_0 &\quad\text{with probability } \frac{\alpha_0}{\alpha_0 +i-1}\\
\end{cases}
$$

여기서 주목할 점은 위 predictive distribution은 같은 distribution들의 mixture로 표현되기 때문에 index에 영향을 받지 않는다는 것이다. 다시 말해서, 임의의 permutation $\sigma$에 대해,

$$
\mathbb P(X_1 \in A_1, \cdots , X_N \in A_N) = \mathbb P(X_{\sigma(1)} \in A_{\sigma(1)}, \cdots , X_{\sigma(N)} \in A_{\sigma(N)})
$$

즉, Dirichlet process로 생성된, 정확히는 $P \sim \text{DP}(\alpha)$, $X_1, \cdots , X_N \vert P \stackrel{iid}{\sim} P$로 얻어진 sample의 joint distribution은 ***exchangeable***하다. 이는 앞 장에서 소개한 De Finetti's theorem과 연관되는데 이에 대한 내용은 뒤에서 더 자세히 다루도록 하겠다.

## 3.2.6 Discreteness

